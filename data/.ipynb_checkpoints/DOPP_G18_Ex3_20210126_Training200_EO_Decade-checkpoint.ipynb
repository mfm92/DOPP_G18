{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "behavioral-family",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction-to-Assignment\" data-toc-modified-id=\"Introduction-to-Assignment-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction to Assignment</a></span><ul class=\"toc-item\"><li><span><a href=\"#First-draft\" data-toc-modified-id=\"First-draft-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>First draft</a></span><ul class=\"toc-item\"><li><span><a href=\"#Topic-and-Questions-to-answer\" data-toc-modified-id=\"Topic-and-Questions-to-answer-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Topic and Questions to answer</a></span></li><li><span><a href=\"#Justification-For-Limit-Of-Scope\" data-toc-modified-id=\"Justification-For-Limit-Of-Scope-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Justification For Limit Of Scope</a></span></li><li><span><a href=\"#Workflow-plan-&amp;-Project-management\" data-toc-modified-id=\"Workflow-plan-&amp;-Project-management-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Workflow plan &amp; Project management</a></span></li><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-1.1.4\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;</span>Data</a></span></li></ul></li><li><span><a href=\"#Second-Draft\" data-toc-modified-id=\"Second-Draft-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Second Draft</a></span></li><li><span><a href=\"#Pivoting-Point\" data-toc-modified-id=\"Pivoting-Point-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Pivoting Point</a></span></li><li><span><a href=\"#Language-change-in-Icelandic-Parliamentary-Speeches\" data-toc-modified-id=\"Language-change-in-Icelandic-Parliamentary-Speeches-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Language change in Icelandic Parliamentary Speeches</a></span></li></ul></li><li><span><a href=\"#Estimating-publication-year-from-Project-Gutenberg\" data-toc-modified-id=\"Estimating-publication-year-from-Project-Gutenberg-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Estimating publication year from Project Gutenberg</a></span><ul class=\"toc-item\"><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-packages\" data-toc-modified-id=\"Import-packages-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Import packages</a></span></li><li><span><a href=\"#Define-Constants\" data-toc-modified-id=\"Define-Constants-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Define Constants</a></span></li></ul></li><li><span><a href=\"#Importing-the-data\" data-toc-modified-id=\"Importing-the-data-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Importing the data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Getting-the-content\" data-toc-modified-id=\"Getting-the-content-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Getting the content</a></span></li><li><span><a href=\"#Data-Cleansing\" data-toc-modified-id=\"Data-Cleansing-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Data Cleansing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Read-a-single-file\" data-toc-modified-id=\"Read-a-single-file-2.2.2.1\"><span class=\"toc-item-num\">2.2.2.1&nbsp;&nbsp;</span>Read a single file</a></span></li><li><span><a href=\"#Return-list-of-all-words\" data-toc-modified-id=\"Return-list-of-all-words-2.2.2.2\"><span class=\"toc-item-num\">2.2.2.2&nbsp;&nbsp;</span>Return list of all words</a></span></li></ul></li></ul></li><li><span><a href=\"#Statistics\" data-toc-modified-id=\"Statistics-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Statistics</a></span><ul class=\"toc-item\"><li><span><a href=\"#First-attempt\" data-toc-modified-id=\"First-attempt-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>First attempt</a></span></li><li><span><a href=\"#Read-all-files,-and-do-preprocessing\" data-toc-modified-id=\"Read-all-files,-and-do-preprocessing-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Read all files, and do preprocessing</a></span></li><li><span><a href=\"#Compare-Word-ranking-between-titles\" data-toc-modified-id=\"Compare-Word-ranking-between-titles-2.3.3\"><span class=\"toc-item-num\">2.3.3&nbsp;&nbsp;</span>Compare Word ranking between titles</a></span></li></ul></li><li><span><a href=\"#Second-testing\" data-toc-modified-id=\"Second-testing-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Second testing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Read-in-from-the-decades-files,-and-see-the-distributions\" data-toc-modified-id=\"Read-in-from-the-decades-files,-and-see-the-distributions-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Read in from the decades files, and see the distributions</a></span></li><li><span><a href=\"#Preliminary-Conclusion\" data-toc-modified-id=\"Preliminary-Conclusion-2.4.2\"><span class=\"toc-item-num\">2.4.2&nbsp;&nbsp;</span>Preliminary Conclusion</a></span></li><li><span><a href=\"#Compare-ranking-between-upload-decades\" data-toc-modified-id=\"Compare-ranking-between-upload-decades-2.4.3\"><span class=\"toc-item-num\">2.4.3&nbsp;&nbsp;</span>Compare ranking between upload-decades</a></span></li></ul></li><li><span><a href=\"#Trying-to-fit-models-to-predict\" data-toc-modified-id=\"Trying-to-fit-models-to-predict-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Trying to fit models to predict</a></span><ul class=\"toc-item\"><li><span><a href=\"#Read-in-files\" data-toc-modified-id=\"Read-in-files-2.5.1\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;</span>Read in files</a></span></li><li><span><a href=\"#Train-models\" data-toc-modified-id=\"Train-models-2.5.2\"><span class=\"toc-item-num\">2.5.2&nbsp;&nbsp;</span>Train models</a></span></li></ul></li><li><span><a href=\"#Realisation-and-conclusion\" data-toc-modified-id=\"Realisation-and-conclusion-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Realisation and conclusion</a></span></li></ul></li><li><span><a href=\"#Studying-language-change-in-Icelandic-parliamentary-speeches\" data-toc-modified-id=\"Studying-language-change-in-Icelandic-parliamentary-speeches-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Studying language change in Icelandic parliamentary speeches</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#The-task\" data-toc-modified-id=\"The-task-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>The task</a></span></li><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-required-libraries\" data-toc-modified-id=\"Load-required-libraries-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Load required libraries</a></span></li><li><span><a href=\"#Get-the-data\" data-toc-modified-id=\"Get-the-data-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Get the data</a></span></li><li><span><a href=\"#Preprocessing-helpers\" data-toc-modified-id=\"Preprocessing-helpers-3.3.3\"><span class=\"toc-item-num\">3.3.3&nbsp;&nbsp;</span>Preprocessing helpers</a></span></li></ul></li><li><span><a href=\"#Preliminary-Data-Analysis\" data-toc-modified-id=\"Preliminary-Data-Analysis-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Preliminary Data Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Zipf's-Law\" data-toc-modified-id=\"Zipf's-Law-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>Zipf's Law</a></span></li><li><span><a href=\"#Disappearing-words-/-new-words\" data-toc-modified-id=\"Disappearing-words-/-new-words-3.4.2\"><span class=\"toc-item-num\">3.4.2&nbsp;&nbsp;</span>Disappearing words / new words</a></span></li><li><span><a href=\"#Development-of-average-sentence-length\" data-toc-modified-id=\"Development-of-average-sentence-length-3.4.3\"><span class=\"toc-item-num\">3.4.3&nbsp;&nbsp;</span>Development of average sentence length</a></span></li><li><span><a href=\"#n-grams\" data-toc-modified-id=\"n-grams-3.4.4\"><span class=\"toc-item-num\">3.4.4&nbsp;&nbsp;</span>n-grams</a></span></li></ul></li><li><span><a href=\"#Building-model-for-classifying-speeches\" data-toc-modified-id=\"Building-model-for-classifying-speeches-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Building model for classifying speeches</a></span><ul class=\"toc-item\"><li><span><a href=\"#Constructing-training-and-test-data\" data-toc-modified-id=\"Constructing-training-and-test-data-3.5.1\"><span class=\"toc-item-num\">3.5.1&nbsp;&nbsp;</span>Constructing training and test data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Train-data\" data-toc-modified-id=\"Train-data-3.5.1.1\"><span class=\"toc-item-num\">3.5.1.1&nbsp;&nbsp;</span>Train data</a></span></li><li><span><a href=\"#Test-data\" data-toc-modified-id=\"Test-data-3.5.1.2\"><span class=\"toc-item-num\">3.5.1.2&nbsp;&nbsp;</span>Test data</a></span></li><li><span><a href=\"#See-classes-distribution-within-train-and-test-sets\" data-toc-modified-id=\"See-classes-distribution-within-train-and-test-sets-3.5.1.3\"><span class=\"toc-item-num\">3.5.1.3&nbsp;&nbsp;</span>See classes distribution within train and test sets</a></span></li></ul></li><li><span><a href=\"#Text-feature-extraction\" data-toc-modified-id=\"Text-feature-extraction-3.5.2\"><span class=\"toc-item-num\">3.5.2&nbsp;&nbsp;</span>Text feature extraction</a></span><ul class=\"toc-item\"><li><span><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-3.5.2.1\"><span class=\"toc-item-num\">3.5.2.1&nbsp;&nbsp;</span>TF-IDF</a></span></li><li><span><a href=\"#Word2Vec\" data-toc-modified-id=\"Word2Vec-3.5.2.2\"><span class=\"toc-item-num\">3.5.2.2&nbsp;&nbsp;</span>Word2Vec</a></span></li><li><span><a href=\"#Doc2Vec\" data-toc-modified-id=\"Doc2Vec-3.5.2.3\"><span class=\"toc-item-num\">3.5.2.3&nbsp;&nbsp;</span>Doc2Vec</a></span></li></ul></li><li><span><a href=\"#Classifiers\" data-toc-modified-id=\"Classifiers-3.5.3\"><span class=\"toc-item-num\">3.5.3&nbsp;&nbsp;</span>Classifiers</a></span><ul class=\"toc-item\"><li><span><a href=\"#Multinominal-Naive-Bayes-(MNB)\" data-toc-modified-id=\"Multinominal-Naive-Bayes-(MNB)-3.5.3.1\"><span class=\"toc-item-num\">3.5.3.1&nbsp;&nbsp;</span>Multinominal Naive Bayes (MNB)</a></span></li><li><span><a href=\"#Support-Vector-Machines\" data-toc-modified-id=\"Support-Vector-Machines-3.5.3.2\"><span class=\"toc-item-num\">3.5.3.2&nbsp;&nbsp;</span>Support Vector Machines</a></span></li><li><span><a href=\"#Random-Forest-Classifier\" data-toc-modified-id=\"Random-Forest-Classifier-3.5.3.3\"><span class=\"toc-item-num\">3.5.3.3&nbsp;&nbsp;</span>Random Forest Classifier</a></span></li></ul></li><li><span><a href=\"#Train-models\" data-toc-modified-id=\"Train-models-3.5.4\"><span class=\"toc-item-num\">3.5.4&nbsp;&nbsp;</span>Train models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model-1:-TFIDF-vectorizer,-select-K-best-and-Multinomial-Naive-Bayes\" data-toc-modified-id=\"Model-1:-TFIDF-vectorizer,-select-K-best-and-Multinomial-Naive-Bayes-3.5.4.1\"><span class=\"toc-item-num\">3.5.4.1&nbsp;&nbsp;</span>Model 1: TFIDF vectorizer, select K best and Multinomial Naive Bayes</a></span></li><li><span><a href=\"#Model-2:-TFIDF-vectorizer,-select-K-best-and-SVC\" data-toc-modified-id=\"Model-2:-TFIDF-vectorizer,-select-K-best-and-SVC-3.5.4.2\"><span class=\"toc-item-num\">3.5.4.2&nbsp;&nbsp;</span>Model 2: TFIDF vectorizer, select K best and SVC</a></span></li><li><span><a href=\"#Model-3:-TFIDF-vectorizer,-select-K-best-and-Random-Forest-Classifier\" data-toc-modified-id=\"Model-3:-TFIDF-vectorizer,-select-K-best-and-Random-Forest-Classifier-3.5.4.3\"><span class=\"toc-item-num\">3.5.4.3&nbsp;&nbsp;</span>Model 3: TFIDF vectorizer, select K best and Random Forest Classifier</a></span></li><li><span><a href=\"#Model-4:-Word2Vec-and-SVC\" data-toc-modified-id=\"Model-4:-Word2Vec-and-SVC-3.5.4.4\"><span class=\"toc-item-num\">3.5.4.4&nbsp;&nbsp;</span>Model 4: Word2Vec and SVC</a></span></li><li><span><a href=\"#Model-5:-Word2Vec-and-Random-Forest-Classifier\" data-toc-modified-id=\"Model-5:-Word2Vec-and-Random-Forest-Classifier-3.5.4.5\"><span class=\"toc-item-num\">3.5.4.5&nbsp;&nbsp;</span>Model 5: Word2Vec and Random Forest Classifier</a></span></li><li><span><a href=\"#Model-6:-Doc2Vec-and-Support-Vector-Machines\" data-toc-modified-id=\"Model-6:-Doc2Vec-and-Support-Vector-Machines-3.5.4.6\"><span class=\"toc-item-num\">3.5.4.6&nbsp;&nbsp;</span>Model 6: Doc2Vec and Support Vector Machines</a></span></li><li><span><a href=\"#Model-7:-Doc2Vec-and-Random-Forest-Classifier\" data-toc-modified-id=\"Model-7:-Doc2Vec-and-Random-Forest-Classifier-3.5.4.7\"><span class=\"toc-item-num\">3.5.4.7&nbsp;&nbsp;</span>Model 7: Doc2Vec and Random Forest Classifier</a></span></li></ul></li><li><span><a href=\"#Compare-CV-results-from-trained-models\" data-toc-modified-id=\"Compare-CV-results-from-trained-models-3.5.5\"><span class=\"toc-item-num\">3.5.5&nbsp;&nbsp;</span>Compare CV results from trained models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Raw-results\" data-toc-modified-id=\"Raw-results-3.5.5.1\"><span class=\"toc-item-num\">3.5.5.1&nbsp;&nbsp;</span>Raw results</a></span></li><li><span><a href=\"#Tradeoff-score-vs-mean-fit-time\" data-toc-modified-id=\"Tradeoff-score-vs-mean-fit-time-3.5.5.2\"><span class=\"toc-item-num\">3.5.5.2&nbsp;&nbsp;</span>Tradeoff score vs mean fit time</a></span></li><li><span><a href=\"#Best-estimator-from-each-model\" data-toc-modified-id=\"Best-estimator-from-each-model-3.5.5.3\"><span class=\"toc-item-num\">3.5.5.3&nbsp;&nbsp;</span>Best estimator from each model</a></span></li><li><span><a href=\"#Next-steps\" data-toc-modified-id=\"Next-steps-3.5.5.4\"><span class=\"toc-item-num\">3.5.5.4&nbsp;&nbsp;</span>Next steps</a></span></li></ul></li></ul></li><li><span><a href=\"#Evaluation-and-model-selection\" data-toc-modified-id=\"Evaluation-and-model-selection-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Evaluation and model selection</a></span></li></ul></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Conclusion</a></span></li><li><span><a href=\"#Further-Works\" data-toc-modified-id=\"Further-Works-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Further Works</a></span><ul class=\"toc-item\"><li><span><a href=\"#Predict-Different-Sources\" data-toc-modified-id=\"Predict-Different-Sources-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Predict Different Sources</a></span></li><li><span><a href=\"#Treating-years-as-Contious-Variables\" data-toc-modified-id=\"Treating-years-as-Contious-Variables-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Treating years as Contious Variables</a></span></li><li><span><a href=\"#Gaining-insight-into-Explanatory-Variables\" data-toc-modified-id=\"Gaining-insight-into-Explanatory-Variables-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Gaining insight into Explanatory Variables</a></span></li><li><span><a href=\"#Additional-Feature-Extraction-and-Classifiers\" data-toc-modified-id=\"Additional-Feature-Extraction-and-Classifiers-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Additional Feature Extraction and Classifiers</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-border",
   "metadata": {},
   "source": [
    "# Introduction to Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-occupation",
   "metadata": {},
   "source": [
    "This is the third Exercise of **188.995 Data-Oriented Programming Paradigms**\n",
    "\n",
    "We are group 18, and consist of:\n",
    "* Guillermo Alamán Requena, Matr. Nr: 11937906\n",
    "* Michael Ferdinand Moser, Matr. Nr: 01123077 \n",
    "* Paul Joe Maliakel, Matr. Nr: 12012422\n",
    "* Gunnar Sjúrðarson Knudsen, Matr. Nr: 12028205\n",
    "\n",
    "In this task we were asked to choose one vaguely worded question, and then narrow the scope, figuring out how to get the data, before finally solving the question at hand.\n",
    "We chose **Question 21**, which contains:\n",
    "* How does the use of various communication languages in countries change over time?\n",
    "* Which languages grow and which disappear,  and what are their characteristics?\n",
    "* Are there other factors that correlate with the appearance or disappearance of languages?\n",
    "\n",
    "\n",
    "We soon realized that the question as stated is far too broad, and we therefore had to limit it.\n",
    "\n",
    "After having discussed amoung our groups, we came to the following plan:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-disposal",
   "metadata": {},
   "source": [
    "## First draft\n",
    "### Topic and Questions to answer\n",
    "We've selected question 21, which is regarding how communication languages in countries change over time.\n",
    "\n",
    "After having discussed the data available, and planned a workflow, we've decided to try to answer the questions:\n",
    "* How has the English language changed in the past 100 years based on word frequencies, sentence length, ...?\n",
    "* Can we find parallel developments between different genres of text?\n",
    "* Can the publication year of a movie/article/whatever be predicted based on the text and its characteristics?\n",
    "\n",
    "\n",
    "### Justification For Limit Of Scope\n",
    "The sample questions stated in the task description are too broad, to be answered in a single 160 hour project.\n",
    "* Lot's of issues, such as: \n",
    "* Lack of census data; \n",
    "* other changes such as phonetic, semantic and syntactic meanings; \n",
    "* High correlation with e.g:\n",
    "    * country population\n",
    "    * age of speakers\n",
    "    * ...\n",
    "* What counts as a language? \n",
    "    * dialect? \n",
    "    * Mutually Intelligible?\n",
    "* Political dimensions\n",
    "* Multilingual people\n",
    "* How do we check accuracy of the available data?\n",
    "* ...\n",
    "\n",
    "\n",
    "Historical data for language use is likely not available for most languages, as it's topics for great research to estimate merely historical populations - especially before 1850 or so. \n",
    "The evolution of languages are much less documented. \n",
    "Lack of census data overall, but other changes are even harder to gauge, such as phonetic, semantic, and syntactic meanings. Highly correlated with population of countries, but also with \"hidden\" correlations, such as age of speakers, ... \n",
    "Even dead languages can be revived. \n",
    "\n",
    "What constitutes a language? Dialect? Mutually Intelligible? Also do not forget the political dimension, e.g. Croatian/Serbian really are just dialects of the same language but they want to keep separate. On the other end of this scheme the variant of Chinese spoken in Beijing may be drastically different from the Chinese spoken in other regions of the country, but still falls under the same \"Chinese\" umbrella to communicate unity. \n",
    "\n",
    "How much is spoken? Should we consider people who studied a language as their second, third... language? If so, how well should be the command over the language for the person to count? A1/B1/C2 level?\n",
    "%How do we check accuracy of the available data?\n",
    "\n",
    "### Workflow plan & Project management\n",
    "* Outline the plan\n",
    "    * Get, understand and clean data: articles/movie scripts/video transcripts over the years (see next section)\n",
    "    * Train-test split: keeping proportion of publication years within the splits.\n",
    "    * Preprocessing: text feature extraction, feature selection, scaling, etc. (Come back here if necessary)\n",
    "    * Visualization: evolution of words over the years, word-clouds and other relevant characteristics.\n",
    "    * Define evaluation metrics, train different models/parameters using CV and select best one for predictions.\n",
    "    * Predict, conclude, report and publish notebook in Kaggle Kernel.\n",
    "* How the work will be divided up between group members\n",
    "    * Acquisition, cleaning and prepossessing of the data will be done commonly.\n",
    "    * Each member of the group will train a model and report results using same evaluation metrics. \n",
    "    * Jointly choose the best model and conclude.\n",
    "    * Presentation, report and publishing will be also split. \n",
    "* Timeline: To be defined after review meeting    \n",
    "\n",
    "\n",
    "### Data\n",
    "\n",
    "Our goal is to get a dataset similar to:\n",
    "\n",
    "\n",
    "\n",
    "| **Corpus** | **Year Published** | **Type**               | ... |\n",
    "|------------|--------------------|------------------------|-----|\n",
    "| Text1      | 1976               | News                   | ... |\n",
    "| Text2      | 1976               | Movie Script           | ... |\n",
    "| ...        | ...                | ...                    | ... |\n",
    "| TextN      | 2009               | Scientific Article     | ... |\n",
    "\n",
    "Feature extraction from texts will be performed to obtain appropriate features for modeling. To build a dataset like this one, we will rely on the following kind sources: \n",
    "\n",
    "* \\url{https://www.kaggle.com/asad1m9a9h6mood/news-articles} - News articles from 2015 until date.\n",
    "* \\url{https://www.kaggle.com/snapcrack/all-the-news} - 143000 articles from 15 American Publications. \n",
    "* NLTK\n",
    "* ...\n",
    "\n",
    "\n",
    "## Second Draft\n",
    "After having a preliminary meeting with Univ.Prof. Dr. Hanbury and Dipl.-Ing. Dr. Piroi, who gave great input, we decided to further limit out goal to only use Project Gutenberg as a datasource, and setting our hypothesis to see whether it was possible to generate a model that predicted the publication year/decade for a set of books.\n",
    "\n",
    "## Pivoting Point\n",
    "After having done a decent portion of work, we reached to the conclusion that our dataset was not suitable to solve the question we had original set out, and we were forced to pivot.\n",
    "\n",
    "We discussed whether we wanted to change the goal from classifying, but as we were all quite interrested in a classification algorithm, and wanted to do proper NLP, we instead searched for another dataset.\n",
    "\n",
    "## Language change in Icelandic Parliamentary Speeches\n",
    "We found the dataset with all icelandic parliamentary speeches going back a century. This is further described in section 3. \n",
    "With this great dataset, our goal was to develop a model that could try to predict which decade a speech is from"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-impact",
   "metadata": {},
   "source": [
    "# Estimating publication year from Project Gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-detection",
   "metadata": {},
   "source": [
    "This was the attempt at our first hypothesis. \n",
    "We import a large corpus of books from Project Gutenberg, and cleanse the data, so it's ready for machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-norman",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We start by setting up all packages needed for the project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-intranet",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "boolean-florida",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T10:29:40.342663Z",
     "start_time": "2021-01-26T10:29:39.725155Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from builtins import str\n",
    "import os\n",
    "from six import u\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from operator import itemgetter    \n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "import random\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import math\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-leeds",
   "metadata": {},
   "source": [
    "### Define Constants\n",
    "Constant that are used in this part is also set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "preliminary-things",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T10:29:40.350187Z",
     "start_time": "2021-01-26T10:29:40.343864Z"
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"processedData\"\n",
    "\n",
    "TEXT_START_MARKERS = frozenset((u(_) for _ in (\n",
    "    \"*END*THE SMALL PRINT\",\n",
    "    \"*** START OF THE PROJECT GUTENBERG\",\n",
    "    \"*** START OF THIS PROJECT GUTENBERG\",\n",
    "    \"This etext was prepared by\",\n",
    "    \"E-text prepared by\",\n",
    "    \"Produced by\",\n",
    "    \"Distributed Proofreading Team\",\n",
    "    \"Proofreading Team at http://www.pgdp.net\",\n",
    "    \"http://gallica.bnf.fr)\",\n",
    "    \"      http://archive.org/details/\",\n",
    "    \"http://www.pgdp.net\",\n",
    "    \"by The Internet Archive)\",\n",
    "    \"by The Internet Archive/Canadian Libraries\",\n",
    "    \"by The Internet Archive/American Libraries\",\n",
    "    \"public domain material from the Internet Archive\",\n",
    "    \"Internet Archive)\",\n",
    "    \"Internet Archive/Canadian Libraries\",\n",
    "    \"Internet Archive/American Libraries\",\n",
    "    \"material from the Google Print project\",\n",
    "    \"*END THE SMALL PRINT\",\n",
    "    \"***START OF THE PROJECT GUTENBERG\",\n",
    "    \"This etext was produced by\",\n",
    "    \"*** START OF THE COPYRIGHTED\",\n",
    "    \"The Project Gutenberg\",\n",
    "    \"http://gutenberg.spiegel.de/ erreichbar.\",\n",
    "    \"Project Runeberg publishes\",\n",
    "    \"Beginning of this Project Gutenberg\",\n",
    "    \"Project Gutenberg Online Distributed\",\n",
    "    \"Gutenberg Online Distributed\",\n",
    "    \"the Project Gutenberg Online Distributed\",\n",
    "    \"Project Gutenberg TEI\",\n",
    "    \"This eBook was prepared by\",\n",
    "    \"http://gutenberg2000.de erreichbar.\",\n",
    "    \"This Etext was prepared by\",\n",
    "    \"This Project Gutenberg Etext was prepared by\",\n",
    "    \"Gutenberg Distributed Proofreaders\",\n",
    "    \"Project Gutenberg Distributed Proofreaders\",\n",
    "    \"the Project Gutenberg Online Distributed Proofreading Team\",\n",
    "    \"**The Project Gutenberg\",\n",
    "    \"*SMALL PRINT!\",\n",
    "    \"More information about this book is at the top of this file.\",\n",
    "    \"tells you about restrictions in how the file may be used.\",\n",
    "    \"l'authorization à les utilizer pour preparer ce texte.\",\n",
    "    \"of the etext through OCR.\",\n",
    "    \"*****These eBooks Were Prepared By Thousands of Volunteers!*****\",\n",
    "    \"We need your donations more than ever!\",\n",
    "    \" *** START OF THIS PROJECT GUTENBERG\",\n",
    "    \"****     SMALL PRINT!\",\n",
    "    '[\"Small Print\" V.',\n",
    "    '      (http://www.ibiblio.org/gutenberg/',\n",
    "    'and the Project Gutenberg Online Distributed Proofreading Team',\n",
    "    'Mary Meehan, and the Project Gutenberg Online Distributed Proofreading',\n",
    "    '                this Project Gutenberg edition.',\n",
    ")))\n",
    "\n",
    "\n",
    "TEXT_END_MARKERS = frozenset((u(_) for _ in (\n",
    "    \"*** END OF THE PROJECT GUTENBERG\",\n",
    "    \"*** END OF THIS PROJECT GUTENBERG\",\n",
    "    \"***END OF THE PROJECT GUTENBERG\",\n",
    "    \"End of the Project Gutenberg\",\n",
    "    \"End of The Project Gutenberg\",\n",
    "    \"Ende dieses Project Gutenberg\",\n",
    "    \"by Project Gutenberg\",\n",
    "    \"End of Project Gutenberg\",\n",
    "    \"End of this Project Gutenberg\",\n",
    "    \"Ende dieses Projekt Gutenberg\",\n",
    "    \"        ***END OF THE PROJECT GUTENBERG\",\n",
    "    \"*** END OF THE COPYRIGHTED\",\n",
    "    \"End of this is COPYRIGHTED\",\n",
    "    \"Ende dieses Etextes \",\n",
    "    \"Ende dieses Project Gutenber\",\n",
    "    \"Ende diese Project Gutenberg\",\n",
    "    \"**This is a COPYRIGHTED Project Gutenberg Etext, Details Above**\",\n",
    "    \"Fin de Project Gutenberg\",\n",
    "    \"The Project Gutenberg Etext of \",\n",
    "    \"Ce document fut presente en lecture\",\n",
    "    \"Ce document fut présenté en lecture\",\n",
    "    \"More information about this book is at the top of this file.\",\n",
    "    \"We need your donations more than ever!\",\n",
    "    \"END OF PROJECT GUTENBERG\",\n",
    "    \" End of the Project Gutenberg\",\n",
    "    \" *** END OF THIS PROJECT GUTENBERG\",\n",
    ")))\n",
    "\n",
    "\n",
    "LEGALESE_START_MARKERS = frozenset((u(_) for _ in (\n",
    "    \"<<THIS ELECTRONIC VERSION OF\",\n",
    ")))\n",
    "\n",
    "\n",
    "LEGALESE_END_MARKERS = frozenset((u(_) for _ in (\n",
    "    \"SERVICE THAT CHARGES FOR DOWNLOAD\",\n",
    ")))\n",
    "\n",
    "TITLE_MARKERS = frozenset((u(_) for _ in (\n",
    "    \"Title:\",\n",
    ")))\n",
    "\n",
    "AUTHOR_MARKERS = frozenset((u(_) for _ in (\n",
    "    \"Author:\",\n",
    ")))\n",
    "DATE_MARKERS = frozenset((u(_) for _ in (\n",
    "    \"Release Date:\",\"Release Date:\"\n",
    ")))\n",
    "LANGUAGE_MARKERS = frozenset((u(_) for _ in (\n",
    "    \"Language:\",\n",
    ")))\n",
    "ENCODING_MARKERS = frozenset((u(_) for _ in (\n",
    "    \"Character set encoding:\",\n",
    ")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-memphis",
   "metadata": {},
   "source": [
    "## Importing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-selection",
   "metadata": {},
   "source": [
    "This is a very rough first draft at importing and cleansing the data. \n",
    "Solution is heavily inspired by https://gist.github.com/mbforbes/cee3fd5bb3a797b059524fe8c8ccdc2b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-cancellation",
   "metadata": {},
   "source": [
    "### Getting the content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-tennis",
   "metadata": {},
   "source": [
    "Start by downloading the repository of (english) books. This is done in bash. Only tested on Ubuntu, but mac should work the same\n",
    "\n",
    "```\n",
    "wget -m -H -nd \"http://www.gutenberg.org/robot/harvest?filetypes[]=txt&langs[]=en\"\n",
    "\n",
    "                http://www.gutenberg.org/robot/harvest?offset=40532&filetypes[]=txt&langs[]=en\n",
    "```\n",
    "Takes a few hours to run, and is stored in a folder called rawContent. \n",
    "This is then copied to another folder, and we can start to clean up the mess\n",
    "\n",
    "First we delete some dublications of the same books:\n",
    "```\n",
    "ls | grep \"\\-8.zip\" | xargs rm\n",
    "ls | grep \"\\-0.zip\" | xargs rm\n",
    "```\n",
    "We can then unzip the files, and remove the zip files\n",
    "```\n",
    "unzip \"*zip\"\n",
    "rm *.zip\n",
    "```\n",
    "\n",
    "Next we take care of some nested foldering\n",
    "```\n",
    "mv */*.txt ./\n",
    "```\n",
    "And finally, we remove all rubbish that isn't a real book:\n",
    "\n",
    "```\n",
    "ls | grep -v \"\\.txt\" | xargs rm -rf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-tucson",
   "metadata": {},
   "source": [
    "### Data Cleansing\n",
    "As the data is not given in a computer-friendly format, a lot of string operations are needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-enterprise",
   "metadata": {},
   "source": [
    "#### Read a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "realistic-cricket",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T10:29:40.361185Z",
     "start_time": "2021-01-26T10:29:40.351393Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "    file = open(file_name, encoding=\"ISO-8859-1\")\n",
    "    file_content = file.read()\n",
    "\n",
    "    lines = file_content.splitlines()\n",
    "    sep = str(os.linesep)\n",
    "\n",
    "    # Initialize results for single book\n",
    "    content_lines = []\n",
    "    i = 0\n",
    "    footer_found = False\n",
    "    ignore_section = False\n",
    "\n",
    "    title = \"\"\n",
    "    author = \"\"\n",
    "    date = \"\"\n",
    "    language = \"\"\n",
    "    encoding = \"\"\n",
    "    year = 0\n",
    "\n",
    "    # Reset flags for each book\n",
    "    title_found = False\n",
    "    author_found = False\n",
    "    date_found = False\n",
    "    language_found = False\n",
    "    encoding_found = False\n",
    "\n",
    "    for line in lines:\n",
    "            reset = False\n",
    "\n",
    "            #print(line)\n",
    "            if i <= 600:\n",
    "                # Shamelessly stolen\n",
    "                if any(line.startswith(token) for token in TEXT_START_MARKERS):\n",
    "                    reset = True\n",
    "\n",
    "                # Extract Metadata\n",
    "                if title_found == False:\n",
    "                    if any(line.startswith(token) for token in TITLE_MARKERS):\n",
    "                        title_found = True\n",
    "                        title = line\n",
    "                if author_found == False:\n",
    "                    if any(line.startswith(token) for token in AUTHOR_MARKERS):\n",
    "                        author_found = True\n",
    "                        author = line\n",
    "                if date_found == False:\n",
    "                    if any(line.startswith(token) for token in DATE_MARKERS):\n",
    "                        date_found = True\n",
    "                        date = line\n",
    "                        year = int(re.findall(r'\\d{4}', date)[0])\n",
    "                if language_found == False:\n",
    "                    if any(line.startswith(token) for token in LANGUAGE_MARKERS):\n",
    "                        language_found = True\n",
    "                        language = line\n",
    "                if encoding_found == False:\n",
    "                    if any(line.startswith(token) for token in ENCODING_MARKERS):\n",
    "                        encoding_found = True\n",
    "                        encoding = line\n",
    "\n",
    "                # More theft from above\n",
    "                if reset:\n",
    "                    content_lines = []\n",
    "                    continue\n",
    "\n",
    "            # I feel like a criminal by now. Guess what? Also stolen\n",
    "            if i >= 100:\n",
    "                if any(line.startswith(token) for token in TEXT_END_MARKERS):\n",
    "                    footer_found = True\n",
    "\n",
    "                if footer_found:\n",
    "                    break\n",
    "\n",
    "            if any(line.startswith(token) for token in LEGALESE_START_MARKERS):\n",
    "                ignore_section = True\n",
    "                continue\n",
    "            elif any(line.startswith(token) for token in LEGALESE_END_MARKERS):\n",
    "                ignore_section = False\n",
    "                continue\n",
    "\n",
    "            if not ignore_section:\n",
    "                if line != \"\": # Screw the blank lines\n",
    "                    content_lines.append(line.rstrip(sep))\n",
    "                i += 1\n",
    "\n",
    "            sep.join(content_lines)\n",
    "\n",
    "    # Do more cleaning\n",
    "    for token in TITLE_MARKERS:\n",
    "        title = title.replace(token, '').lstrip().rstrip()\n",
    "    for token in AUTHOR_MARKERS:\n",
    "        author = author.replace(token, '').lstrip().rstrip()\n",
    "    for token in LANGUAGE_MARKERS:\n",
    "        language = language.replace(token, '').lstrip().rstrip()\n",
    "    for token in DATE_MARKERS:\n",
    "        date = date.replace(token, '').lstrip().rstrip()\n",
    "    for token in ENCODING_MARKERS:\n",
    "        encoding = encoding.replace(token, '').lstrip().rstrip()\n",
    "    return title, author, date, year, language, encoding, content_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-coalition",
   "metadata": {},
   "source": [
    "#### Return list of all words\n",
    "Currently quite an empty function. However, I assume that some cleaning of cases etc. will be done here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "square-pontiac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T10:29:40.372788Z",
     "start_time": "2021-01-26T10:29:40.362123Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_words(content_lines):\n",
    "    all_text_lower = \" \".join(content_lines).lower()\n",
    "    words = re.findall(r'(\\b[A-Za-z][a-z]{2,9}\\b)', all_text_lower)\n",
    "\n",
    "    # Do more cleansing. E.g. cases and stuff\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-hazard",
   "metadata": {},
   "source": [
    "## Statistics\n",
    "We start by doing some exploratory data analysis, to see how well our scraping works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-mexican",
   "metadata": {},
   "source": [
    "### First attempt\n",
    "Trying a simple word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "color-edition",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T10:29:40.385821Z",
     "start_time": "2021-01-26T10:29:40.374859Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_word_frequencies(words):\n",
    "    frequency = {}\n",
    "    for word in words:\n",
    "        count = frequency.get(word,0)\n",
    "        frequency[word] = count + 1\n",
    "\n",
    "    word_count = len(words)\n",
    "    unique_word_count = 0\n",
    "    word_list = []\n",
    "    word_list_count = []\n",
    "    for key, value in reversed(sorted(frequency.items(), key = itemgetter(1))):\n",
    "        word_list.append(key)\n",
    "        word_list_count.append(value)\n",
    "        unique_word_count = unique_word_count + 1\n",
    "    \n",
    "    word_list_freq = [freq / word_count for freq in word_list_count]\n",
    "    \n",
    "    word_freq = pd.DataFrame(list(zip(word_list, word_list_count, word_list_freq))\n",
    "                             , columns = ['Word', 'count', 'freq'])\n",
    "    \n",
    "    word_freq['rank'] = word_freq['count'].rank(ascending = False, method=\"dense\")\n",
    "\n",
    "    return(word_freq, unique_word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-objective",
   "metadata": {},
   "source": [
    "### Read all files, and do preprocessing\n",
    "Well... Only ten files currently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "married-closing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T10:29:44.707408Z",
     "start_time": "2021-01-26T10:29:40.388318Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get all filenames\n",
    "files = [f for f in listdir(file_path) if isfile(join(file_path, f))]\n",
    "files = list(filter(lambda file: file[0].isdigit(), files))\n",
    "random.shuffle(files)\n",
    "\n",
    "\n",
    "# Do only subset\n",
    "files = files[0:10]\n",
    "\n",
    "list_of_file = []\n",
    "list_of_title = []\n",
    "list_of_author = []\n",
    "list_of_date = []\n",
    "list_of_year = []\n",
    "list_of_language = []\n",
    "list_of_encoding = []\n",
    "list_of_word_count = []\n",
    "list_of_unique_word_count = []\n",
    "list_of_word_frequencies = []\n",
    "iter_ = 0\n",
    "\n",
    "for file in files:\n",
    "    # Read in basic information from file\n",
    "    title, author, date, year, language, encoding, content_lines = read_file(file_path + \"/\" + file)\n",
    "    line_count = len(content_lines)\n",
    "\n",
    "    # Not sure if we want this for later:\n",
    "    #content_all = \" \".join(content_lines)\n",
    "    \n",
    "    # Split into words (and do various cleaning)\n",
    "    words = get_words(content_lines)\n",
    "    word_count = len(words)\n",
    "\n",
    "    # First analysis, but should do something proper\n",
    "    word_frequencies_table, unique_word_count = get_word_frequencies(words)\n",
    "    \n",
    "    # Append to results\n",
    "    list_of_file.append(file)\n",
    "    list_of_title.append(title)\n",
    "    list_of_author.append(author)\n",
    "    list_of_date.append(date)\n",
    "    list_of_year.append(year)\n",
    "    list_of_language.append(language)\n",
    "    list_of_encoding.append(encoding)\n",
    "    list_of_word_count.append(word_count)\n",
    "    list_of_unique_word_count.append(unique_word_count)\n",
    "    list_of_word_frequencies.append(word_frequencies_table)\n",
    "    \n",
    "    \n",
    "    # Show basic information\n",
    "    #print(iter_)\n",
    "    iter_ = iter_ + 1\n",
    "    #print(\"################################\")\n",
    "    #print(\"################################\")\n",
    "    #print(\"Filename: \" + str(file))\n",
    "    #print(\"Title: \" + str(title))\n",
    "    #print(\"Author(s): \" + str(author))\n",
    "    #print(\"Date: \" + str(date))\n",
    "    #print(\"Year: \" + str(year))\n",
    "    #print(\"Language: \" + str(language))\n",
    "    #print(\"Encoding: \" + str(encoding))\n",
    "    #print(\"################################\")\n",
    "    #print(\"Words in book: \" + str(word_count))\n",
    "    #print(\"Unique words in book: \" + str(unique_word_count))\n",
    "    #print(\"################################\")\n",
    "    #print(word_frequencies_table)\n",
    "\n",
    "# Feel free to change to dict? list? separate files?\n",
    "## nested dataframes works, but looks super ungly when printing\n",
    "### Fuck it - This is tooo useless killing it again\n",
    "#all_res = pd.DataFrame(list(zip(list_of_file\n",
    "#                                , list_of_title\n",
    "#                                , list_of_author\n",
    "#                                , list_of_date\n",
    "#                                , list_of_language\n",
    "#                                , list_of_encoding\n",
    "#                                , list_of_word_count\n",
    "#                                , list_of_unique_word_count\n",
    "#                                , list_of_word_frequencies\n",
    "#                                ))\n",
    "#                             , columns = ['file'\n",
    "#                                          , 'title'\n",
    "#                                          , 'author'\n",
    "#                                          , 'date'\n",
    "#                                          , 'language'\n",
    "#                                          , 'encoding'\n",
    "#                                          , 'word_count'\n",
    "#                                          , 'unique_word_count'\n",
    "#                                          , 'word_frequencies'\n",
    "#                                         ]\n",
    "#                      )\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-gilbert",
   "metadata": {},
   "source": [
    "### Compare Word ranking between titles\n",
    "This is our first attemt at seeing how the ranking of words change between titles. Idea is to see that the zipf-distribution changes as time passes buy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "nominated-christian",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T10:29:47.186168Z",
     "start_time": "2021-01-26T10:29:44.708708Z"
    }
   },
   "outputs": [],
   "source": [
    "list_count= []\n",
    "list_freq = []\n",
    "list_rank = []\n",
    "\n",
    "\n",
    "col_names = list_of_title.copy()\n",
    "col_names.insert(0,'Word')\n",
    "\n",
    "for df in list_of_word_frequencies:\n",
    "    list_count.append(df[['Word', 'count']])\n",
    "    list_freq.append(df[['Word', 'freq']])\n",
    "    list_rank.append(df[['Word', 'rank']])\n",
    "    \n",
    "df_count = reduce(lambda left, right: pd.merge(left, right, on=\"Word\", how='outer'), list_count)\n",
    "df_count.columns = col_names\n",
    "df_count['Sum'] = df_count.drop('Word', axis=1).apply(lambda x: x.sum(), axis=1)\n",
    "df_count = df_count.sort_values(ascending = False, by=['Sum'])\n",
    "\n",
    "df_freq = reduce(lambda left, right: pd.merge(left, right, on=\"Word\", how='outer'), list_freq)\n",
    "df_freq.columns = col_names\n",
    "df_freq['Avg'] = df_freq.drop('Word', axis=1).apply(lambda x: x.mean(), axis=1)\n",
    "df_freq = df_freq.sort_values(ascending = False, by=['Avg'])\n",
    "\n",
    "df_rank = reduce(lambda left, right: pd.merge(left, right, on=\"Word\", how='outer'), list_rank)\n",
    "df_rank.columns = col_names\n",
    "df_rank['Avg'] = df_rank.drop('Word', axis=1).apply(lambda x: x.mean(), axis=1)\n",
    "df_rank = df_rank.sort_values(by=['Avg'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "interesting-jenny",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T10:29:47.224922Z",
     "start_time": "2021-01-26T10:29:47.187130Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>A Connecticut Yankee in King Arthur's Court, Part 2.</th>\n",
       "      <th>The Journal of the Debates in the Convention which Framed the Constitution of the United States</th>\n",
       "      <th>The Myth of Hiawatha, and Other Oral Legends, Mythologic and Allegoric, of the North American Indians</th>\n",
       "      <th>A Song of a Single Note</th>\n",
       "      <th>Christian's Mistake</th>\n",
       "      <th>The Head of The Family</th>\n",
       "      <th>The O'Ruddy</th>\n",
       "      <th>Heroes of the Middle West</th>\n",
       "      <th>At the Councillor's</th>\n",
       "      <th>The Industries of Animals</th>\n",
       "      <th>Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14241</th>\n",
       "      <td>letts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>that</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>was</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>7.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>with</td>\n",
       "      <td>9.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>his</td>\n",
       "      <td>10.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>for</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>not</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>but</td>\n",
       "      <td>7.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>13.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>had</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>15.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14242</th>\n",
       "      <td>widden</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>this</td>\n",
       "      <td>19.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>they</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>have</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17011</th>\n",
       "      <td>flora</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14243</th>\n",
       "      <td>betty</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16227</th>\n",
       "      <td>salle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>all</td>\n",
       "      <td>12.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>19.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>from</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16228</th>\n",
       "      <td>tonty</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>which</td>\n",
       "      <td>34.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>22.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16229</th>\n",
       "      <td>illinois</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>him</td>\n",
       "      <td>16.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14327</th>\n",
       "      <td>paddy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>one</td>\n",
       "      <td>16.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>24.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10647</th>\n",
       "      <td>neil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>you</td>\n",
       "      <td>8.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2321</th>\n",
       "      <td>gov</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>would</td>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>27.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word  A Connecticut Yankee in King Arthur's Court, Part 2.  \\\n",
       "1           the                                                2.0      \n",
       "0           and                                                1.0      \n",
       "14241     letts                                                NaN      \n",
       "3          that                                                4.0      \n",
       "2           was                                                3.0      \n",
       "8          with                                                9.0      \n",
       "9           his                                               10.0      \n",
       "4           for                                                5.0      \n",
       "16          not                                               15.0      \n",
       "6           but                                                7.0      \n",
       "5           had                                                6.0      \n",
       "14242    widden                                                NaN      \n",
       "23         this                                               19.0      \n",
       "10         they                                               11.0      \n",
       "13         have                                               14.0      \n",
       "17011     flora                                                NaN      \n",
       "14243     betty                                                NaN      \n",
       "16227     salle                                                NaN      \n",
       "11          all                                               12.0      \n",
       "20         from                                               17.0      \n",
       "16228     tonty                                                NaN      \n",
       "45        which                                               34.0      \n",
       "16229  illinois                                                NaN      \n",
       "17          him                                               16.0      \n",
       "14327     paddy                                                NaN      \n",
       "18          one                                               16.0      \n",
       "10647      neil                                                NaN      \n",
       "7           you                                                8.0      \n",
       "2321        gov                                                NaN      \n",
       "19        would                                               16.0      \n",
       "\n",
       "       The Journal of the Debates in the Convention which Framed the Constitution of the United States  \\\n",
       "1                                                    1.0                                                 \n",
       "0                                                    2.0                                                 \n",
       "14241                                                NaN                                                 \n",
       "3                                                    3.0                                                 \n",
       "2                                                    4.0                                                 \n",
       "8                                                   15.0                                                 \n",
       "9                                                   23.0                                                 \n",
       "4                                                    6.0                                                 \n",
       "16                                                   7.0                                                 \n",
       "6                                                   19.0                                                 \n",
       "5                                                   20.0                                                 \n",
       "14242                                                NaN                                                 \n",
       "23                                                  12.0                                                 \n",
       "10                                                  13.0                                                 \n",
       "13                                                  16.0                                                 \n",
       "17011                                                NaN                                                 \n",
       "14243                                                NaN                                                 \n",
       "16227                                                NaN                                                 \n",
       "11                                                  21.0                                                 \n",
       "20                                                   9.0                                                 \n",
       "16228                                                NaN                                                 \n",
       "45                                                  10.0                                                 \n",
       "16229                                                NaN                                                 \n",
       "17                                                  75.0                                                 \n",
       "14327                                                NaN                                                 \n",
       "18                                                  25.0                                                 \n",
       "10647                                                NaN                                                 \n",
       "7                                                  155.0                                                 \n",
       "2321                                                27.0                                                 \n",
       "19                                                   8.0                                                 \n",
       "\n",
       "       The Myth of Hiawatha, and Other Oral Legends, Mythologic and Allegoric, of the North American Indians  \\\n",
       "1                                                    1.0                                                       \n",
       "0                                                    2.0                                                       \n",
       "14241                                                NaN                                                       \n",
       "3                                                    6.0                                                       \n",
       "2                                                    4.0                                                       \n",
       "8                                                    8.0                                                       \n",
       "9                                                    3.0                                                       \n",
       "4                                                   12.0                                                       \n",
       "16                                                  15.0                                                       \n",
       "6                                                   11.0                                                       \n",
       "5                                                   10.0                                                       \n",
       "14242                                                NaN                                                       \n",
       "23                                                  16.0                                                       \n",
       "10                                                   5.0                                                       \n",
       "13                                                  30.0                                                       \n",
       "17011                                                NaN                                                       \n",
       "14243                                                NaN                                                       \n",
       "16227                                                NaN                                                       \n",
       "11                                                  21.0                                                       \n",
       "20                                                  19.0                                                       \n",
       "16228                                                NaN                                                       \n",
       "45                                                  18.0                                                       \n",
       "16229                                                NaN                                                       \n",
       "17                                                   7.0                                                       \n",
       "14327                                                NaN                                                       \n",
       "18                                                  23.0                                                       \n",
       "10647                                                NaN                                                       \n",
       "7                                                    9.0                                                       \n",
       "2321                                                 NaN                                                       \n",
       "19                                                  35.0                                                       \n",
       "\n",
       "       A Song of a Single Note  Christian's Mistake  The Head of The Family  \\\n",
       "1                          1.0                  1.0                     1.0   \n",
       "0                          2.0                  2.0                     2.0   \n",
       "14241                      NaN                  NaN                     5.0   \n",
       "3                          7.0                  6.0                    10.0   \n",
       "2                          3.0                  5.0                    11.0   \n",
       "8                         11.0                 11.0                     8.0   \n",
       "9                          8.0                 14.0                     4.0   \n",
       "4                         10.0                 10.0                    13.0   \n",
       "16                         9.0                  8.0                    25.0   \n",
       "6                         13.0                 12.0                    14.0   \n",
       "5                         12.0                  7.0                    15.0   \n",
       "14242                      NaN                  NaN                    16.0   \n",
       "23                        21.0                 20.0                    26.0   \n",
       "10                        17.0                 33.0                    28.0   \n",
       "13                        14.0                 18.0                    22.0   \n",
       "17011                      NaN                  NaN                     NaN   \n",
       "14243                      NaN                  NaN                    19.0   \n",
       "16227                      NaN                  NaN                     NaN   \n",
       "11                        20.0                 15.0                    18.0   \n",
       "20                        35.0                 35.0                    27.0   \n",
       "16228                      NaN                  NaN                     NaN   \n",
       "45                        53.0                 13.0                    25.0   \n",
       "16229                      NaN                  NaN                     NaN   \n",
       "17                        16.0                 19.0                    12.0   \n",
       "14327                      NaN                  NaN                     NaN   \n",
       "18                        26.0                 23.0                    29.0   \n",
       "10647                     25.0                  NaN                     NaN   \n",
       "7                          5.0                  9.0                     6.0   \n",
       "2321                       NaN                  NaN                     NaN   \n",
       "19                        19.0                 28.0                    26.0   \n",
       "\n",
       "       The O'Ruddy  Heroes of the Middle West  At the Councillor's  \\\n",
       "1              1.0                        1.0                  1.0   \n",
       "0              2.0                        2.0                  2.0   \n",
       "14241          NaN                        NaN                  NaN   \n",
       "3              5.0                        7.0                  7.0   \n",
       "2              4.0                        3.0                  5.0   \n",
       "8              6.0                        4.0                  8.0   \n",
       "9              9.0                        3.0                 10.0   \n",
       "4              8.0                       11.0                 12.0   \n",
       "16            13.0                       16.0                 11.0   \n",
       "6             10.0                       13.0                 15.0   \n",
       "5             11.0                        6.0                  6.0   \n",
       "14242          NaN                        NaN                  NaN   \n",
       "23            17.0                       14.0                 18.0   \n",
       "10            21.0                        5.0                 44.0   \n",
       "13            12.0                       25.0                 16.0   \n",
       "17011          NaN                        NaN                 19.0   \n",
       "14243          NaN                        NaN                  NaN   \n",
       "16227          NaN                       19.0                  NaN   \n",
       "11            23.0                       19.0                 22.0   \n",
       "20            22.0                       10.0                 13.0   \n",
       "16228          NaN                       22.0                  NaN   \n",
       "45            34.0                       15.0                 20.0   \n",
       "16229          NaN                       23.0                  NaN   \n",
       "17            14.0                       12.0                 26.0   \n",
       "14327         24.0                        NaN                  NaN   \n",
       "18            29.0                       23.0                 29.0   \n",
       "10647          NaN                        NaN                  NaN   \n",
       "7              3.0                       33.0                  9.0   \n",
       "2321           NaN                        NaN                  NaN   \n",
       "19            15.0                       36.0                 27.0   \n",
       "\n",
       "       The Industries of Animals        Avg  \n",
       "1                            1.0   1.100000  \n",
       "0                            2.0   1.900000  \n",
       "14241                        NaN   5.000000  \n",
       "3                            9.0   6.400000  \n",
       "2                           32.0   7.400000  \n",
       "8                            4.0   8.400000  \n",
       "9                           12.0   9.600000  \n",
       "4                           10.0   9.700000  \n",
       "16                          11.0  13.000000  \n",
       "6                           19.0  13.300000  \n",
       "5                           63.0  15.600000  \n",
       "14242                        NaN  16.000000  \n",
       "23                           7.0  17.000000  \n",
       "10                           5.0  18.200000  \n",
       "13                          17.0  18.400000  \n",
       "17011                        NaN  19.000000  \n",
       "14243                        NaN  19.000000  \n",
       "16227                        NaN  19.000000  \n",
       "11                          23.0  19.400000  \n",
       "20                          13.0  20.000000  \n",
       "16228                        NaN  22.000000  \n",
       "45                           3.0  22.500000  \n",
       "16229                        NaN  23.000000  \n",
       "17                          33.0  23.000000  \n",
       "14327                        NaN  24.000000  \n",
       "18                          21.0  24.400000  \n",
       "10647                        NaN  25.000000  \n",
       "7                            NaN  26.333333  \n",
       "2321                         NaN  27.000000  \n",
       "19                          61.0  27.100000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rank.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "disciplinary-bridal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T10:29:47.241918Z",
     "start_time": "2021-01-26T10:29:47.225922Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>A Connecticut Yankee in King Arthur's Court, Part 2.</th>\n",
       "      <th>The Journal of the Debates in the Convention which Framed the Constitution of the United States</th>\n",
       "      <th>The Myth of Hiawatha, and Other Oral Legends, Mythologic and Allegoric, of the North American Indians</th>\n",
       "      <th>A Song of a Single Note</th>\n",
       "      <th>Christian's Mistake</th>\n",
       "      <th>The Head of The Family</th>\n",
       "      <th>The O'Ruddy</th>\n",
       "      <th>Heroes of the Middle West</th>\n",
       "      <th>At the Councillor's</th>\n",
       "      <th>The Industries of Animals</th>\n",
       "      <th>Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the</td>\n",
       "      <td>0.062928</td>\n",
       "      <td>0.130677</td>\n",
       "      <td>0.087913</td>\n",
       "      <td>0.059755</td>\n",
       "      <td>0.046411</td>\n",
       "      <td>0.050736</td>\n",
       "      <td>0.066958</td>\n",
       "      <td>0.095773</td>\n",
       "      <td>0.082373</td>\n",
       "      <td>0.103202</td>\n",
       "      <td>0.078673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and</td>\n",
       "      <td>0.065669</td>\n",
       "      <td>0.021019</td>\n",
       "      <td>0.051288</td>\n",
       "      <td>0.055466</td>\n",
       "      <td>0.040183</td>\n",
       "      <td>0.046154</td>\n",
       "      <td>0.040690</td>\n",
       "      <td>0.044907</td>\n",
       "      <td>0.034620</td>\n",
       "      <td>0.036773</td>\n",
       "      <td>0.043677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14241</th>\n",
       "      <td>letts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.022259</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.022259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>was</td>\n",
       "      <td>0.025354</td>\n",
       "      <td>0.013637</td>\n",
       "      <td>0.014473</td>\n",
       "      <td>0.020376</td>\n",
       "      <td>0.018870</td>\n",
       "      <td>0.012111</td>\n",
       "      <td>0.018421</td>\n",
       "      <td>0.014508</td>\n",
       "      <td>0.014132</td>\n",
       "      <td>0.002552</td>\n",
       "      <td>0.015444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>that</td>\n",
       "      <td>0.022613</td>\n",
       "      <td>0.020911</td>\n",
       "      <td>0.010974</td>\n",
       "      <td>0.013155</td>\n",
       "      <td>0.015589</td>\n",
       "      <td>0.012766</td>\n",
       "      <td>0.017729</td>\n",
       "      <td>0.009629</td>\n",
       "      <td>0.013584</td>\n",
       "      <td>0.009221</td>\n",
       "      <td>0.014617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>his</td>\n",
       "      <td>0.008566</td>\n",
       "      <td>0.004799</td>\n",
       "      <td>0.022070</td>\n",
       "      <td>0.013026</td>\n",
       "      <td>0.008186</td>\n",
       "      <td>0.023895</td>\n",
       "      <td>0.011349</td>\n",
       "      <td>0.014508</td>\n",
       "      <td>0.010728</td>\n",
       "      <td>0.007451</td>\n",
       "      <td>0.012458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>you</td>\n",
       "      <td>0.010735</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.009943</td>\n",
       "      <td>0.016058</td>\n",
       "      <td>0.011971</td>\n",
       "      <td>0.019967</td>\n",
       "      <td>0.020995</td>\n",
       "      <td>0.002591</td>\n",
       "      <td>0.011801</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>with</td>\n",
       "      <td>0.009822</td>\n",
       "      <td>0.006677</td>\n",
       "      <td>0.010119</td>\n",
       "      <td>0.010724</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.016039</td>\n",
       "      <td>0.012567</td>\n",
       "      <td>0.013299</td>\n",
       "      <td>0.013015</td>\n",
       "      <td>0.012146</td>\n",
       "      <td>0.011461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>for</td>\n",
       "      <td>0.014162</td>\n",
       "      <td>0.012399</td>\n",
       "      <td>0.008288</td>\n",
       "      <td>0.011668</td>\n",
       "      <td>0.010535</td>\n",
       "      <td>0.008511</td>\n",
       "      <td>0.011598</td>\n",
       "      <td>0.007297</td>\n",
       "      <td>0.008956</td>\n",
       "      <td>0.008141</td>\n",
       "      <td>0.010155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>her</td>\n",
       "      <td>0.001485</td>\n",
       "      <td>0.000608</td>\n",
       "      <td>0.006877</td>\n",
       "      <td>0.017759</td>\n",
       "      <td>0.023868</td>\n",
       "      <td>0.015057</td>\n",
       "      <td>0.004138</td>\n",
       "      <td>0.001079</td>\n",
       "      <td>0.028038</td>\n",
       "      <td>0.001229</td>\n",
       "      <td>0.010014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>had</td>\n",
       "      <td>0.013248</td>\n",
       "      <td>0.005667</td>\n",
       "      <td>0.008926</td>\n",
       "      <td>0.010453</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>0.007201</td>\n",
       "      <td>0.009965</td>\n",
       "      <td>0.010104</td>\n",
       "      <td>0.014013</td>\n",
       "      <td>0.001472</td>\n",
       "      <td>0.009470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>she</td>\n",
       "      <td>0.002855</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.007067</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.024669</td>\n",
       "      <td>0.007529</td>\n",
       "      <td>0.004014</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>0.021369</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.008450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>not</td>\n",
       "      <td>0.006852</td>\n",
       "      <td>0.011986</td>\n",
       "      <td>0.006538</td>\n",
       "      <td>0.011882</td>\n",
       "      <td>0.012661</td>\n",
       "      <td>0.003273</td>\n",
       "      <td>0.008415</td>\n",
       "      <td>0.005397</td>\n",
       "      <td>0.009117</td>\n",
       "      <td>0.008066</td>\n",
       "      <td>0.008419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>but</td>\n",
       "      <td>0.011535</td>\n",
       "      <td>0.005787</td>\n",
       "      <td>0.008641</td>\n",
       "      <td>0.007607</td>\n",
       "      <td>0.009193</td>\n",
       "      <td>0.007529</td>\n",
       "      <td>0.010601</td>\n",
       "      <td>0.005916</td>\n",
       "      <td>0.006464</td>\n",
       "      <td>0.005011</td>\n",
       "      <td>0.007828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>they</td>\n",
       "      <td>0.008451</td>\n",
       "      <td>0.007285</td>\n",
       "      <td>0.012588</td>\n",
       "      <td>0.006363</td>\n",
       "      <td>0.003767</td>\n",
       "      <td>0.002291</td>\n",
       "      <td>0.005342</td>\n",
       "      <td>0.011486</td>\n",
       "      <td>0.002352</td>\n",
       "      <td>0.010842</td>\n",
       "      <td>0.007077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14242</th>\n",
       "      <td>widden</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>said</td>\n",
       "      <td>0.002627</td>\n",
       "      <td>0.001585</td>\n",
       "      <td>0.005263</td>\n",
       "      <td>0.004747</td>\n",
       "      <td>0.004251</td>\n",
       "      <td>0.025205</td>\n",
       "      <td>0.011667</td>\n",
       "      <td>0.002159</td>\n",
       "      <td>0.003715</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.006146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>him</td>\n",
       "      <td>0.006167</td>\n",
       "      <td>0.001618</td>\n",
       "      <td>0.010621</td>\n",
       "      <td>0.007107</td>\n",
       "      <td>0.005482</td>\n",
       "      <td>0.008838</td>\n",
       "      <td>0.007501</td>\n",
       "      <td>0.006995</td>\n",
       "      <td>0.003930</td>\n",
       "      <td>0.002515</td>\n",
       "      <td>0.006077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>have</td>\n",
       "      <td>0.007081</td>\n",
       "      <td>0.006341</td>\n",
       "      <td>0.004097</td>\n",
       "      <td>0.007378</td>\n",
       "      <td>0.006918</td>\n",
       "      <td>0.004255</td>\n",
       "      <td>0.008525</td>\n",
       "      <td>0.003368</td>\n",
       "      <td>0.006110</td>\n",
       "      <td>0.005309</td>\n",
       "      <td>0.005938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>this</td>\n",
       "      <td>0.004911</td>\n",
       "      <td>0.007329</td>\n",
       "      <td>0.006145</td>\n",
       "      <td>0.005048</td>\n",
       "      <td>0.005296</td>\n",
       "      <td>0.002946</td>\n",
       "      <td>0.006311</td>\n",
       "      <td>0.005657</td>\n",
       "      <td>0.005187</td>\n",
       "      <td>0.010227</td>\n",
       "      <td>0.005905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word  A Connecticut Yankee in King Arthur's Court, Part 2.  \\\n",
       "1         the                                           0.062928      \n",
       "0         and                                           0.065669      \n",
       "14241   letts                                                NaN      \n",
       "2         was                                           0.025354      \n",
       "3        that                                           0.022613      \n",
       "9         his                                           0.008566      \n",
       "7         you                                           0.010735      \n",
       "8        with                                           0.009822      \n",
       "4         for                                           0.014162      \n",
       "90        her                                           0.001485      \n",
       "5         had                                           0.013248      \n",
       "39        she                                           0.002855      \n",
       "16        not                                           0.006852      \n",
       "6         but                                           0.011535      \n",
       "10       they                                           0.008451      \n",
       "14242  widden                                                NaN      \n",
       "42       said                                           0.002627      \n",
       "17        him                                           0.006167      \n",
       "13       have                                           0.007081      \n",
       "23       this                                           0.004911      \n",
       "\n",
       "       The Journal of the Debates in the Convention which Framed the Constitution of the United States  \\\n",
       "1                                               0.130677                                                 \n",
       "0                                               0.021019                                                 \n",
       "14241                                                NaN                                                 \n",
       "2                                               0.013637                                                 \n",
       "3                                               0.020911                                                 \n",
       "9                                               0.004799                                                 \n",
       "7                                               0.000489                                                 \n",
       "8                                               0.006677                                                 \n",
       "4                                               0.012399                                                 \n",
       "90                                              0.000608                                                 \n",
       "5                                               0.005667                                                 \n",
       "39                                              0.000391                                                 \n",
       "16                                              0.011986                                                 \n",
       "6                                               0.005787                                                 \n",
       "10                                              0.007285                                                 \n",
       "14242                                                NaN                                                 \n",
       "42                                              0.001585                                                 \n",
       "17                                              0.001618                                                 \n",
       "13                                              0.006341                                                 \n",
       "23                                              0.007329                                                 \n",
       "\n",
       "       The Myth of Hiawatha, and Other Oral Legends, Mythologic and Allegoric, of the North American Indians  \\\n",
       "1                                               0.087913                                                       \n",
       "0                                               0.051288                                                       \n",
       "14241                                                NaN                                                       \n",
       "2                                               0.014473                                                       \n",
       "3                                               0.010974                                                       \n",
       "9                                               0.022070                                                       \n",
       "7                                               0.009943                                                       \n",
       "8                                               0.010119                                                       \n",
       "4                                               0.008288                                                       \n",
       "90                                              0.006877                                                       \n",
       "5                                               0.008926                                                       \n",
       "39                                              0.007067                                                       \n",
       "16                                              0.006538                                                       \n",
       "6                                               0.008641                                                       \n",
       "10                                              0.012588                                                       \n",
       "14242                                                NaN                                                       \n",
       "42                                              0.005263                                                       \n",
       "17                                              0.010621                                                       \n",
       "13                                              0.004097                                                       \n",
       "23                                              0.006145                                                       \n",
       "\n",
       "       A Song of a Single Note  Christian's Mistake  The Head of The Family  \\\n",
       "1                     0.059755             0.046411                0.050736   \n",
       "0                     0.055466             0.040183                0.046154   \n",
       "14241                      NaN                  NaN                0.022259   \n",
       "2                     0.020376             0.018870                0.012111   \n",
       "3                     0.013155             0.015589                0.012766   \n",
       "9                     0.013026             0.008186                0.023895   \n",
       "7                     0.016058             0.011971                0.019967   \n",
       "8                     0.010724             0.010200                0.016039   \n",
       "4                     0.011668             0.010535                0.008511   \n",
       "90                    0.017759             0.023868                0.015057   \n",
       "5                     0.010453             0.013649                0.007201   \n",
       "39                    0.015000             0.024669                0.007529   \n",
       "16                    0.011882             0.012661                0.003273   \n",
       "6                     0.007607             0.009193                0.007529   \n",
       "10                    0.006363             0.003767                0.002291   \n",
       "14242                      NaN                  NaN                0.006219   \n",
       "42                    0.004747             0.004251                0.025205   \n",
       "17                    0.007107             0.005482                0.008838   \n",
       "13                    0.007378             0.006918                0.004255   \n",
       "23                    0.005048             0.005296                0.002946   \n",
       "\n",
       "       The O'Ruddy  Heroes of the Middle West  At the Councillor's  \\\n",
       "1         0.066958                   0.095773             0.082373   \n",
       "0         0.040690                   0.044907             0.034620   \n",
       "14241          NaN                        NaN                  NaN   \n",
       "2         0.018421                   0.014508             0.014132   \n",
       "3         0.017729                   0.009629             0.013584   \n",
       "9         0.011349                   0.014508             0.010728   \n",
       "7         0.020995                   0.002591             0.011801   \n",
       "8         0.012567                   0.013299             0.013015   \n",
       "4         0.011598                   0.007297             0.008956   \n",
       "90        0.004138                   0.001079             0.028038   \n",
       "5         0.009965                   0.010104             0.014013   \n",
       "39        0.004014                   0.000734             0.021369   \n",
       "16        0.008415                   0.005397             0.009117   \n",
       "6         0.010601                   0.005916             0.006464   \n",
       "10        0.005342                   0.011486             0.002352   \n",
       "14242          NaN                        NaN                  NaN   \n",
       "42        0.011667                   0.002159             0.003715   \n",
       "17        0.007501                   0.006995             0.003930   \n",
       "13        0.008525                   0.003368             0.006110   \n",
       "23        0.006311                   0.005657             0.005187   \n",
       "\n",
       "       The Industries of Animals       Avg  \n",
       "1                       0.103202  0.078673  \n",
       "0                       0.036773  0.043677  \n",
       "14241                        NaN  0.022259  \n",
       "2                       0.002552  0.015444  \n",
       "3                       0.009221  0.014617  \n",
       "9                       0.007451  0.012458  \n",
       "7                            NaN  0.011617  \n",
       "8                       0.012146  0.011461  \n",
       "4                       0.008141  0.010155  \n",
       "90                      0.001229  0.010014  \n",
       "5                       0.001472  0.009470  \n",
       "39                      0.000876  0.008450  \n",
       "16                      0.008066  0.008419  \n",
       "6                       0.005011  0.007828  \n",
       "10                      0.010842  0.007077  \n",
       "14242                        NaN  0.006219  \n",
       "42                      0.000242  0.006146  \n",
       "17                      0.002515  0.006077  \n",
       "13                      0.005309  0.005938  \n",
       "23                      0.010227  0.005905  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_freq['Avg'] = df_freq.drop('Word', axis=1).apply(lambda x: x.mean(), axis=1)\n",
    "df_freq = df_freq.sort_values(ascending = False, by=['Avg'])\n",
    "\n",
    "df_freq.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "innocent-decline",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T10:29:47.263161Z",
     "start_time": "2021-01-26T10:29:47.242759Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>A Connecticut Yankee in King Arthur's Court, Part 2.</th>\n",
       "      <th>The Journal of the Debates in the Convention which Framed the Constitution of the United States</th>\n",
       "      <th>The Myth of Hiawatha, and Other Oral Legends, Mythologic and Allegoric, of the North American Indians</th>\n",
       "      <th>A Song of a Single Note</th>\n",
       "      <th>Christian's Mistake</th>\n",
       "      <th>The Head of The Family</th>\n",
       "      <th>The O'Ruddy</th>\n",
       "      <th>Heroes of the Middle West</th>\n",
       "      <th>At the Councillor's</th>\n",
       "      <th>The Industries of Animals</th>\n",
       "      <th>Sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the</td>\n",
       "      <td>551.0</td>\n",
       "      <td>12036.0</td>\n",
       "      <td>6481.0</td>\n",
       "      <td>4179.0</td>\n",
       "      <td>2489.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>4838.0</td>\n",
       "      <td>2218.0</td>\n",
       "      <td>7671.0</td>\n",
       "      <td>5540.0</td>\n",
       "      <td>46158.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and</td>\n",
       "      <td>575.0</td>\n",
       "      <td>1936.0</td>\n",
       "      <td>3781.0</td>\n",
       "      <td>3879.0</td>\n",
       "      <td>2155.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>2940.0</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>3224.0</td>\n",
       "      <td>1974.0</td>\n",
       "      <td>21645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>was</td>\n",
       "      <td>222.0</td>\n",
       "      <td>1256.0</td>\n",
       "      <td>1067.0</td>\n",
       "      <td>1425.0</td>\n",
       "      <td>1012.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1331.0</td>\n",
       "      <td>336.0</td>\n",
       "      <td>1316.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>8139.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>that</td>\n",
       "      <td>198.0</td>\n",
       "      <td>1926.0</td>\n",
       "      <td>809.0</td>\n",
       "      <td>920.0</td>\n",
       "      <td>836.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1281.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>495.0</td>\n",
       "      <td>7992.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>her</td>\n",
       "      <td>13.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>507.0</td>\n",
       "      <td>1242.0</td>\n",
       "      <td>1280.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2611.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>6145.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>his</td>\n",
       "      <td>75.0</td>\n",
       "      <td>442.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>911.0</td>\n",
       "      <td>439.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>820.0</td>\n",
       "      <td>336.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>6122.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>with</td>\n",
       "      <td>86.0</td>\n",
       "      <td>615.0</td>\n",
       "      <td>746.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>547.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>908.0</td>\n",
       "      <td>308.0</td>\n",
       "      <td>1212.0</td>\n",
       "      <td>652.0</td>\n",
       "      <td>5873.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>for</td>\n",
       "      <td>124.0</td>\n",
       "      <td>1142.0</td>\n",
       "      <td>611.0</td>\n",
       "      <td>816.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>838.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>834.0</td>\n",
       "      <td>437.0</td>\n",
       "      <td>5562.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>you</td>\n",
       "      <td>94.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>733.0</td>\n",
       "      <td>1123.0</td>\n",
       "      <td>642.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1517.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1099.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5374.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>she</td>\n",
       "      <td>25.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>521.0</td>\n",
       "      <td>1049.0</td>\n",
       "      <td>1323.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>5321.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>not</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1104.0</td>\n",
       "      <td>482.0</td>\n",
       "      <td>831.0</td>\n",
       "      <td>679.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>608.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>849.0</td>\n",
       "      <td>433.0</td>\n",
       "      <td>5181.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>had</td>\n",
       "      <td>116.0</td>\n",
       "      <td>522.0</td>\n",
       "      <td>658.0</td>\n",
       "      <td>731.0</td>\n",
       "      <td>732.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>1305.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>5119.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>but</td>\n",
       "      <td>101.0</td>\n",
       "      <td>533.0</td>\n",
       "      <td>637.0</td>\n",
       "      <td>532.0</td>\n",
       "      <td>493.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>766.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>602.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>4093.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>they</td>\n",
       "      <td>74.0</td>\n",
       "      <td>671.0</td>\n",
       "      <td>928.0</td>\n",
       "      <td>445.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>386.0</td>\n",
       "      <td>266.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>3780.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>this</td>\n",
       "      <td>43.0</td>\n",
       "      <td>675.0</td>\n",
       "      <td>453.0</td>\n",
       "      <td>353.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>456.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>483.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>3436.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>have</td>\n",
       "      <td>62.0</td>\n",
       "      <td>584.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>516.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>616.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>569.0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>3396.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>which</td>\n",
       "      <td>22.0</td>\n",
       "      <td>733.0</td>\n",
       "      <td>405.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>448.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>423.0</td>\n",
       "      <td>697.0</td>\n",
       "      <td>3280.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>from</td>\n",
       "      <td>52.0</td>\n",
       "      <td>739.0</td>\n",
       "      <td>399.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>358.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>683.0</td>\n",
       "      <td>347.0</td>\n",
       "      <td>3184.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>him</td>\n",
       "      <td>54.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>783.0</td>\n",
       "      <td>497.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>542.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>366.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>3009.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>all</td>\n",
       "      <td>67.0</td>\n",
       "      <td>459.0</td>\n",
       "      <td>389.0</td>\n",
       "      <td>376.0</td>\n",
       "      <td>376.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>346.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>392.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>2732.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word  A Connecticut Yankee in King Arthur's Court, Part 2.  \\\n",
       "1     the                                              551.0      \n",
       "0     and                                              575.0      \n",
       "2     was                                              222.0      \n",
       "3    that                                              198.0      \n",
       "90    her                                               13.0      \n",
       "9     his                                               75.0      \n",
       "8    with                                               86.0      \n",
       "4     for                                              124.0      \n",
       "7     you                                               94.0      \n",
       "39    she                                               25.0      \n",
       "16    not                                               60.0      \n",
       "5     had                                              116.0      \n",
       "6     but                                              101.0      \n",
       "10   they                                               74.0      \n",
       "23   this                                               43.0      \n",
       "13   have                                               62.0      \n",
       "45  which                                               22.0      \n",
       "20   from                                               52.0      \n",
       "17    him                                               54.0      \n",
       "11    all                                               67.0      \n",
       "\n",
       "    The Journal of the Debates in the Convention which Framed the Constitution of the United States  \\\n",
       "1                                             12036.0                                                 \n",
       "0                                              1936.0                                                 \n",
       "2                                              1256.0                                                 \n",
       "3                                              1926.0                                                 \n",
       "90                                               56.0                                                 \n",
       "9                                               442.0                                                 \n",
       "8                                               615.0                                                 \n",
       "4                                              1142.0                                                 \n",
       "7                                                45.0                                                 \n",
       "39                                               36.0                                                 \n",
       "16                                             1104.0                                                 \n",
       "5                                               522.0                                                 \n",
       "6                                               533.0                                                 \n",
       "10                                              671.0                                                 \n",
       "23                                              675.0                                                 \n",
       "13                                              584.0                                                 \n",
       "45                                              733.0                                                 \n",
       "20                                              739.0                                                 \n",
       "17                                              149.0                                                 \n",
       "11                                              459.0                                                 \n",
       "\n",
       "    The Myth of Hiawatha, and Other Oral Legends, Mythologic and Allegoric, of the North American Indians  \\\n",
       "1                                              6481.0                                                       \n",
       "0                                              3781.0                                                       \n",
       "2                                              1067.0                                                       \n",
       "3                                               809.0                                                       \n",
       "90                                              507.0                                                       \n",
       "9                                              1627.0                                                       \n",
       "8                                               746.0                                                       \n",
       "4                                               611.0                                                       \n",
       "7                                               733.0                                                       \n",
       "39                                              521.0                                                       \n",
       "16                                              482.0                                                       \n",
       "5                                               658.0                                                       \n",
       "6                                               637.0                                                       \n",
       "10                                              928.0                                                       \n",
       "23                                              453.0                                                       \n",
       "13                                              302.0                                                       \n",
       "45                                              405.0                                                       \n",
       "20                                              399.0                                                       \n",
       "17                                              783.0                                                       \n",
       "11                                              389.0                                                       \n",
       "\n",
       "    A Song of a Single Note  Christian's Mistake  The Head of The Family  \\\n",
       "1                    4179.0               2489.0                   155.0   \n",
       "0                    3879.0               2155.0                   141.0   \n",
       "2                    1425.0               1012.0                    37.0   \n",
       "3                     920.0                836.0                    39.0   \n",
       "90                   1242.0               1280.0                    46.0   \n",
       "9                     911.0                439.0                    73.0   \n",
       "8                     750.0                547.0                    49.0   \n",
       "4                     816.0                565.0                    26.0   \n",
       "7                    1123.0                642.0                    61.0   \n",
       "39                   1049.0               1323.0                    23.0   \n",
       "16                    831.0                679.0                    10.0   \n",
       "5                     731.0                732.0                    22.0   \n",
       "6                     532.0                493.0                    23.0   \n",
       "10                    445.0                202.0                     7.0   \n",
       "23                    353.0                284.0                     9.0   \n",
       "13                    516.0                371.0                    13.0   \n",
       "45                    151.0                448.0                    10.0   \n",
       "20                    220.0                193.0                     8.0   \n",
       "17                    497.0                294.0                    27.0   \n",
       "11                    376.0                376.0                    17.0   \n",
       "\n",
       "    The O'Ruddy  Heroes of the Middle West  At the Councillor's  \\\n",
       "1        4838.0                     2218.0               7671.0   \n",
       "0        2940.0                     1040.0               3224.0   \n",
       "2        1331.0                      336.0               1316.0   \n",
       "3        1281.0                      223.0               1265.0   \n",
       "90        299.0                       25.0               2611.0   \n",
       "9         820.0                      336.0                999.0   \n",
       "8         908.0                      308.0               1212.0   \n",
       "4         838.0                      169.0                834.0   \n",
       "7        1517.0                       60.0               1099.0   \n",
       "39        290.0                       17.0               1990.0   \n",
       "16        608.0                      125.0                849.0   \n",
       "5         720.0                      234.0               1305.0   \n",
       "6         766.0                      137.0                602.0   \n",
       "10        386.0                      266.0                219.0   \n",
       "23        456.0                      131.0                483.0   \n",
       "13        616.0                       78.0                569.0   \n",
       "45        263.0                      128.0                423.0   \n",
       "20        358.0                      185.0                683.0   \n",
       "17        542.0                      162.0                366.0   \n",
       "11        346.0                      101.0                392.0   \n",
       "\n",
       "    The Industries of Animals      Sum  \n",
       "1                      5540.0  46158.0  \n",
       "0                      1974.0  21645.0  \n",
       "2                       137.0   8139.0  \n",
       "3                       495.0   7992.0  \n",
       "90                       66.0   6145.0  \n",
       "9                       400.0   6122.0  \n",
       "8                       652.0   5873.0  \n",
       "4                       437.0   5562.0  \n",
       "7                         NaN   5374.0  \n",
       "39                       47.0   5321.0  \n",
       "16                      433.0   5181.0  \n",
       "5                        79.0   5119.0  \n",
       "6                       269.0   4093.0  \n",
       "10                      582.0   3780.0  \n",
       "23                      549.0   3436.0  \n",
       "13                      285.0   3396.0  \n",
       "45                      697.0   3280.0  \n",
       "20                      347.0   3184.0  \n",
       "17                      135.0   3009.0  \n",
       "11                      209.0   2732.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_count.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-strategy",
   "metadata": {},
   "source": [
    "## Second testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-finish",
   "metadata": {},
   "source": [
    "This definately needs some proper refactoring, but Was curious whether we get anything decent from reading a bunch of random books in\n",
    "\n",
    "Requires an additional folder \"decades\" in the root directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "narrow-stomach",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T10:29:54.416494Z",
     "start_time": "2021-01-26T10:29:47.264088Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get all filenames\n",
    "files = [f for f in listdir(file_path) if isfile(join(file_path, f))]\n",
    "\n",
    "# Do only subset\n",
    "## Is done for 5000 files already, so set down to 20 to increase performance. 5000 books are currently stored in the file\n",
    "files = files[0:20]\n",
    "\n",
    "\n",
    "counter = 0\n",
    "for file in files:\n",
    "    counter = counter + 1\n",
    "    # Read in basic information from file\n",
    "    title, author, date, year, language, encoding, content_lines = read_file(file_path + \"/\" + file)\n",
    "    #line_count = len(content_lines)\n",
    "    decade = math.floor(year / 10) * 10\n",
    "    decade_file = \"decades/\" + str(decade) + \".txt\"\n",
    "    content_all = \" \".join(content_lines)\n",
    "    \n",
    "    if os.path.exists(decade_file):\n",
    "        append_write = 'a' # append if already exists\n",
    "    else:\n",
    "        append_write = 'w' # make a new file if not\n",
    "\n",
    "    fileWriter = open(decade_file,append_write)\n",
    "    fileWriter.write(content_all + '\\n')\n",
    "    fileWriter.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-delicious",
   "metadata": {},
   "source": [
    "### Read in from the decades files, and see the distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "formed-amazon",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T10:30:31.816303Z",
     "start_time": "2021-01-26T10:29:54.417453Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00.txt', '0.txt', '2010.txt', '2000.txt', '2020.txt', '1990.txt']\n",
      "2020.txt\n",
      "2010.txt\n",
      "2000.txt\n",
      "1990.txt\n",
      "00.txt\n",
      "0.txt\n"
     ]
    }
   ],
   "source": [
    "# Get all filenames\n",
    "files = [f for f in listdir(\"decades\") if isfile(join(\"decades\", f))]\n",
    "print(files)\n",
    "files.sort(reverse=True)\n",
    "\n",
    "\n",
    "col_names = []\n",
    "col_names.append(\"Word\")\n",
    "\n",
    "tables = []\n",
    "\n",
    "for file_name in files:\n",
    "    print(file_name)\n",
    "    \n",
    "    file = open(\"decades/\" + file_name, encoding=\"ISO-8859-1\")\n",
    "    file_content = file.read()\n",
    "    \n",
    "    # Split into words (and do various cleaning)\n",
    "    all_text_lower = file_content.lower()\n",
    "    words = re.findall(r'(\\b[A-Za-z][a-z]{2,9}\\b)', all_text_lower)\n",
    "\n",
    "    # First analysis, but should do something proper\n",
    "    word_frequencies_table, unique_word_count = get_word_frequencies(words)\n",
    "    tables.append(word_frequencies_table)\n",
    "    col_names.append(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-sociology",
   "metadata": {},
   "source": [
    "### Preliminary Conclusion\n",
    "We see that even though the books are quite old, no decade prior to 1990s is found.\n",
    "\n",
    "This is when we found out that the \"year\" that's registered in the dataset is the upload-date. \n",
    "\n",
    "Haven gotten this far, we however decided to see if we could find a pattern in this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-paper",
   "metadata": {},
   "source": [
    "### Compare ranking between upload-decades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "gentle-deposit",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T10:30:33.102091Z",
     "start_time": "2021-01-26T10:30:31.817456Z"
    }
   },
   "outputs": [],
   "source": [
    "list_count= []\n",
    "list_freq = []\n",
    "list_rank = []\n",
    "\n",
    "for df in tables:\n",
    "    #list_count.append(df[['Word', 'count']])\n",
    "    #list_freq.append(df[['Word', 'freq']])\n",
    "    list_rank.append(df[['Word', 'rank']])\n",
    "    \n",
    "#df_count = reduce(lambda left, right: pd.merge(left, right, on=\"Word\", how='outer'), list_count)\n",
    "#df_count.columns = col_names\n",
    "\n",
    "#df_freq = reduce(lambda left, right: pd.merge(left, right, on=\"Word\", how='outer'), list_freq)\n",
    "#df_freq.columns = col_names\n",
    "\n",
    "df_rank = reduce(lambda left, right: pd.merge(left, right, on=\"Word\", how='outer'), list_rank)\n",
    "df_rank.columns = col_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "defined-snowboard",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T10:30:33.139070Z",
     "start_time": "2021-01-26T10:30:33.103144Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>2020.txt</th>\n",
       "      <th>2010.txt</th>\n",
       "      <th>2000.txt</th>\n",
       "      <th>1990.txt</th>\n",
       "      <th>00.txt</th>\n",
       "      <th>0.txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>was</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>with</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>for</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>his</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>not</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>had</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>but</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>which</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>they</td>\n",
       "      <td>13.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>from</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>were</td>\n",
       "      <td>15.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>have</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>this</td>\n",
       "      <td>17.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>are</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>she</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>all</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>their</td>\n",
       "      <td>21.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>him</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>her</td>\n",
       "      <td>23.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>its</td>\n",
       "      <td>24.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>one</td>\n",
       "      <td>25.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>there</td>\n",
       "      <td>26.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>them</td>\n",
       "      <td>27.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>what</td>\n",
       "      <td>28.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>has</td>\n",
       "      <td>29.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>been</td>\n",
       "      <td>30.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>will</td>\n",
       "      <td>31.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>would</td>\n",
       "      <td>32.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>said</td>\n",
       "      <td>33.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>when</td>\n",
       "      <td>34.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>more</td>\n",
       "      <td>35.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>who</td>\n",
       "      <td>36.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>into</td>\n",
       "      <td>37.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>out</td>\n",
       "      <td>38.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>then</td>\n",
       "      <td>39.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>other</td>\n",
       "      <td>40.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>men</td>\n",
       "      <td>41.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>only</td>\n",
       "      <td>42.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>can</td>\n",
       "      <td>43.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>upon</td>\n",
       "      <td>44.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>our</td>\n",
       "      <td>45.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>than</td>\n",
       "      <td>46.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>now</td>\n",
       "      <td>47.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>time</td>\n",
       "      <td>48.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>power</td>\n",
       "      <td>49.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>327.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>111.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>great</td>\n",
       "      <td>50.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>these</td>\n",
       "      <td>51.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>government</td>\n",
       "      <td>52.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>439.0</td>\n",
       "      <td>534.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>146.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>man</td>\n",
       "      <td>53.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>over</td>\n",
       "      <td>54.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>139.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>could</td>\n",
       "      <td>55.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>very</td>\n",
       "      <td>56.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>your</td>\n",
       "      <td>57.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>first</td>\n",
       "      <td>58.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>society</td>\n",
       "      <td>59.0</td>\n",
       "      <td>584.0</td>\n",
       "      <td>601.0</td>\n",
       "      <td>535.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>231.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>two</td>\n",
       "      <td>60.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>made</td>\n",
       "      <td>61.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>such</td>\n",
       "      <td>62.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>about</td>\n",
       "      <td>63.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>some</td>\n",
       "      <td>64.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>any</td>\n",
       "      <td>65.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>did</td>\n",
       "      <td>66.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>know</td>\n",
       "      <td>67.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>pendleton</td>\n",
       "      <td>68.0</td>\n",
       "      <td>3460.0</td>\n",
       "      <td>3769.0</td>\n",
       "      <td>773.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>same</td>\n",
       "      <td>69.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>well</td>\n",
       "      <td>70.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>under</td>\n",
       "      <td>71.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>may</td>\n",
       "      <td>72.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>general</td>\n",
       "      <td>73.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>374.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>195.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>before</td>\n",
       "      <td>74.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>most</td>\n",
       "      <td>75.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>even</td>\n",
       "      <td>76.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>much</td>\n",
       "      <td>77.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>like</td>\n",
       "      <td>78.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>stephanie</td>\n",
       "      <td>79.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3748.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>lorraine</td>\n",
       "      <td>80.0</td>\n",
       "      <td>3340.0</td>\n",
       "      <td>3632.0</td>\n",
       "      <td>766.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>383.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>those</td>\n",
       "      <td>81.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>down</td>\n",
       "      <td>82.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>back</td>\n",
       "      <td>83.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>217.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>came</td>\n",
       "      <td>84.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>236.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>see</td>\n",
       "      <td>85.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>how</td>\n",
       "      <td>86.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>way</td>\n",
       "      <td>87.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>145.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>think</td>\n",
       "      <td>88.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>little</td>\n",
       "      <td>89.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>without</td>\n",
       "      <td>90.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>here</td>\n",
       "      <td>91.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>against</td>\n",
       "      <td>92.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>people</td>\n",
       "      <td>93.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>124.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>after</td>\n",
       "      <td>94.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>must</td>\n",
       "      <td>95.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>don</td>\n",
       "      <td>95.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>274.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>where</td>\n",
       "      <td>96.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>never</td>\n",
       "      <td>97.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>own</td>\n",
       "      <td>98.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>right</td>\n",
       "      <td>99.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>161.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word  2020.txt  2010.txt  2000.txt  1990.txt  00.txt  0.txt\n",
       "0          the       1.0       1.0       1.0       1.0     1.0    1.0\n",
       "1          and       2.0       2.0       2.0       2.0     2.0    2.0\n",
       "2         that       3.0       3.0       3.0       3.0     4.0    3.0\n",
       "3          was       4.0       4.0       4.0       4.0    23.0    5.0\n",
       "4          you       5.0      10.0       8.0       5.0   163.0   19.0\n",
       "5         with       6.0       6.0       6.0       7.0     3.0    6.0\n",
       "6          for       7.0       7.0       7.0      11.0    13.0    8.0\n",
       "7          his       8.0       5.0       5.0       6.0     5.0    4.0\n",
       "8          not       9.0       8.0      11.0      12.0    11.0    9.0\n",
       "9          had      10.0       9.0       9.0      10.0    40.0   14.0\n",
       "10         but      11.0      11.0      10.0      13.0    72.0   10.0\n",
       "11       which      12.0      12.0      15.0      22.0    82.0    7.0\n",
       "12        they      13.0      17.0      14.0      16.0    58.0   21.0\n",
       "13        from      14.0      15.0      18.0      21.0    25.0   15.0\n",
       "14        were      15.0      21.0      21.0      20.0    81.0   22.0\n",
       "15        have      16.0      16.0      16.0      18.0    22.0   11.0\n",
       "16        this      17.0      13.0      17.0      15.0     8.0   13.0\n",
       "17         are      18.0      18.0      24.0      27.0    37.0   16.0\n",
       "18         she      19.0      19.0      13.0       9.0    65.0   47.0\n",
       "19         all      20.0      20.0      20.0      17.0    15.0   12.0\n",
       "20       their      21.0      24.0      25.0      26.0    21.0   30.0\n",
       "21         him      22.0      22.0      19.0      14.0    28.0   20.0\n",
       "22         her      23.0      14.0      12.0       8.0    20.0   33.0\n",
       "23         its      24.0      40.0      53.0      98.0    68.0   67.0\n",
       "24         one      25.0      23.0      22.0      25.0    17.0   28.0\n",
       "25       there      26.0      25.0      23.0      23.0    37.0   32.0\n",
       "26        them      27.0      28.0      28.0      32.0    55.0   38.0\n",
       "27        what      28.0      33.0      32.0      24.0    56.0   24.0\n",
       "28         has      29.0      36.0      45.0      46.0   176.0   40.0\n",
       "29        been      30.0      29.0      31.0      33.0    30.0   23.0\n",
       "30        will      31.0      32.0      33.0      37.0    85.0   27.0\n",
       "31       would      32.0      30.0      29.0      31.0   194.0   25.0\n",
       "32        said      33.0      31.0      26.0      19.0    61.0  141.0\n",
       "33        when      34.0      27.0      27.0      28.0   105.0   34.0\n",
       "34        more      35.0      34.0      36.0      42.0   190.0   26.0\n",
       "35         who      36.0      26.0      30.0      30.0    27.0   18.0\n",
       "36        into      37.0      38.0      37.0      39.0    97.0   63.0\n",
       "37         out      38.0      35.0      34.0      29.0    62.0   77.0\n",
       "38        then      39.0      37.0      35.0      35.0    47.0   50.0\n",
       "39       other      40.0      44.0      56.0      58.0    88.0   66.0\n",
       "40         men      41.0      72.0      79.0      81.0   108.0   53.0\n",
       "41        only      42.0      48.0      54.0      60.0   192.0   71.0\n",
       "42         can      43.0      59.0      50.0      45.0    98.0   54.0\n",
       "43        upon      44.0      51.0      57.0      80.0   121.0   89.0\n",
       "44         our      45.0      56.0      49.0      91.0   149.0   43.0\n",
       "45        than      46.0      47.0      48.0      64.0   201.0   31.0\n",
       "46         now      47.0      43.0      39.0      38.0   134.0   42.0\n",
       "47        time      48.0      42.0      43.0      49.0    96.0   57.0\n",
       "48       power      49.0     247.0     249.0     327.0   204.0  111.0\n",
       "49       great      50.0      63.0      62.0      79.0    64.0   37.0\n",
       "50       these      51.0      50.0      66.0      87.0    48.0   60.0\n",
       "51  government      52.0     391.0     439.0     534.0     NaN  146.0\n",
       "52         man      53.0      41.0      41.0      34.0   222.0   29.0\n",
       "53        over      54.0      68.0      64.0      62.0    92.0  139.0\n",
       "54       could      55.0      46.0      40.0      41.0   199.0   64.0\n",
       "55        very      56.0      45.0      42.0      50.0   159.0   65.0\n",
       "56        your      57.0      57.0      46.0      36.0   201.0   55.0\n",
       "57       first      58.0      65.0      77.0      77.0   219.0   81.0\n",
       "58     society      59.0     584.0     601.0     535.0     NaN  231.0\n",
       "59         two      60.0      52.0      58.0      70.0   124.0  100.0\n",
       "60        made      61.0      61.0      63.0      83.0   174.0   82.0\n",
       "61        such      62.0      64.0      75.0      78.0   146.0   45.0\n",
       "62       about      63.0      53.0      44.0      40.0    99.0  113.0\n",
       "63        some      64.0      39.0      38.0      44.0    70.0   49.0\n",
       "64         any      65.0      54.0      55.0      55.0   130.0   49.0\n",
       "65         did      66.0      60.0      52.0      54.0   154.0   99.0\n",
       "66        know      67.0      83.0      74.0      48.0   159.0   98.0\n",
       "67   pendleton      68.0    3460.0    3769.0     773.0     NaN    NaN\n",
       "68        same      69.0      99.0     124.0     118.0   203.0  112.0\n",
       "69        well      70.0      67.0      59.0      52.0    54.0   58.0\n",
       "70       under      71.0     102.0     121.0     153.0   129.0  103.0\n",
       "71         may      72.0      49.0      69.0      86.0   162.0   36.0\n",
       "72     general      73.0     197.0     227.0     374.0   254.0  195.0\n",
       "73      before      74.0      66.0      65.0      59.0   101.0   84.0\n",
       "74        most      75.0      81.0      87.0     120.0   243.0   46.0\n",
       "75        even      76.0      89.0      98.0     125.0    77.0   78.0\n",
       "76        much      77.0      77.0      76.0      76.0   228.0   73.0\n",
       "77        like      78.0      62.0      51.0      47.0    10.0   74.0\n",
       "78   stephanie      79.0       NaN    3748.0       NaN     NaN    NaN\n",
       "79    lorraine      80.0    3340.0    3632.0     766.0     NaN  383.0\n",
       "80       those      81.0      80.0      91.0     138.0    26.0   44.0\n",
       "81        down      82.0      76.0      70.0      63.0    63.0  126.0\n",
       "82        back      83.0      96.0      86.0      85.0   165.0  217.0\n",
       "83        came      84.0      94.0      85.0      74.0   172.0  236.0\n",
       "84         see      85.0      69.0      61.0      51.0   150.0   88.0\n",
       "85         how      86.0      84.0      73.0      57.0   152.0   56.0\n",
       "86         way      87.0      88.0      82.0      69.0   118.0  145.0\n",
       "87       think      88.0     125.0     104.0      82.0   227.0  114.0\n",
       "88      little      89.0      55.0      47.0      43.0   229.0   80.0\n",
       "89     without      90.0     105.0     107.0     107.0   105.0   79.0\n",
       "90        here      91.0      82.0      81.0      68.0    89.0   93.0\n",
       "91     against      92.0     138.0     143.0     171.0   220.0   92.0\n",
       "92      people      93.0     111.0     116.0     121.0   203.0  124.0\n",
       "93       after      94.0      58.0      60.0      53.0   103.0   95.0\n",
       "94        must      95.0      71.0      78.0      88.0   213.0   69.0\n",
       "95         don      95.0     144.0     105.0      56.0     NaN  274.0\n",
       "96       where      96.0      75.0      71.0      84.0   151.0  114.0\n",
       "97       never      97.0      87.0      83.0      75.0   172.0   83.0\n",
       "98         own      98.0      92.0      92.0      96.0   111.0   68.0\n",
       "99       right      99.0     133.0     135.0     117.0   236.0  161.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rank.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-jewelry",
   "metadata": {},
   "source": [
    "## Trying to fit models to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-memorabilia",
   "metadata": {},
   "source": [
    "### Read in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "invalid-directory",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T10:30:33.350944Z",
     "start_time": "2021-01-26T10:30:33.140056Z"
    }
   },
   "outputs": [],
   "source": [
    "file_contents = []\n",
    "targets = []\n",
    "\n",
    "files = [f for f in listdir(file_path) if isfile(join(file_path, f))]\n",
    "files = list(filter(lambda file: file[0].isdigit(), files))\n",
    "random.shuffle(files)\n",
    "\n",
    "targets_=['70','80','90','00','10']\n",
    "iter_ = 0\n",
    "\n",
    "for f in files[:120]:\n",
    "    file = open(\"processedData/\" + f, encoding=\"ISO-8859-1\")\n",
    "    file_contents.append(file.read())\n",
    "    iter_ = iter_+1\n",
    "    targets.append(targets_[iter_%5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-mortality",
   "metadata": {},
   "source": [
    "### Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "muslim-coating",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T10:30:57.629834Z",
     "start_time": "2021-01-26T10:30:33.352090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   20.1s finished\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('kbest', SelectKBest(chi2, k=100)),\n",
    "    ('nb', MultinomialNB()),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    #'vect__max_df': [1.0),\n",
    "    # 'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    #'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    # 'tfidf__use_idf': (True, False),\n",
    "    # 'tfidf__norm': ('l1', 'l2'),\n",
    "    #'clf__max_iter': (20),\n",
    "    #'clf__alpha': (0.00001),\n",
    "    #'clf__penalty': ('l2'),\n",
    "    # 'clf__max_iter': (10, 50, 80),\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, verbose=1)\n",
    "\n",
    "grid_search.fit(file_contents, targets)\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-involvement",
   "metadata": {},
   "source": [
    "## Realisation and conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-fabric",
   "metadata": {},
   "source": [
    "At this point, we came to the conclusion that \"year\" in the Gutenberg dataset shows when the data **was published** to the project, and not the release date of the book.\n",
    "\n",
    "We searched for possible solutions to get the years for book publications, but were unable to find any free API that we could link to our current dataset.\n",
    "\n",
    "We therefore went on a search for other datasets, and to remake our hypothesis entirely.\n",
    "Thus, this part ended in a blind spot. However science is not only about the results, but also about the discoveries along the way, and therefore it is added into this file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-chapel",
   "metadata": {},
   "source": [
    "# Studying language change in Icelandic parliamentary speeches\n",
    "\n",
    "Our task involves research into **language change over the past 100 years**. Additionally we have been tasked with working out factors that influence language change. \n",
    "\n",
    "Another proposed research question could have been focused on figuring out which languages are going extinct. This particular task has been found out to be near impossible to answer given the available data. It is estimated to be very hard to come up with data that capture the amount of speakers for a large enough ranges of combinations of language and year. Furthermore, any data that are available are likely to apply a different definition of \"speaker\" (sometimes including second/third... language speakers, sometimes not) and is also likely to contain politically motivated noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-valve",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-moldova",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Therefore, we decided to search for English language corpora containing a wide array of text documents collected over the past century for predefined dialects of English and genre of text (movie, articles, books, ...). This surprisingly turned out to be a complex endeavour as all high quality corpora were available only for a big price tag. \n",
    "\n",
    "We also looked into the material provided by the Guttenberg Project [Link](https://www.projekt-gutenberg.org/). This turned out to be promising at first sight as it appears that there is a lot of recently published material. However release date of these documents does not match the year when the documents were actually written and soon enough we figured out that all material is from before 1923. This obviously did not allow us to look much into language change of the 20th and 21st century.\n",
    "\n",
    "_Gerlach, M., & Font-Clos, F. (2020). A standardized Project Gutenberg corpus for statistical analysis of natural language and quantitative linguistics. Entropy, 22(1), 126._\n",
    "\n",
    "Theoretically one could obtain books from after 1923 and include them into the analysis. But one would quickly run into copyright/licensing issues here.\n",
    "\n",
    "Obtaining the content of these books and preprocessing them for the purposes of data analysis turned out to be quite cumbersome as well. Look at Gunnar's notebooks (first draft [here](firstDraft.ipynb), second draft [here](secondDraft.ipynb)) for the details. \n",
    "\n",
    "Finally we turned to looking for non-English corpora and **found an annotated corpus including pre-factured lemmatization of [Icelandic parlimentary speeches](https://clarin.is/en/resources/parliament/) from 1911 until 2018:**\n",
    "\n",
    "_Steingrímsson, Steinþór, Sigrún Helgadóttir, Eiríkur Rögnvaldsson, Starkaður Barkarson and Jón Guðnason. 2018. Risamálheild: A Very Large Icelandic Text Corpus. Proceedings of LREC 2018, pp. 4361-4366. Myazaki, Japan._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-framing",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## The task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-medicare",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the line with our goal of analyzing the change in language over the past 100 years, we decided to train different models and assess their ability to predict whether an speech held in the Icelandic Parlament belongs to a particular decade. In the end, this is a **document classification task**  in which the input is a large set of parlament speeches and the target/class is the decade in which the speeches were held. \n",
    "\n",
    "A good performance of our proposed classifiers may support the idea that Icelandic has envolved in the years. However, the fact that the models would perform well is not enough to assert that the language has changed. It could be that what has actually changed are the topics or even the way of documenting the speeches. Anyway, for us it was really exiciting to check whether we are able to **fit a  model that predicts reasonably well the decade of an speech by only using the speech itself.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-yacht",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-sigma",
   "metadata": {},
   "source": [
    "In this section, we provide the **setup for a successful** implementation (or replication) of our experiment within this Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-washer",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Load required libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-naples",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The following libraries are used during the next sections and therefore need to be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "arctic-season",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T10:30:57.918780Z",
     "start_time": "2021-01-26T10:30:57.630794Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import glob\n",
    "from nltk.probability import FreqDist\n",
    "import random\n",
    "from functools import reduce\n",
    "from nltk import ngrams\n",
    "# Used for building models for classifying:\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "pressing-fleet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T10:30:57.921797Z",
     "start_time": "2021-01-26T10:30:57.920020Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#needed afterwards too\n",
    "namespace = \"{http://www.tei-c.org/ns/1.0}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-spectrum",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Get the data\n",
    "\n",
    "Data can be downloaded from here: http://www.malfong.is/index.php?dlid=81&lang=en. However, we provided already in our submission file the specifications on how to get the data of our assignment.\n",
    "\n",
    "Then extract zip folder such that a folder labelled `CC_BY` shows up in the parent folder of this notebook. *Test*: `ls ../CC_BY/althingi` should work when run from `.../IcelandicParliamentSpeeches.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-section",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Preprocessing helpers\n",
    "\n",
    "The data are available as XML. The text has already been preprocessed to be separated into paragraphs, sentences and words. Furthermore each word tag also includes a `lemma` attribute relating inflected/declensed forms of words to its lemma. This has been done by the authors of the original paper using Machine Learning approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-leader",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Given a relative path to a file, pull out a list with all the words. This can be achieved by looking for all tags of type `w`, additionally also retrieve the lemma for each word.\n",
    "\n",
    "We will discard all sentences of length 3 or smaller to remove noise and to avoid that our models are able to detect year of speech just based on some short introductory/outro phrases. Furthermore the raw data appear to contain plenty of elements tagged as words that comprise of just a single letter followed by a dot. These will be removed here as well.\n",
    "\n",
    "⚠️ *Pitfall*: The namespace from above must be included when parsing out content from these XML files based on tag names.\n",
    "\n",
    "⚠️ In this kind of preprocessing we lose information about sentence boundaries as all punctuation items from the raw data are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "moved-retention",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T10:30:57.927757Z",
     "start_time": "2021-01-26T10:30:57.922699Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def extract_words(path):\n",
    "    xml_tree = ET.parse(open(path, 'r', encoding=\"utf8\"))\n",
    "    words = []\n",
    "    lemmata = []\n",
    "    \n",
    "    for sentence in xml_tree.getroot().iter('{}s'.format(namespace)):\n",
    "        words_in_sent = sentence.findall('{}w'.format(namespace))\n",
    "        if len(words_in_sent) > 2:\n",
    "            for word in sentence.findall('{}w'.format(namespace)):\n",
    "                if not word.text.endswith('.'):\n",
    "                    words.append(word.text)\n",
    "                    lemmata.append(word.attrib['lemma'])\n",
    "        \n",
    "    return words, lemmata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-magnitude",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Extract content of files separated into sentences, note that all stop items are wrapped in a `p` tag in the original documents and are not included here.\n",
    "\n",
    "Also note that some further pre-processing could be done here to exclude items such as numbers, percentages, names, abbreviations, etc. In the original documents these are also assigned to be words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "revolutionary-america",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T10:30:57.935221Z",
     "start_time": "2021-01-26T10:30:57.928653Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def extract_sentences(path, lemma=False):\n",
    "    xml_tree = ET.parse(open(path, 'r', encoding=\"utf8\"))\n",
    "    sentences = []\n",
    "    \n",
    "    for sentence in xml_tree.getroot().iter('{}s'.format(namespace)):\n",
    "        sentence_cur = []\n",
    "        words_in_sent = sentence.findall('{}w'.format(namespace))\n",
    "        \n",
    "        if len(words_in_sent) > 2:\n",
    "            for word in sentence.findall('{}w'.format(namespace)):\n",
    "                if not word.text.endswith('.'):\n",
    "                    if lemma:\n",
    "                        sentence_cur.append(word.attrib['lemma'])\n",
    "                    else:\n",
    "                        sentence_cur.append(word.text)\n",
    "            \n",
    "            sentences.append(sentence_cur)\n",
    "        \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-rendering",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Retrieve a random selection of `k` file names from the entire corpus. The files must be of type `xml`. This method does not load the entire corpus into memory and allows you to work with smaller selections for test purposes. This method samples only from the `althingi` folder so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "piano-value",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T10:30:57.940163Z",
     "start_time": "2021-01-26T10:30:57.936222Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_random_sample(k):\n",
    "    files = [filename for filename in glob.iglob('../CC_BY/althingi/**/*.xml', \n",
    "                                                 recursive=True)]\n",
    "    return random.sample(files, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dental-butter",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T10:30:59.299818Z",
     "start_time": "2021-01-26T10:30:57.941037Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "files = [filename for filename in glob.iglob('../CC_BY/althingi/**/*.xml', recursive=True)]\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-castle",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Do the same as above but choose `k` files only from a given year (range: 1911-2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "valuable-indicator",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T10:30:59.307175Z",
     "start_time": "2021-01-26T10:30:59.302814Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_files_for_year(year, k):\n",
    "    files = [filename for filename in glob.iglob('../CC_BY/althingi/{}/'.format(year) + '**/*.xml', \n",
    "                                                 recursive=True)]\n",
    "    return random.sample(files, min(len(files), k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-share",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Preliminary Data Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-darwin",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this section, we perform a preliminary data analysis to get a better insight of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-brass",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Zipf's Law\n",
    "\n",
    "First using frequency distributions of the Natural Language ToolKit (`NLTK`) to look into whether or not we can confirm [Zipf's Law](https://en.wikipedia.org/wiki/Zipf%27s_law) based on the data we have.\n",
    "\n",
    "⚠️ Note that the analysis is done based on 15 randomly selected files from the entire corpus at this point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "figured-execution",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T10:31:00.304417Z",
     "start_time": "2021-01-26T10:30:59.308547Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Samples', ylabel='Counts'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "\n",
    "for file in get_random_sample(15):\n",
    "    words.extend(extract_words(file)[1])\n",
    "    \n",
    "fq = FreqDist(word.lower() for word in words)\n",
    "fq.plot(25, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-establishment",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Visualizing the same data but with using the logarithm of the occurrences, this should ideally obtain a straight line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cheap-width",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T10:31:00.399069Z",
     "start_time": "2021-01-26T10:31:00.305292Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "freq_df = pd.DataFrame.from_dict(fq, orient='index', columns=['word_occur'])\n",
    "freq_df.sort_values(by='word_occur', inplace=True, ascending=False)\n",
    "freq_df.word_occur = np.log2(freq_df['word_occur'])\n",
    "freq_df.head(25).plot(kind='line')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-nursing",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Disappearing words / new words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-waters",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here is a description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "grateful-hands",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T10:31:00.483652Z",
     "start_time": "2021-01-26T10:31:00.400077Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "words_1914 = []\n",
    "words_2014 = []\n",
    "\n",
    "for file in get_files_for_year(1914, 25):\n",
    "    words_1914.extend(extract_words(file)[1])\n",
    "    \n",
    "for file in get_files_for_year(2014, 25):\n",
    "    words_2014.extend(extract_words(file)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-funeral",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Development of average sentence length\n",
    "\n",
    "This is just one possible metric for the development/analysis of language complexity. There is so much more you could come up with here.\n",
    "\n",
    "Obviously our choice to discard very short sentences in the preprocessing step has an impact on the values here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "stone-edmonton",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T10:31:06.062153Z",
     "start_time": "2021-01-26T10:31:00.484533Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def avg_sentence_length_year(year, k):\n",
    "    sentence_len = []\n",
    "    for file in get_files_for_year(year, k):\n",
    "        sentences = extract_sentences(file)\n",
    "        sentence_len.extend([len(s) for s in sentences])\n",
    "    \n",
    "    return reduce(lambda a, b: a + b, sentence_len) / len(sentence_len)\n",
    "        \n",
    "sentence_len_years = []\n",
    "\n",
    "for year in range(1924, 2018):\n",
    "    sentence_len_years.append(avg_sentence_length_year(year, 20))\n",
    "    \n",
    "avg_df = pd.DataFrame(sentence_len_years, index=range(1924, 2018), columns=['avg_sent_len'])\n",
    "avg_df.plot(kind='line')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earlier-recycling",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-necklace",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here is a description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "distinguished-oliver",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T10:31:07.222001Z",
     "start_time": "2021-01-26T10:31:06.063096Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def most_common_ngrams(n, top_k, sample):\n",
    "    file_contents = []\n",
    "\n",
    "    for file in get_random_sample(sample):\n",
    "        file_contents.extend(extract_words(file)[1])\n",
    "    \n",
    "    fq_ngr = FreqDist(ngrams(file_contents, n))\n",
    "    fq_ngr.plot(top_k, cumulative=False)\n",
    "    \n",
    "most_common_ngrams(n=3, top_k=25, sample=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-bolivia",
   "metadata": {},
   "source": [
    "## Building model for classifying speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-penny",
   "metadata": {},
   "source": [
    "This section is the main core of the modelling task of this assignment. It is organized as follows:\n",
    "\n",
    "* Firstly, the data is splitted into **training and test sets** according to the criteria that in our opinion fits better to the nature of data and task.\n",
    "* Secondly, we describe the **methods** that are going to be used for **feature extraction** from our documents.\n",
    "* After that, we describe the **classifiers chosen** to be trained and why they were selected.\n",
    "* Then, we **train 7 models** combining the feature extraction techniques described and the classifiers selected. This is done through a cross-validated grid search in which many hyperparameters are combined. The goal of this search is to find the **best hyperparameter combination of each of the 7 models.**\n",
    "* Finally, we **compare the results from the training within and between the models.**\n",
    "* Evaluation on test data will be performed in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-poster",
   "metadata": {},
   "source": [
    "### Constructing training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-priority",
   "metadata": {},
   "source": [
    "Our whole dataset contains **380285 speeches** hold in the Icelandic parliament from 1911 to 2017. In order to perform our train-test split, we took into account the following considerations:\n",
    "\n",
    "* Documents are **classified in directories by year and month instead of decade.**\n",
    "* Decades (classes) are highly **unbalanced.** There are much more documents from laterdecades as from the earlier ones. As an example, 1912 has only 14 documents while 2011 has 13957. This may introduce bias in the training of the models if not dealt. \n",
    "\n",
    "To solve the first problem, we use the help function get_files_for_year() created above, which takes n documents from an specified year. After that, for each of the documents, the year is substituted by the decade as shown in the next two sections. This can be done iteratively through a list of years. In this way, **we obtain a dataset with a bunch of corpora labelled by decade.**\n",
    "\n",
    "To solve the problem of unbalance within classes, we **limit the number of documents to be extracted from each year to 200 for the training set.** This way, we ensure that there will not be too big differences within the number of documents sampled within the years (maximum of 200 vs minimum of 14) and neither within the decades. We choose 200 since we consider it to be a good balance for **undersampling the majority classes but not loosing as much information as we would keep it to minimum of 14**.\n",
    "\n",
    "Note that, in order to make the runtimes of our notebook shorter (feasible) we **skip intermediate decades from our classification task.** This would simulate that there were not speeches held in some decades. We like to imagine it as weird regime which combines a decade of democracy followed by a decade of dictatorship. In summary:\n",
    "\n",
    "- 1910s, 1930s, 1950s, 1970s, 1990s, and 2010s are considered.\n",
    "- Whereas 1920s, 1940s, 1960s, 1980s, and 2000s are discarded.\n",
    "\n",
    "We will perform a train/test split of the approximate proportion 80/20. We will see why it will be approximate in the next two sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-rotation",
   "metadata": {},
   "source": [
    "#### Train data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-hometown",
   "metadata": {},
   "source": [
    "**8 years out of the 10 years** that form a decade are chosen for each of the 6 decades considered for the train set. **The other 2 are left for the test set**. The selection of the years was completly random. For each of the decades a maximum of 1600 documents are chosen. However, this will not be equal for all the decandes, since, as explained above, not all the years have at least 200 documents. \n",
    "\n",
    "Note that for the last decade, we just have documents until 2017. The split will be 6 years (train) vs 1 (test) in this case. Same applies for first decade (in this case, 7 vs. 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extended-jewelry",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-26T10:29:39.876Z"
    }
   },
   "outputs": [],
   "source": [
    "#set seed for reproducibility\n",
    "random.seed(123)\n",
    "\n",
    "file_contents = []\n",
    "targets = []\n",
    "\n",
    "\n",
    "for year in [1911, 1912, 1914, 1915, 1916, 1918, 1919,\n",
    "             1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, \n",
    "             1951, 1952, 1953, 1955, 1956, 1957, 1958, 1959,\n",
    "             1970, 1971, 1972, 1973, 1974, 1975, 1978, 1979,\n",
    "             1990, 1991, 1992, 1993, 1995, 1996, 1997, 1999,\n",
    "             2010, 2011, 2012, 2013, 2014, 2016, 2017]:\n",
    "    for file in get_files_for_year(year, 200):\n",
    "        file_contents.append(extract_words(file)[1])\n",
    "        targets.append(year - year%10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "actual-validity",
   "metadata": {},
   "source": [
    "Let's randomly choose a fixed number of documents (here currently: 200) from various different decades. Then passing (document, decade) pairs to the model below. The decade is computed by subtracting `mod(<year>, 10)` from `<year>`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-borough",
   "metadata": {},
   "source": [
    "#### Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-angel",
   "metadata": {},
   "source": [
    "Choose the other **2 years that were not selected** within the decades in the train set. In this case, **we do not have to limit the number of documents for year.** It doesn't make sense to undersample the test set since it represents \"unseen\" data. And, unseen data should be as close to reality as possible. That means, that it is normal that there are much more documents from later decades than from earlier. \n",
    "\n",
    "So, instead of 200, we will put there a very large number to be sure that all the documents from every year are selected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-balloon",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-26T10:29:39.881Z"
    }
   },
   "outputs": [],
   "source": [
    "#seed for reproducibility\n",
    "random.seed(123)\n",
    "\n",
    "file_contents_test = []\n",
    "targets_test = []\n",
    "\n",
    "for year in [1913, 1917,1930, 1939, 1950, 1954, 1976, 1977, 1994, 2013,2015]:\n",
    "    for file in get_files_for_year(year, 200000):\n",
    "        file_contents_test.append(extract_words(file)[1])\n",
    "        targets_test.append(year - year%10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-horse",
   "metadata": {},
   "source": [
    "#### See classes distribution within train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-chicken",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-26T10:29:39.886Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(Counter(targets).keys()) \n",
    "print(Counter(targets).values()) \n",
    "\n",
    "print(Counter(targets_test).keys()) \n",
    "print(Counter(targets_test).values()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-oasis",
   "metadata": {},
   "source": [
    "We see that although there are some differences within the classes for the train split, it is acceptable to perform the classification task. Maximum within the classes for training is 1600.\n",
    "\n",
    "Test set is expected to have much more class imbalance. However, our model should dealt with it thanks to the undersampling that was performed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-swiss",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Text feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-assurance",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We have considered 3 different methods for text feature extraction: Tf-idf, word2vec and doc2vec. All of them will be implemented through the corresponding functions from *sklearn* library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-vinyl",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-concentrate",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Helper function to transform the data so that it is in the right format for the tfidfVectorizer() function that will be used later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-adoption",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-26T10:29:39.894Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class JoinElement(object):\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        #joins the elements of a list (which represents a document) into a single string \n",
    "        #with a blank space separation between each word\n",
    "        return [' '.join(X[i]) for i in range(len(X))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-elements",
   "metadata": {
    "hidden": true
   },
   "source": [
    "More information about it: [sklearn documentation.](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-illinois",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-rover",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<ins>Original paper</ins>:\n",
    "Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed representations of words and phrases and their compositionality. Advances in neural information processing systems, 26, 3111-3119.\n",
    "\n",
    "With this model every word is assigned a unique vector of configurable cardinality such that the dot product of two randomly chosen vectors should be proportional to the semantic similarity for the associated words. This happens during the training step using logistic regression and sliding windows. Personally I found that this video delivers a solid explanation of the concepts: https://www.youtube.com/watch?v=QyrUentbkvw\n",
    "\n",
    "However, since we are working with entire documents as training items we have to somehow aggregate the vectors for every word in a given document. This can be done e.g. by taking the mean and/or summing up the vectors (see `MeanEmbeddingVectorizer`), optionally weighted by TF-IDF (see `MeanEmbeddingVectorizerTfidf`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-conditions",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-26T10:29:39.900Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(BaseEstimator):\n",
    "    def fit(self, X, y):\n",
    "        self.word2vec = Word2Vec(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec.wv[w] for w in words if w in self.word2vec.wv.vocab]\n",
    "                    or [np.zeros(self.word2vec.vector_size)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-ottawa",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-26T10:29:39.903Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizerTfidf(BaseEstimator):\n",
    "    def fit(self, X, y):\n",
    "        self.word2vec = Word2Vec(X)\n",
    "        self.X_joined = [' '.join(X[i]) for i in range(len(X))]\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.transformed = self.vectorizer.fit_transform(self.X_joined)\n",
    "        self.transformed = pd.DataFrame.sparse.from_spmatrix(self.transformed)\n",
    "        return self\n",
    "    \n",
    "    def tfidf(self, w, docid):\n",
    "        if w in self.vectorizer.vocabulary_:\n",
    "            return self.transformed[self.vectorizer.vocabulary_[w]][docid]\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec.wv[w] * self.tfidf(w, i) for w in words if w in self.word2vec.wv.vocab]\n",
    "                    or [np.zeros(self.word2vec.vector_size)], axis=0)\n",
    "            for i, words in enumerate(X)\n",
    "        ])\n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        self = self.fit(X, y)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-recall",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-designer",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Finally we are attempting to build a model using _Doc2Vec_. After training this model with our training corpus we receive a vector of configurable cardinality for each document.\n",
    "\n",
    "<ins>Original paper</ins>: Le, Quoc, and Tomas Mikolov. \"Distributed representations of sentences and documents.\" International conference on machine learning. 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-conviction",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-26T10:29:39.908Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Doc2Vectorizer(BaseEstimator):\n",
    "    def __init__(self, window=2, vector_size=100):\n",
    "        self.window = window\n",
    "        self.vector_size = vector_size\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        docs = [TaggedDocument(X[i], [y[i]]) for i in range(len(X))]\n",
    "        self.doc_vec = Doc2Vec(docs, vector_size=self.vector_size, window=self.window, min_count=1, workers=4)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return [self.doc_vec.infer_vector(X[i]) for i in range(len(X))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-prediction",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**BERT** (*Bidirectional Encoder Representations from Transformers*) is also interesting to look at, but we'll skip this here because we predict training a model from scratch would use up too many resources. Given more time however you could search for pretrained networks that roughly serve the purpose of classification of documents according to publication year.\n",
    "\n",
    "<ins>Paper</ins>: Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-adapter",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-fetish",
   "metadata": {
    "hidden": true
   },
   "source": [
    "3 different classifiers are going to be trained: Multinomial Naive Bayes, Support Vector Machines and Random Forest Classifier. All of them will be implemented using sklearn library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-drama",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Multinominal Naive Bayes (MNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-rating",
   "metadata": {
    "hidden": true
   },
   "source": [
    "MNB is a common method for document classification due to its good balance between computational efficiency and predictive performance [(Eibe, 2006)](https://www.cs.waikato.ac.nz/~eibe/pubs/FrankAndBouckaertPKDD06new.pdf). Therefore, we decided to choose it as one of our classifiers. \n",
    "\n",
    "Details on the algorithm implementation can be found in the [sklearn documentation.](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html).\n",
    "\n",
    "The set of hyperparameters chosen to perform the grid search cross-validation during the training are based on the recommendations[ from this article. ](https://medium.com/@kocur4d/hyper-parameter-tuning-with-pipelines-5310aff069d6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-journey",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-garage",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Support vector machines are widely used for classification purposes. What is more, it improves Multinominal Naive Bayes in terms of performance in most of the classification taks. Thus, it was also chosen as one of our classifiers to be trained.\n",
    "\n",
    "Details on the algorithm implementation can be found in [sklearn documentation.](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n",
    "\n",
    "The set of hyperparameters chosen to perform the grid search cross-validation during the training are based on the recommendations [ from this article. ](https://www.vebuso.com/2020/03/svm-hyperparameter-tuning-using-gridsearchcv/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-surgery",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-sharing",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Random Forest Classifier is one of the best methods according to the literature for classification tasks. However, the runtime may be extremly large (specially when increasing the size of the forest within grid search CV setups). \n",
    "\n",
    "Details on the algorithm implementation can be found in [sklearn documentation.](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html).\n",
    "\n",
    "The set of hyperparameters chosen to perform the grid search cross-validation during the training are based on the recommendations [ from this article. ](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-captain",
   "metadata": {},
   "source": [
    "### Train models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-discipline",
   "metadata": {},
   "source": [
    "Since there are 3 methods for feature extraction and 3 classifiers, we should train 9 kind of models with their different combinations of hyperparameters. However, multinomial naive bayes does not take negative values produced by Word2Vec and Doc2Vec. Therefore, we have 7.\n",
    "\n",
    "For each model, **a grid search is performed with different combinations of hyperparameters** for the classifiers and the text extraction methods. Afterwards, the most relevant results of each of the models are stored in a pandas data frame.\n",
    "\n",
    "The goal of this grid search is to find the best combination of hyperparameters for each of our 7 combinations.\n",
    "\n",
    "Note that **ideally we should perform a random search prior to the grid search to limit the scope of the best hyperparameters** to be used and then perform a more accurate search. However, this would lead to a tedious notebook and extremly large runtimes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-conspiracy",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Model 1: TFIDF vectorizer, select K best and Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-situation",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-26T10:29:39.921Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#choose parameters for the different steps in the pipeline\n",
    "parameters_model_1 = {\n",
    "    \n",
    "    #select KBest\n",
    "    #k number of top features to select (default 10)\n",
    "    \"k_best__k\": [10, 500],\n",
    "    #score function to be used (default f_classif)\n",
    "    \"k_best__score_func\": [chi2],\n",
    "    \n",
    "    #MultinomialNaiveBayes\n",
    "    #alpha is a parameter for smoothing (default value is 1)\n",
    "    \"MNB__alpha\": np.linspace(0.5, 1.5, 4), \n",
    "    #whether to learn class prior probabilities or not (dafult value is True)\n",
    "    \"MNB__fit_prior\": [True,False],\n",
    "    \n",
    "    #TFIDF Vectorizer\n",
    "    #Whether the feature should be made of word or character n-grams (default word)\n",
    "    \"tfidf__analyzer\": [\"word\"],\n",
    "    #Smooth idf weights by adding one to document frequencies, \n",
    "    #as if an extra document was seen containing every term in the collection exactly once.\n",
    "    #Prevents zero divisions (default True)\n",
    "    \"tfidf__smooth_idf\": [True, False]\n",
    "    \n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "#build a pipeline\n",
    "model_1_pipeline = Pipeline([\n",
    "        #joins list into a single string\n",
    "        ('join', JoinElement()),\n",
    "        #tfidf vectorizer\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        #select 1000 best from word vectors\n",
    "        ('k_best', SelectKBest()),\n",
    "        #apply naive bayes\n",
    "        ('MNB', MultinomialNB())\n",
    "    ])\n",
    "\n",
    "#design grid search\n",
    "grid_search_model_1 = GridSearchCV(\n",
    "    #pipeline to be followed\n",
    "    model_1_pipeline,\n",
    "    #parameters\n",
    "    param_grid=parameters_model_1,\n",
    "    #number of folds for CV\n",
    "    cv=5,\n",
    "    #scoring to be considered for the cv\n",
    "    scoring = \"accuracy\",\n",
    "    #parallelize if possible\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#fit the grid search for training data\n",
    "grid_search_model_1.fit(file_contents, targets)\n",
    "\n",
    "#save results of cross validation\n",
    "cv_results_model_1 = pd.DataFrame(grid_search_model_1.cv_results_)\n",
    "\n",
    "#filter columns to be kept in the dataframe\n",
    "filter_col = [col for col in cv_results_model_1 if (col.startswith(\"param_\") or col.startswith(\"mean_\") or col.startswith(\"rank\"))]\n",
    "\n",
    "#save results with only filtered columns\n",
    "cv_results_model_1 = cv_results_model_1[filter_col]\n",
    "\n",
    "#save name of the model for later comparison\n",
    "cv_results_model_1.insert(loc=0, column=\"Model\", value= \"1\")\n",
    "\n",
    "#round mean_test_score\n",
    "cv_results_model_1[\"mean_test_score\"] = cv_results_model_1[\"mean_test_score\"].round(2)\n",
    "\n",
    "#show best 5 sorted by mean_test_score\n",
    "display(cv_results_model_1.sort_values(by=\"mean_test_score\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-leadership",
   "metadata": {},
   "source": [
    "#### Model 2: TFIDF vectorizer, select K best and SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-bandwidth",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-26T10:29:39.926Z"
    }
   },
   "outputs": [],
   "source": [
    "#choose parameters for the different steps in the pipeline\n",
    "parameters_model_2 = {\n",
    "\n",
    "    #select KBest\n",
    "    #k number of top features to select (default 10)\n",
    "    \"k_best__k\": [10, 500],\n",
    "    #score function to be used (default f_classif)\n",
    "    \"k_best__score_func\": [chi2],\n",
    "    \n",
    "    #SVC\n",
    "    #Specifies the kernel type to be used in the algorithm\n",
    "    \"SVC__kernel\" : [\"linear\", \"poly\", \"sigmoid\"],\n",
    "    #Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid'\n",
    "    \"SVC__gamma\": [1,0.1,0.001],\n",
    "    #Regularization parameter. The strength of the regularization is inversely proportional to C. \n",
    "    #Must be strictly positive. The penalty is a squared l2 penalty.\n",
    "    \"SVC__C\": [0.1,1, 10, 100],\n",
    "    \n",
    "    #TFIDF Vectorizer\n",
    "    #Whether the feature should be made of word or character n-grams (default word)\n",
    "    \"tfidf__analyzer\": [\"word\"],\n",
    "    #Smooth idf weights by adding one to document frequencies, \n",
    "    #as if an extra document was seen containing every term in the collection exactly once.\n",
    "    #Prevents zero divisions (default True)\n",
    "    \"tfidf__smooth_idf\": [True, False]\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "#build a pipeline\n",
    "model_2_pipeline = Pipeline([\n",
    "        #joins list into a single string\n",
    "        ('join', JoinElement()),\n",
    "        #tfidf vectorizer\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        #select 1000 best from word vectors\n",
    "        ('k_best', SelectKBest()),\n",
    "        #apply naive bayes\n",
    "        ('SVC', SVC())\n",
    "    ])\n",
    "\n",
    "#design grid search\n",
    "grid_search_model_2 = GridSearchCV(\n",
    "    #pipeline to be followed\n",
    "    model_2_pipeline,\n",
    "    #parameters\n",
    "    param_grid=parameters_model_2,\n",
    "    #number of folds for CV\n",
    "    cv=5,\n",
    "    #scoring to be considered for the cv\n",
    "    scoring = \"accuracy\",\n",
    "    #parallelize if possible\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#fit the grid search for training data\n",
    "grid_search_model_2.fit(file_contents, targets)\n",
    "\n",
    "#save results of cross validation\n",
    "cv_results_model_2 = pd.DataFrame(grid_search_model_2.cv_results_)\n",
    "\n",
    "#filter columns to be kept in the dataframe\n",
    "filter_col = [col for col in cv_results_model_2 if (col.startswith(\"param_\") or col.startswith(\"mean_\") or col.startswith(\"rank\"))]\n",
    "\n",
    "#save results with only filtered columns\n",
    "cv_results_model_2 = cv_results_model_2[filter_col]\n",
    "\n",
    "#save name of the model for later comparison\n",
    "cv_results_model_2.insert(loc=0, column=\"Model\", value= \"2\")\n",
    "\n",
    "#round mean_test_score\n",
    "cv_results_model_2[\"mean_test_score\"] = cv_results_model_2[\"mean_test_score\"].round(2)\n",
    "\n",
    "#show best 5 sorted by mean_test_score\n",
    "display(cv_results_model_2.sort_values(by=\"mean_test_score\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-peace",
   "metadata": {},
   "source": [
    "#### Model 3: TFIDF vectorizer, select K best and Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-kernel",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-26T10:29:39.931Z"
    }
   },
   "outputs": [],
   "source": [
    "#choose parameters for the different steps in the pipeline\n",
    "parameters_model_3 = {\n",
    "    \n",
    "    #select KBest\n",
    "    #k number of top features to select (default 10)\n",
    "    \"k_best__k\": [10, 500],\n",
    "    #score function to be used (default f_classif)\n",
    "    \"k_best__score_func\": [chi2],\n",
    "    \n",
    "    #RF classifier\n",
    "    #nThe number of trees in the forest (default is 100)\n",
    "    \"clf__n_estimators\" : [10,100,200],\n",
    "    #The minimum number of samples required to split an internal node (default is 2 but it is a large dataset)\n",
    "    \"clf__min_samples_split\": [10, 40, 80],\n",
    "    #The number of features to consider when looking for the best split (default \"auto\" but sparse dataset)\n",
    "    'clf__max_features': [\"auto\", 10],\n",
    "    #maximum depth of the tree (default None)\n",
    "    'clf__max_depth': [10, None],\n",
    "\n",
    "    \n",
    "    \n",
    "    #TFIDF Vectorizer\n",
    "    #Whether the feature should be made of word or character n-grams (default word)\n",
    "    \"tfidf__analyzer\": [\"word\"],\n",
    "    #Smooth idf weights by adding one to document frequencies, \n",
    "    #as if an extra document was seen containing every term in the collection exactly once.\n",
    "    #Prevents zero divisions (default True)\n",
    "    \"tfidf__smooth_idf\": [True, False]\n",
    "\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "#build a pipeline\n",
    "model_3_pipeline = Pipeline([\n",
    "        #joins list into a single string\n",
    "        ('join', JoinElement()),\n",
    "        #tfidf vectorizer\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        #select 1000 best from word vectors\n",
    "        ('k_best', SelectKBest()),\n",
    "        #apply naive bayes\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "#design grid search\n",
    "grid_search_model_3 = GridSearchCV(\n",
    "    #pipeline to be followed\n",
    "    model_3_pipeline,\n",
    "    #parameters\n",
    "    param_grid=parameters_model_3,\n",
    "    #number of folds for CV\n",
    "    cv=5,\n",
    "    #scoring to be considered for the cv\n",
    "    scoring = \"accuracy\",\n",
    "    #parallelize if possible\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#fit the grid search for training data\n",
    "grid_search_model_3.fit(file_contents, targets)\n",
    "\n",
    "#save results of cross validation\n",
    "cv_results_model_3 = pd.DataFrame(grid_search_model_3.cv_results_)\n",
    "\n",
    "#filter columns to be kept in the dataframe\n",
    "filter_col = [col for col in cv_results_model_3 if (col.startswith(\"param_\") or col.startswith(\"mean_\") or col.startswith(\"rank\"))]\n",
    "\n",
    "#save results with only filtered columns\n",
    "cv_results_model_3 = cv_results_model_3[filter_col]\n",
    "\n",
    "#save name of the model for later comparison\n",
    "cv_results_model_3.insert(loc=0, column=\"Model\", value= \"3\")\n",
    "\n",
    "#round mean_test_score\n",
    "cv_results_model_3[\"mean_test_score\"] = cv_results_model_3[\"mean_test_score\"].round(2)\n",
    "\n",
    "#show best 5 sorted by mean_test_score\n",
    "display(cv_results_model_3.sort_values(by=\"mean_test_score\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-morgan",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Model 4: Word2Vec and SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-booking",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-26T10:29:39.934Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#choose parameters for the different steps in the pipeline\n",
    "parameters_model_4 = {\n",
    "    \n",
    "    #SVC\n",
    "    #Specifies the kernel type to be used in the algorithm\n",
    "    \"SVC__kernel\" : [\"linear\", \"poly\", \"sigmoid\"],\n",
    "    #Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid'\n",
    "    \"SVC__gamma\": [1,0.1,0.001],\n",
    "    #Regularization parameter. The strength of the regularization is inversely proportional to C. \n",
    "    #Must be strictly positive. The penalty is a squared l2 penalty.\n",
    "    \"SVC__C\": [0.1,1, 10, 100]\n",
    "    \n",
    "    #defaults for Word2Vec\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "#build a pipeline\n",
    "model_4_pipeline = Pipeline([\n",
    "        #tfidf vectorizer\n",
    "        ('word2vec', MeanEmbeddingVectorizer()),\n",
    "        #apply naive bayes\n",
    "        ('SVC', SVC())\n",
    "    ])\n",
    "\n",
    "#design grid search\n",
    "grid_search_model_4 = GridSearchCV(\n",
    "    #pipeline to be followed\n",
    "    model_4_pipeline,\n",
    "    #parameters\n",
    "    param_grid=parameters_model_4,\n",
    "    #number of folds for CV\n",
    "    cv=5,\n",
    "    #scoring to be considered for the cv\n",
    "    scoring = \"accuracy\",\n",
    "    #parallelize if possible\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#fit the grid search for training data\n",
    "grid_search_model_4.fit(file_contents, targets)\n",
    "\n",
    "#save results of cross validation\n",
    "cv_results_model_4 = pd.DataFrame(grid_search_model_4.cv_results_)\n",
    "\n",
    "#filter columns to be kept in the dataframe\n",
    "filter_col = [col for col in cv_results_model_4 if (col.startswith(\"param_\") or col.startswith(\"mean_\") or col.startswith(\"rank\"))]\n",
    "\n",
    "#save results with only filtered columns\n",
    "cv_results_model_4 = cv_results_model_4[filter_col]\n",
    "\n",
    "#save name of the model for later comparison\n",
    "cv_results_model_4.insert(loc=0, column=\"Model\", value= \"4\")\n",
    "\n",
    "#round mean_test_score\n",
    "cv_results_model_4[\"mean_test_score\"] = cv_results_model_4[\"mean_test_score\"].round(2)\n",
    "\n",
    "#show best 5 sorted by mean_test_score\n",
    "display(cv_results_model_4.sort_values(by=\"mean_test_score\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-wiring",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Model 5: Word2Vec and Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-message",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-26T10:29:39.938Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#choose parameters for the different steps in the pipeline\n",
    "parameters_model_5 = {\n",
    "    \n",
    "    #RF classifier\n",
    "    #nThe number of trees in the forest (default is 100)\n",
    "    \"clf__n_estimators\" : [10,100,200],\n",
    "    #The minimum number of samples required to split an internal node (default is 2 but it is a large dataset)\n",
    "    \"clf__min_samples_split\": [10, 40, 80],\n",
    "    #The number of features to consider when looking for the best split (default \"auto\" but sparse dataset)\n",
    "    'clf__max_features': [\"auto\", 10],\n",
    "    #maximum depth of the tree (default None)\n",
    "    'clf__max_depth': [10, None]\n",
    "    \n",
    "    #defaults word2vec\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "#build a pipeline\n",
    "model_5_pipeline = Pipeline([\n",
    "        #tfidf vectorizer\n",
    "        ('word2vec', MeanEmbeddingVectorizer()),\n",
    "        #apply naive bayes\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "#design grid search\n",
    "grid_search_model_5 = GridSearchCV(\n",
    "    #pipeline to be followed\n",
    "    model_5_pipeline,\n",
    "    #parameters\n",
    "    param_grid=parameters_model_5,\n",
    "    #number of folds for CV\n",
    "    cv=5,\n",
    "    #scoring to be considered for the cv\n",
    "    scoring = \"accuracy\",\n",
    "    #parallelize if possible\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#fit the grid search for training data\n",
    "grid_search_model_5.fit(file_contents, targets)\n",
    "\n",
    "#save results of cross validation\n",
    "cv_results_model_5 = pd.DataFrame(grid_search_model_5.cv_results_)\n",
    "\n",
    "#filter columns to be kept in the dataframe\n",
    "filter_col = [col for col in cv_results_model_5 if (col.startswith(\"param_\") or col.startswith(\"mean_\") or col.startswith(\"rank\"))]\n",
    "\n",
    "#save results with only filtered columns\n",
    "cv_results_model_5 = cv_results_model_5[filter_col]\n",
    "\n",
    "#save name of the model for later comparison\n",
    "cv_results_model_5.insert(loc=0, column=\"Model\", value= \"5\")\n",
    "\n",
    "#round mean_test_score\n",
    "cv_results_model_5[\"mean_test_score\"] = cv_results_model_5[\"mean_test_score\"].round(2)\n",
    "\n",
    "#show best 5 sorted by mean_test_score\n",
    "display(cv_results_model_5.sort_values(by=\"mean_test_score\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-vacation",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Model 6: Doc2Vec and Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-tribune",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-26T10:29:39.942Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#choose parameters for the different steps in the pipeline\n",
    "parameters_model_6 = {\n",
    "    \n",
    "    #SVC\n",
    "    #Specifies the kernel type to be used in the algorithm\n",
    "    \"SVC__kernel\" : [\"linear\", \"poly\", \"sigmoid\"],\n",
    "    #Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid'\n",
    "    \"SVC__gamma\": [1,0.1,0.001],\n",
    "    #Regularization parameter. The strength of the regularization is inversely proportional to C. \n",
    "    #Must be strictly positive. The penalty is a squared l2 penalty.\n",
    "    \"SVC__C\": [0.1,1, 10, 100]\n",
    "    \n",
    "    #defaults doc2vec\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "#build a pipeline\n",
    "model_6_pipeline = Pipeline([\n",
    "        #tfidf vectorizer\n",
    "        ('doc2vec', Doc2Vectorizer()),\n",
    "        #apply naive bayes\n",
    "        ('SVC', SVC())\n",
    "    ])\n",
    "\n",
    "#design grid search\n",
    "grid_search_model_6 = GridSearchCV(\n",
    "    #pipeline to be followed\n",
    "    model_6_pipeline,\n",
    "    #parameters\n",
    "    param_grid=parameters_model_6,\n",
    "    #number of folds for CV\n",
    "    cv=5,\n",
    "    #scoring to be considered for the cv\n",
    "    scoring = \"accuracy\",\n",
    "    #parallelize if possible\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#fit the grid search for training data\n",
    "grid_search_model_6.fit(file_contents, targets)\n",
    "\n",
    "#save results of cross validation\n",
    "cv_results_model_6 = pd.DataFrame(grid_search_model_6.cv_results_)\n",
    "\n",
    "#filter columns to be kept in the dataframe\n",
    "filter_col = [col for col in cv_results_model_6 if (col.startswith(\"param_\") or col.startswith(\"mean_\") or col.startswith(\"rank\"))]\n",
    "\n",
    "#save results with only filtered columns\n",
    "cv_results_model_6 = cv_results_model_6[filter_col]\n",
    "\n",
    "#save name of the model for later comparison\n",
    "cv_results_model_6.insert(loc=0, column=\"Model\", value= \"6\")\n",
    "\n",
    "#round mean_test_score\n",
    "cv_results_model_6[\"mean_test_score\"] = cv_results_model_6[\"mean_test_score\"].round(2)\n",
    "\n",
    "#show best 5 sorted by mean_test_score\n",
    "display(cv_results_model_6.sort_values(by=\"mean_test_score\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-soldier",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Model 7: Doc2Vec and Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-freight",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-26T10:29:39.947Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#choose parameters for the different steps in the pipeline\n",
    "parameters_model_7 = {\n",
    "    \n",
    "    \n",
    "    #RF classifier\n",
    "    #nThe number of trees in the forest (default is 100)\n",
    "    \"clf__n_estimators\" : [10,100,200],\n",
    "    #The minimum number of samples required to split an internal node (default is 2 but it is a large dataset)\n",
    "    \"clf__min_samples_split\": [10, 40, 80],\n",
    "    #The number of features to consider when looking for the best split (default \"auto\" but sparse dataset)\n",
    "    'clf__max_features': [\"auto\", 10],\n",
    "    #maximum depth of the tree (default None)\n",
    "    'clf__max_depth': [10, None]\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "#build a pipeline\n",
    "model_7_pipeline = Pipeline([\n",
    "        #tfidf vectorizer\n",
    "        ('doc2vec', Doc2Vectorizer()),\n",
    "        #apply naive bayes\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "#design grid search\n",
    "grid_search_model_7 = GridSearchCV(\n",
    "    #pipeline to be followed\n",
    "    model_7_pipeline,\n",
    "    #parameters\n",
    "    param_grid=parameters_model_7,\n",
    "    #number of folds for CV\n",
    "    cv=5,\n",
    "    #scoring to be considered for the cv\n",
    "    scoring = \"accuracy\",\n",
    "    #parallelize if possible\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#fit the grid search for training data\n",
    "grid_search_model_7.fit(file_contents, targets)\n",
    "\n",
    "#save results of cross validation\n",
    "cv_results_model_7 = pd.DataFrame(grid_search_model_7.cv_results_)\n",
    "\n",
    "#filter columns to be kept in the dataframe\n",
    "filter_col = [col for col in cv_results_model_7 if (col.startswith(\"param_\") or col.startswith(\"mean_\") or col.startswith(\"rank\"))]\n",
    "\n",
    "#save results with only filtered columns\n",
    "cv_results_model_7 = cv_results_model_7[filter_col]\n",
    "\n",
    "#save name of the model for later comparison\n",
    "cv_results_model_7.insert(loc=0, column=\"Model\", value= \"7\")\n",
    "\n",
    "#round mean_test_score\n",
    "cv_results_model_7[\"mean_test_score\"] = cv_results_model_7[\"mean_test_score\"].round(2)\n",
    "\n",
    "#show best 5 sorted by mean_test_score\n",
    "display(cv_results_model_7.sort_values(by=\"mean_test_score\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-congo",
   "metadata": {},
   "source": [
    "### Compare CV results from trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-relief",
   "metadata": {},
   "source": [
    "In this section, the results from CV are compared within the trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-friendship",
   "metadata": {},
   "source": [
    "#### Raw results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-springfield",
   "metadata": {},
   "source": [
    "⚠️ Export results from grid serach. This allows us to experiment with visualiazations and results from CV without having to rerun the whole script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-lyric",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-26T10:29:39.954Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_results.to_csv(\"cv_results.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-franklin",
   "metadata": {},
   "source": [
    "A dataframe showing the best models according to the **mean accuracy within the test folds** used for cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-account",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-26T10:29:39.958Z"
    }
   },
   "outputs": [],
   "source": [
    "#merge cv results into 1 that keeps the relevant information\n",
    "\n",
    "#empty dataframe that will keep all the results\n",
    "cv_results = pd.DataFrame()\n",
    "\n",
    "#loop over cv results\n",
    "for i in [cv_results_model_1, cv_results_model_2, cv_results_model_3, cv_results_model_4,\n",
    "          cv_results_model_5, cv_results_model_6, cv_results_model_7]:\n",
    "    \n",
    "    #select relevant columns\n",
    "    selected = i[[\"Model\",\"mean_fit_time\",\"mean_score_time\",\"mean_test_score\"]]\n",
    "    \n",
    "    #append to cv results\n",
    "    cv_results = cv_results.append(selected)\n",
    "    \n",
    "#show models with best scores\n",
    "display(cv_results.sort_values(by=\"mean_test_score\", ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-klein",
   "metadata": {},
   "source": [
    "⚠️ *This table could be improved by also indicating the parameters used in each model but I thought it would be a bit overwhelming*\n",
    "\n",
    "⚠️⚠️ Models x y seem to achieve a better accuracy since they appear more often within the first positions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heated-diameter",
   "metadata": {},
   "source": [
    "#### Tradeoff score vs mean fit time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-parent",
   "metadata": {},
   "source": [
    "A plot to check if there is some kind of tradeoff between accuracy and runtime of the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-calendar",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-26T10:29:39.964Z"
    }
   },
   "outputs": [],
   "source": [
    "#group by model\n",
    "groups = cv_results.groupby(\"Model\")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "for name, group in groups:\n",
    "    ax.plot(group.mean_fit_time, group.mean_test_score, marker='o', linestyle='', ms=12, label=\"Model %s\" %name)\n",
    "ax.legend(loc = 1)\n",
    "plt.xlabel(\"Mean fit time\")\n",
    "plt.ylabel(\"Mean test Score (accuracy)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-paint",
   "metadata": {},
   "source": [
    "⚠️⚠️ Clear relation between runtime of the models and accuracy within test folds?\n",
    "\n",
    "⚠️⚠️ Some pre-processing or classifier takes more time to be run?\n",
    "\n",
    "⚠️⚠️ Further analysis on what it increases runtime of the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-venice",
   "metadata": {},
   "source": [
    "#### Best estimator from each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-nashville",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-26T10:29:39.970Z"
    }
   },
   "outputs": [],
   "source": [
    "#group by model and take best mean_test_score for each type of model\n",
    "best_models = cv_results.groupby(\"Model\")[[\"mean_test_score\"]].max()\n",
    "\n",
    "#plot\n",
    "plt.bar(best_models.index, best_models[\"mean_test_score\"])\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Mean_test_score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-legend",
   "metadata": {},
   "source": [
    "⚠️⚠️ As expected best models are x and y\n",
    "\n",
    "⚠️⚠️ Worse models are x and y\n",
    "\n",
    "⚠️⚠️ Are they stable to changes in hyperparameters? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-month",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-26T10:29:39.974Z"
    }
   },
   "outputs": [],
   "source": [
    "#mean of mean scores within the folds for each model\n",
    "stability = cv_results.groupby(\"Model\")[[\"mean_test_score\"]].mean()\n",
    "#standard deviation of the same\n",
    "stability[\"standard_deviation\"] = cv_results.groupby(\"Model\")[[\"mean_test_score\"]].std()\n",
    "\n",
    "#show\n",
    "stability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-lottery",
   "metadata": {},
   "source": [
    "⚠️⚠️ Model x is not very stable to the change of hyperparameters. What is influencing it so much?\n",
    "\n",
    "⚠️⚠️ Show an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-failure",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-26T10:29:39.979Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(cv_results_model_1[\"param_MNB__alpha\"], cv_results_model_1[\"mean_test_score\"], marker=\"o\")\n",
    "plt.xlabel(\"Parameter alpha\")\n",
    "plt.ylabel(\"Mean_test_score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-louisville",
   "metadata": {},
   "source": [
    "#### Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-andorra",
   "metadata": {},
   "source": [
    "This grid searches helped us to see which combinations of models and hyperparameters are expected to be the best, how stable they are and other insights. However, **we cannot draw strong conclusions on this since we are still dealing with train data.** This step is only helping us to understand the models better and choose the ones with which we want to test (or validate). To proceed further we decided to select **the best combination of hyperparameters for each of the 7 models, predict on test data**, evaluate and draw conclusions. That is done in the next section of this notebook. We know that this are not strictly the best 7 models (see raw results), but we wanted to include more diversity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-hearts",
   "metadata": {},
   "source": [
    "## Evaluation and model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-marks",
   "metadata": {},
   "source": [
    "**Predict on test data** using the best combinations of hyperparameters used in the models obtained in the training phase and evaluate using different metrics.\n",
    "\n",
    "⚠️ *This simulates predictions on unseen data. However, since it is done for many models and then we will choose the best model out of them, it behaves more like a validation set that would help us choose which model we would apply to actually unseen data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-roads",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-26T10:29:39.986Z"
    }
   },
   "outputs": [],
   "source": [
    "#add models to be evaluated\n",
    "models = [\n",
    "    grid_search_model_1,\n",
    "    grid_search_model_2,\n",
    "    grid_search_model_3,\n",
    "    grid_search_model_4,\n",
    "    grid_search_model_5,\n",
    "    grid_search_model_6,\n",
    "    grid_search_model_7\n",
    "]\n",
    "\n",
    "evaluation = pd.DataFrame(columns=[\"model\"\n",
    "                                   , \"mean_fit_time\", \"accuracy\"\n",
    "                                   , \"recall_macro\", \"recall_micro\"\n",
    "                                   , \"precision_macro\", \"precision_micro\"\n",
    "                                   , \"f1_macro\", \"f1_micro\"\n",
    "                                   , \"model_definition\"\n",
    "                                  ])\n",
    "\n",
    "i = -1 # Ensure that first item is index 0 in the loop\n",
    "for model_ in models:\n",
    "    # Yucky method of finding mean fit times:\n",
    "    i = i +1\n",
    "    mean_fit_time = cv_results.groupby(\"Model\")[\"mean_fit_time\"].mean()[i]\n",
    "    \n",
    "    # Predict\n",
    "    preds = model_.best_estimator_.predict(file_contents_test)\n",
    "    model = cv_results.iloc[model_.best_index_,0]\n",
    "\n",
    "    # Calculate metrix\n",
    "    to_append = [\n",
    "            \"Model \" + str(i+1),\n",
    "            mean_fit_time, \n",
    "            accuracy_score(y_true=targets_test,y_pred=preds),\n",
    "            #choose micro or macro according to criteria\n",
    "            recall_score(y_true=targets_test,y_pred=preds, average=\"macro\"),\n",
    "            recall_score(y_true=targets_test,y_pred=preds, average=\"micro\"),\n",
    "            precision_score(y_true=targets_test,y_pred=preds, average=\"macro\"),\n",
    "            precision_score(y_true=targets_test,y_pred=preds, average=\"micro\"),\n",
    "            f1_score(y_true=targets_test,y_pred=preds, average=\"macro\"),\n",
    "            f1_score(y_true=targets_test,y_pred=preds, average=\"micro\"),\n",
    "            model_\n",
    "            ]\n",
    "    \n",
    "    #Append Metrics\n",
    "    evaluation_length = len(evaluation)\n",
    "    evaluation.loc[evaluation_length] = to_append\n",
    "    \n",
    "    # Print results and Confusion Matrix for each model\n",
    "    print(\"####################################################################\")\n",
    "    print(\"####################################################################\")\n",
    "    print(\"                        Model \"+ str(i+1) + \":\")\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    \n",
    "    print(evaluation.loc[i, 'mean_fit_time':'f1_macro'])\n",
    "    print(\"\\n\")\n",
    "    print(\"Pipeline: \")\n",
    "    print(model_.best_estimator_)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"Confusion Matrix: \")\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    plot_confusion_matrix(estimator=model_.best_estimator_\n",
    "                          , X=file_contents_test\n",
    "                          , y_true=targets_test\n",
    "                          , ax=ax\n",
    "                         )\n",
    "    plt.show()\n",
    "    \n",
    "# Print table of the models compared and sorted:\n",
    "evaluation.sort_values(by=\"accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-multimedia",
   "metadata": {},
   "source": [
    "The **best estimator we found after everything is:** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-listing",
   "metadata": {},
   "source": [
    "Main **conclusion**: for unseen data, we would choose to use the estimator from above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-waters",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-niger",
   "metadata": {},
   "source": [
    "**Draw conclusions from above. but add more evaulations and stuff....**\n",
    "**Should probably be done after proper train/test is defined, and full model is tried**\n",
    "\n",
    "Some observations:\n",
    "* Model 1,2,3 (TFIDF) seems to work best overall, no matter training size\n",
    "* Model 5,6,7 (Doc2Vec) takes by far the most time, but their accuracy greatly increased when increasing training set\n",
    "\n",
    "\n",
    "As often is the case in the fields of science, not all research leads to useable results. We ended up having to remodel our plans several times during this project, including a complete pivot of the datasets.\n",
    "\n",
    "This did however give us some insight into how larger projects are managed. This also lead us to an interesting path of looking at a relatively obscure language.\n",
    "\n",
    "Although further works is possible, we reached the conclusion that there is a change in the Icelandic spoken language throughout time, and it is therefore possible to train models that estimates which decade a given speech is from. However, take into account what explained in section 3.2: it may be also due to other factors (for instance, topic used).\n",
    "\n",
    "Overall we did work with Data-Oriented Programming best practices. We were able to develop a scientific workflow. From the given data, we managed to train a model for prediction on test data with **decent results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-crash",
   "metadata": {},
   "source": [
    "# Further Works\n",
    "As we drilled down this dataset, we kept getting new ideas that we would like to experiment with, and try to gain better insight.\n",
    "Specifically, our next steps would be:\n",
    "\n",
    "## Predict Different Sources\n",
    "As it currently stands, we are trying to estimate a decade of speeches from \"Althingi\". However, the dataset has several other sources of Icelandic; both written and spoken (TV scripts, cinema and others).\n",
    "\n",
    "We would like to see if it was possible to extend our model to be able to classify the source.\n",
    "\n",
    "## Treating years as Contious Variables\n",
    "We are currently treating decades as a class. By discretizing results from a regression algorithm, we think it should be possible to keep some nominal knowledge of the ordering of the years, and thus improving our predictions. It would be also interesting to see if we can achieve also decent prediction by narrowing a bit the intervals for the years (instead of decades, lustrums). And of course, it would be interesting to rerun the model in a more powerful machine using all the decades instead of discarding the intermediate ones.\n",
    "\n",
    "## Gaining insight into Explanatory Variables\n",
    "From our results, it is clear that it is somewhat possible to predict the decades. However, we are still treating the algorithms as \"Black Boxes\". \n",
    "We would like to dive deeper into the decision treas/boundaries, to see if we can locate what it is that makes the predictions possible. It might be new words introduced, semantic changes, or something entirely different.\n",
    "\n",
    "\n",
    "## Additional Feature Extraction and Classifiers\n",
    "We would like to extend the list to include more classifiers, as well as trying to develop some additional feature extractions.\n",
    "E.g. \"Glove Embedding\"\n",
    "E.g. \"Neural networks\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "105px",
    "width": "242px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "421.771px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

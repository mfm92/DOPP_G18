{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "behavioral-family",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction-to-Assignment\" data-toc-modified-id=\"Introduction-to-Assignment-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction to Assignment</a></span><ul class=\"toc-item\"><li><span><a href=\"#First-draft\" data-toc-modified-id=\"First-draft-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>First draft</a></span><ul class=\"toc-item\"><li><span><a href=\"#Topic-and-Questions-to-answer\" data-toc-modified-id=\"Topic-and-Questions-to-answer-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Topic and Questions to answer</a></span></li><li><span><a href=\"#Justification-For-Limit-Of-Scope\" data-toc-modified-id=\"Justification-For-Limit-Of-Scope-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Justification For Limit Of Scope</a></span></li><li><span><a href=\"#Workflow-plan-&amp;-Project-management\" data-toc-modified-id=\"Workflow-plan-&amp;-Project-management-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Workflow plan &amp; Project management</a></span></li><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-1.1.4\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;</span>Data</a></span></li></ul></li><li><span><a href=\"#Second-Draft\" data-toc-modified-id=\"Second-Draft-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Second Draft</a></span></li><li><span><a href=\"#Pivoting-Point\" data-toc-modified-id=\"Pivoting-Point-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Pivoting Point</a></span></li><li><span><a href=\"#Language-change-in-Icelandic-Parliamentary-Speeches\" data-toc-modified-id=\"Language-change-in-Icelandic-Parliamentary-Speeches-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Language change in Icelandic Parliamentary Speeches</a></span></li></ul></li><li><span><a href=\"#Estimating-publication-year-from-Project-Gutenberg\" data-toc-modified-id=\"Estimating-publication-year-from-Project-Gutenberg-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Estimating publication year from Project Gutenberg</a></span><ul class=\"toc-item\"><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-packages\" data-toc-modified-id=\"Import-packages-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Import packages</a></span></li><li><span><a href=\"#Define-Constants\" data-toc-modified-id=\"Define-Constants-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Define Constants</a></span></li></ul></li><li><span><a href=\"#Importing-the-data\" data-toc-modified-id=\"Importing-the-data-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Importing the data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Getting-the-content\" data-toc-modified-id=\"Getting-the-content-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Getting the content</a></span></li><li><span><a href=\"#Data-Cleansing\" data-toc-modified-id=\"Data-Cleansing-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Data Cleansing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Read-a-single-file\" data-toc-modified-id=\"Read-a-single-file-2.2.2.1\"><span class=\"toc-item-num\">2.2.2.1&nbsp;&nbsp;</span>Read a single file</a></span></li><li><span><a href=\"#Return-list-of-all-words\" data-toc-modified-id=\"Return-list-of-all-words-2.2.2.2\"><span class=\"toc-item-num\">2.2.2.2&nbsp;&nbsp;</span>Return list of all words</a></span></li></ul></li></ul></li><li><span><a href=\"#Statistics\" data-toc-modified-id=\"Statistics-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Statistics</a></span><ul class=\"toc-item\"><li><span><a href=\"#First-attempt\" data-toc-modified-id=\"First-attempt-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>First attempt</a></span></li><li><span><a href=\"#Read-all-files,-and-do-preprocessing\" data-toc-modified-id=\"Read-all-files,-and-do-preprocessing-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Read all files, and do preprocessing</a></span></li><li><span><a href=\"#Compare-Word-ranking-between-titles\" data-toc-modified-id=\"Compare-Word-ranking-between-titles-2.3.3\"><span class=\"toc-item-num\">2.3.3&nbsp;&nbsp;</span>Compare Word ranking between titles</a></span></li></ul></li><li><span><a href=\"#Second-testing\" data-toc-modified-id=\"Second-testing-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Second testing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Read-in-from-the-decades-files,-and-see-the-distributions\" data-toc-modified-id=\"Read-in-from-the-decades-files,-and-see-the-distributions-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Read in from the decades files, and see the distributions</a></span></li><li><span><a href=\"#Preliminary-Conclusion\" data-toc-modified-id=\"Preliminary-Conclusion-2.4.2\"><span class=\"toc-item-num\">2.4.2&nbsp;&nbsp;</span>Preliminary Conclusion</a></span></li><li><span><a href=\"#Compare-ranking-between-upload-decades\" data-toc-modified-id=\"Compare-ranking-between-upload-decades-2.4.3\"><span class=\"toc-item-num\">2.4.3&nbsp;&nbsp;</span>Compare ranking between upload-decades</a></span></li></ul></li><li><span><a href=\"#Trying-to-fit-models-to-predict\" data-toc-modified-id=\"Trying-to-fit-models-to-predict-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Trying to fit models to predict</a></span><ul class=\"toc-item\"><li><span><a href=\"#Read-in-files\" data-toc-modified-id=\"Read-in-files-2.5.1\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;</span>Read in files</a></span></li><li><span><a href=\"#Train-models\" data-toc-modified-id=\"Train-models-2.5.2\"><span class=\"toc-item-num\">2.5.2&nbsp;&nbsp;</span>Train models</a></span></li></ul></li><li><span><a href=\"#Realisation-and-conclusion\" data-toc-modified-id=\"Realisation-and-conclusion-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Realisation and conclusion</a></span></li></ul></li><li><span><a href=\"#Studying-language-change-in-Icelandic-parliamentary-speeches\" data-toc-modified-id=\"Studying-language-change-in-Icelandic-parliamentary-speeches-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Studying language change in Icelandic parliamentary speeches</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#The-task\" data-toc-modified-id=\"The-task-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>The task</a></span></li><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-required-libraries\" data-toc-modified-id=\"Load-required-libraries-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Load required libraries</a></span></li><li><span><a href=\"#Get-the-data\" data-toc-modified-id=\"Get-the-data-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Get the data</a></span></li><li><span><a href=\"#Preprocessing-helpers\" data-toc-modified-id=\"Preprocessing-helpers-3.3.3\"><span class=\"toc-item-num\">3.3.3&nbsp;&nbsp;</span>Preprocessing helpers</a></span></li></ul></li><li><span><a href=\"#Preliminary-Data-Analysis\" data-toc-modified-id=\"Preliminary-Data-Analysis-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Preliminary Data Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Zipf's-Law\" data-toc-modified-id=\"Zipf's-Law-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>Zipf's Law</a></span></li><li><span><a href=\"#Disappearing-words-/-new-words\" data-toc-modified-id=\"Disappearing-words-/-new-words-3.4.2\"><span class=\"toc-item-num\">3.4.2&nbsp;&nbsp;</span>Disappearing words / new words</a></span></li><li><span><a href=\"#Development-of-average-sentence-length\" data-toc-modified-id=\"Development-of-average-sentence-length-3.4.3\"><span class=\"toc-item-num\">3.4.3&nbsp;&nbsp;</span>Development of average sentence length</a></span></li><li><span><a href=\"#n-grams\" data-toc-modified-id=\"n-grams-3.4.4\"><span class=\"toc-item-num\">3.4.4&nbsp;&nbsp;</span>n-grams</a></span></li></ul></li><li><span><a href=\"#Building-model-for-classifying-speeches\" data-toc-modified-id=\"Building-model-for-classifying-speeches-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Building model for classifying speeches</a></span><ul class=\"toc-item\"><li><span><a href=\"#Constructing-training-and-test-data\" data-toc-modified-id=\"Constructing-training-and-test-data-3.5.1\"><span class=\"toc-item-num\">3.5.1&nbsp;&nbsp;</span>Constructing training and test data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Train-data\" data-toc-modified-id=\"Train-data-3.5.1.1\"><span class=\"toc-item-num\">3.5.1.1&nbsp;&nbsp;</span>Train data</a></span></li><li><span><a href=\"#Test-data\" data-toc-modified-id=\"Test-data-3.5.1.2\"><span class=\"toc-item-num\">3.5.1.2&nbsp;&nbsp;</span>Test data</a></span></li><li><span><a href=\"#See-classes-distribution-within-train-and-test-sets\" data-toc-modified-id=\"See-classes-distribution-within-train-and-test-sets-3.5.1.3\"><span class=\"toc-item-num\">3.5.1.3&nbsp;&nbsp;</span>See classes distribution within train and test sets</a></span></li></ul></li><li><span><a href=\"#Text-feature-extraction\" data-toc-modified-id=\"Text-feature-extraction-3.5.2\"><span class=\"toc-item-num\">3.5.2&nbsp;&nbsp;</span>Text feature extraction</a></span><ul class=\"toc-item\"><li><span><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-3.5.2.1\"><span class=\"toc-item-num\">3.5.2.1&nbsp;&nbsp;</span>TF-IDF</a></span></li><li><span><a href=\"#Word2Vec\" data-toc-modified-id=\"Word2Vec-3.5.2.2\"><span class=\"toc-item-num\">3.5.2.2&nbsp;&nbsp;</span>Word2Vec</a></span></li><li><span><a href=\"#Doc2Vec\" data-toc-modified-id=\"Doc2Vec-3.5.2.3\"><span class=\"toc-item-num\">3.5.2.3&nbsp;&nbsp;</span>Doc2Vec</a></span></li></ul></li><li><span><a href=\"#Classifiers\" data-toc-modified-id=\"Classifiers-3.5.3\"><span class=\"toc-item-num\">3.5.3&nbsp;&nbsp;</span>Classifiers</a></span><ul class=\"toc-item\"><li><span><a href=\"#Multinominal-Naive-Bayes-(MNB)\" data-toc-modified-id=\"Multinominal-Naive-Bayes-(MNB)-3.5.3.1\"><span class=\"toc-item-num\">3.5.3.1&nbsp;&nbsp;</span>Multinominal Naive Bayes (MNB)</a></span></li><li><span><a href=\"#Support-Vector-Machines\" data-toc-modified-id=\"Support-Vector-Machines-3.5.3.2\"><span class=\"toc-item-num\">3.5.3.2&nbsp;&nbsp;</span>Support Vector Machines</a></span></li><li><span><a href=\"#Random-Forest-Classifier\" data-toc-modified-id=\"Random-Forest-Classifier-3.5.3.3\"><span class=\"toc-item-num\">3.5.3.3&nbsp;&nbsp;</span>Random Forest Classifier</a></span></li></ul></li><li><span><a href=\"#Train-models\" data-toc-modified-id=\"Train-models-3.5.4\"><span class=\"toc-item-num\">3.5.4&nbsp;&nbsp;</span>Train models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model-1:-TFIDF-vectorizer,-select-K-best-and-Multinomial-Naive-Bayes\" data-toc-modified-id=\"Model-1:-TFIDF-vectorizer,-select-K-best-and-Multinomial-Naive-Bayes-3.5.4.1\"><span class=\"toc-item-num\">3.5.4.1&nbsp;&nbsp;</span>Model 1: TFIDF vectorizer, select K best and Multinomial Naive Bayes</a></span></li><li><span><a href=\"#Model-2:-TFIDF-vectorizer,-select-K-best-and-SVC\" data-toc-modified-id=\"Model-2:-TFIDF-vectorizer,-select-K-best-and-SVC-3.5.4.2\"><span class=\"toc-item-num\">3.5.4.2&nbsp;&nbsp;</span>Model 2: TFIDF vectorizer, select K best and SVC</a></span></li><li><span><a href=\"#Model-3:-TFIDF-vectorizer,-select-K-best-and-Random-Forest-Classifier\" data-toc-modified-id=\"Model-3:-TFIDF-vectorizer,-select-K-best-and-Random-Forest-Classifier-3.5.4.3\"><span class=\"toc-item-num\">3.5.4.3&nbsp;&nbsp;</span>Model 3: TFIDF vectorizer, select K best and Random Forest Classifier</a></span></li><li><span><a href=\"#Model-4:-Word2Vec-and-SVC\" data-toc-modified-id=\"Model-4:-Word2Vec-and-SVC-3.5.4.4\"><span class=\"toc-item-num\">3.5.4.4&nbsp;&nbsp;</span>Model 4: Word2Vec and SVC</a></span></li><li><span><a href=\"#Model-5:-Word2Vec-and-Random-Forest-Classifier\" data-toc-modified-id=\"Model-5:-Word2Vec-and-Random-Forest-Classifier-3.5.4.5\"><span class=\"toc-item-num\">3.5.4.5&nbsp;&nbsp;</span>Model 5: Word2Vec and Random Forest Classifier</a></span></li><li><span><a href=\"#Model-6:-Doc2Vec-and-Support-Vector-Machines\" data-toc-modified-id=\"Model-6:-Doc2Vec-and-Support-Vector-Machines-3.5.4.6\"><span class=\"toc-item-num\">3.5.4.6&nbsp;&nbsp;</span>Model 6: Doc2Vec and Support Vector Machines</a></span></li><li><span><a href=\"#Model-7:-Doc2Vec-and-Random-Forest-Classifier\" data-toc-modified-id=\"Model-7:-Doc2Vec-and-Random-Forest-Classifier-3.5.4.7\"><span class=\"toc-item-num\">3.5.4.7&nbsp;&nbsp;</span>Model 7: Doc2Vec and Random Forest Classifier</a></span></li></ul></li><li><span><a href=\"#Compare-CV-results-from-trained-models\" data-toc-modified-id=\"Compare-CV-results-from-trained-models-3.5.5\"><span class=\"toc-item-num\">3.5.5&nbsp;&nbsp;</span>Compare CV results from trained models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Raw-results\" data-toc-modified-id=\"Raw-results-3.5.5.1\"><span class=\"toc-item-num\">3.5.5.1&nbsp;&nbsp;</span>Raw results</a></span></li><li><span><a href=\"#Tradeoff-score-vs-mean-fit-time\" data-toc-modified-id=\"Tradeoff-score-vs-mean-fit-time-3.5.5.2\"><span class=\"toc-item-num\">3.5.5.2&nbsp;&nbsp;</span>Tradeoff score vs mean fit time</a></span></li><li><span><a href=\"#Best-estimator-from-each-model\" data-toc-modified-id=\"Best-estimator-from-each-model-3.5.5.3\"><span class=\"toc-item-num\">3.5.5.3&nbsp;&nbsp;</span>Best estimator from each model</a></span></li><li><span><a href=\"#Next-steps\" data-toc-modified-id=\"Next-steps-3.5.5.4\"><span class=\"toc-item-num\">3.5.5.4&nbsp;&nbsp;</span>Next steps</a></span></li></ul></li></ul></li><li><span><a href=\"#Evaluation-and-model-selection\" data-toc-modified-id=\"Evaluation-and-model-selection-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Evaluation and model selection</a></span></li></ul></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Conclusion</a></span></li><li><span><a href=\"#Further-Works\" data-toc-modified-id=\"Further-Works-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Further Works</a></span><ul class=\"toc-item\"><li><span><a href=\"#Predict-Different-Sources\" data-toc-modified-id=\"Predict-Different-Sources-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Predict Different Sources</a></span></li><li><span><a href=\"#Treating-years-as-Contious-Variables\" data-toc-modified-id=\"Treating-years-as-Contious-Variables-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Treating years as Contious Variables</a></span></li><li><span><a href=\"#Gaining-insight-into-Explanatory-Variables\" data-toc-modified-id=\"Gaining-insight-into-Explanatory-Variables-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Gaining insight into Explanatory Variables</a></span></li><li><span><a href=\"#Additional-Feature-Extraction-and-Classifiers\" data-toc-modified-id=\"Additional-Feature-Extraction-and-Classifiers-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Additional Feature Extraction and Classifiers</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-border",
   "metadata": {},
   "source": [
    "# Introduction to Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-occupation",
   "metadata": {},
   "source": [
    "This is the third Exercise of **188.995 Data-Oriented Programming Paradigms**\n",
    "\n",
    "We are group 18, and consist of:\n",
    "* Guillermo Alamán Requena, Matr. Nr: 11937906\n",
    "* Michael Ferdinand Moser, Matr. Nr: 01123077 \n",
    "* Paul Joe Maliakel, Matr. Nr: 12012422\n",
    "* Gunnar Sjúrðarson Knudsen, Matr. Nr: 12028205\n",
    "\n",
    "In this task we were asked to choose one vaguely worded question, and then narrow the scope, figuring out how to get the data, before finally solving the question at hand.\n",
    "We chose **Question 21**, which contains:\n",
    "* How does the use of various communication languages in countries change over time?\n",
    "* Which languages grow and which disappear,  and what are their characteristics?\n",
    "* Are there other factors that correlate with the appearance or disappearance of languages?\n",
    "\n",
    "\n",
    "We soon realized that the question as stated is far too broad, and we therefore had to limit it.\n",
    "\n",
    "After having discussed amoung our groups, we came to the following plan:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-disposal",
   "metadata": {},
   "source": [
    "## First draft\n",
    "### Topic and Questions to answer\n",
    "We've selected question 21, which is regarding how communication languages in countries change over time.\n",
    "\n",
    "After having discussed the data available, and planned a workflow, we've decided to try to answer the questions:\n",
    "* How has the English language changed in the past 100 years based on word frequencies, sentence length, ...?\n",
    "* Can we find parallel developments between different genres of text?\n",
    "* Can the publication year of a movie/article/whatever be predicted based on the text and its characteristics?\n",
    "\n",
    "\n",
    "### Justification For Limit Of Scope\n",
    "The sample questions stated in the task description are too broad, to be answered in a single 160 hour project.\n",
    "* Lot's of issues, such as: \n",
    "* Lack of census data; \n",
    "* other changes such as phonetic, semantic and syntactic meanings; \n",
    "* High correlation with e.g:\n",
    "    * country population\n",
    "    * age of speakers\n",
    "    * ...\n",
    "* What counts as a language? \n",
    "    * dialect? \n",
    "    * Mutually Intelligible?\n",
    "* Political dimensions\n",
    "* Multilingual people\n",
    "* How do we check accuracy of the available data?\n",
    "* ...\n",
    "\n",
    "\n",
    "Historical data for language use is likely not available for most languages, as it's topics for great research to estimate merely historical populations - especially before 1850 or so. \n",
    "The evolution of languages are much less documented. \n",
    "Lack of census data overall, but other changes are even harder to gauge, such as phonetic, semantic, and syntactic meanings. Highly correlated with population of countries, but also with \"hidden\" correlations, such as age of speakers, ... \n",
    "Even dead languages can be revived. \n",
    "\n",
    "What constitutes a language? Dialect? Mutually Intelligible? Also do not forget the political dimension, e.g. Croatian/Serbian really are just dialects of the same language but they want to keep separate. On the other end of this scheme the variant of Chinese spoken in Beijing may be drastically different from the Chinese spoken in other regions of the country, but still falls under the same \"Chinese\" umbrella to communicate unity. \n",
    "\n",
    "How much is spoken? Should we consider people who studied a language as their second, third... language? If so, how well should be the command over the language for the person to count? A1/B1/C2 level?\n",
    "%How do we check accuracy of the available data?\n",
    "\n",
    "### Workflow plan & Project management\n",
    "* Outline the plan\n",
    "    * Get, understand and clean data: articles/movie scripts/video transcripts over the years (see next section)\n",
    "    * Train-test split: keeping proportion of publication years within the splits.\n",
    "    * Preprocessing: text feature extraction, feature selection, scaling, etc. (Come back here if necessary)\n",
    "    * Visualization: evolution of words over the years, word-clouds and other relevant characteristics.\n",
    "    * Define evaluation metrics, train different models/parameters using CV and select best one for predictions.\n",
    "    * Predict, conclude, report and publish notebook in Kaggle Kernel.\n",
    "* How the work will be divided up between group members\n",
    "    * Acquisition, cleaning and prepossessing of the data will be done commonly.\n",
    "    * Each member of the group will train a model and report results using same evaluation metrics. \n",
    "    * Jointly choose the best model and conclude.\n",
    "    * Presentation, report and publishing will be also split. \n",
    "* Timeline: To be defined after review meeting    \n",
    "\n",
    "\n",
    "### Data\n",
    "\n",
    "Our goal is to get a dataset similar to:\n",
    "\n",
    "\n",
    "\n",
    "| **Corpus** | **Year Published** | **Type**               | ... |\n",
    "|------------|--------------------|------------------------|-----|\n",
    "| Text1      | 1976               | News                   | ... |\n",
    "| Text2      | 1976               | Movie Script           | ... |\n",
    "| ...        | ...                | ...                    | ... |\n",
    "| TextN      | 2009               | Scientific Article     | ... |\n",
    "\n",
    "Feature extraction from texts will be performed to obtain appropriate features for modeling. To build a dataset like this one, we will rely on the following kind sources: \n",
    "\n",
    "* \\url{https://www.kaggle.com/asad1m9a9h6mood/news-articles} - News articles from 2015 until date.\n",
    "* \\url{https://www.kaggle.com/snapcrack/all-the-news} - 143000 articles from 15 American Publications. \n",
    "* NLTK\n",
    "* ...\n",
    "\n",
    "\n",
    "## Second Draft\n",
    "After having a preliminary meeting with Univ.Prof. Dr. Hanbury and Dipl.-Ing. Dr. Piroi, who gave great input, we decided to further limit out goal to only use Project Gutenberg as a datasource, and setting our hypothesis to see whether it was possible to generate a model that predicted the publication year/decade for a set of books.\n",
    "\n",
    "## Pivoting Point\n",
    "After having done a decent portion of work, we reached to the conclusion that our dataset was not suitable to solve the question we had original set out, and we were forced to pivot.\n",
    "\n",
    "We discussed whether we wanted to change the goal from classifying, but as we were all quite interrested in a classification algorithm, and wanted to do proper NLP, we instead searched for another dataset.\n",
    "\n",
    "## Language change in Icelandic Parliamentary Speeches\n",
    "We found the dataset with all icelandic parliamentary speeches going back a century. This is further described in section 3. \n",
    "With this great dataset, our goal was to develop a model that could try to predict which decade a speech is from"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-impact",
   "metadata": {},
   "source": [
    "# Estimating publication year from Project Gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-detection",
   "metadata": {},
   "source": [
    "This was the attempt at our first hypothesis. \n",
    "We import a large corpus of books from Project Gutenberg, and cleanse the data, so it's ready for machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-norman",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We start by setting up all packages needed for the project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-intranet",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "boolean-florida",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:07:34.042882Z",
     "start_time": "2021-01-26T11:07:33.423754Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from builtins import str\n",
    "import os\n",
    "from six import u\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from operator import itemgetter    \n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "import random\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import math\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-leeds",
   "metadata": {},
   "source": [
    "### Define Constants\n",
    "Constant that are used in this part is also set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "preliminary-things",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:07:34.049941Z",
     "start_time": "2021-01-26T11:07:34.043931Z"
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"processedData\"\n",
    "\n",
    "TEXT_START_MARKERS = frozenset((u(_) for _ in (\n",
    "    \"*END*THE SMALL PRINT\",\n",
    "    \"*** START OF THE PROJECT GUTENBERG\",\n",
    "    \"*** START OF THIS PROJECT GUTENBERG\",\n",
    "    \"This etext was prepared by\",\n",
    "    \"E-text prepared by\",\n",
    "    \"Produced by\",\n",
    "    \"Distributed Proofreading Team\",\n",
    "    \"Proofreading Team at http://www.pgdp.net\",\n",
    "    \"http://gallica.bnf.fr)\",\n",
    "    \"      http://archive.org/details/\",\n",
    "    \"http://www.pgdp.net\",\n",
    "    \"by The Internet Archive)\",\n",
    "    \"by The Internet Archive/Canadian Libraries\",\n",
    "    \"by The Internet Archive/American Libraries\",\n",
    "    \"public domain material from the Internet Archive\",\n",
    "    \"Internet Archive)\",\n",
    "    \"Internet Archive/Canadian Libraries\",\n",
    "    \"Internet Archive/American Libraries\",\n",
    "    \"material from the Google Print project\",\n",
    "    \"*END THE SMALL PRINT\",\n",
    "    \"***START OF THE PROJECT GUTENBERG\",\n",
    "    \"This etext was produced by\",\n",
    "    \"*** START OF THE COPYRIGHTED\",\n",
    "    \"The Project Gutenberg\",\n",
    "    \"http://gutenberg.spiegel.de/ erreichbar.\",\n",
    "    \"Project Runeberg publishes\",\n",
    "    \"Beginning of this Project Gutenberg\",\n",
    "    \"Project Gutenberg Online Distributed\",\n",
    "    \"Gutenberg Online Distributed\",\n",
    "    \"the Project Gutenberg Online Distributed\",\n",
    "    \"Project Gutenberg TEI\",\n",
    "    \"This eBook was prepared by\",\n",
    "    \"http://gutenberg2000.de erreichbar.\",\n",
    "    \"This Etext was prepared by\",\n",
    "    \"This Project Gutenberg Etext was prepared by\",\n",
    "    \"Gutenberg Distributed Proofreaders\",\n",
    "    \"Project Gutenberg Distributed Proofreaders\",\n",
    "    \"the Project Gutenberg Online Distributed Proofreading Team\",\n",
    "    \"**The Project Gutenberg\",\n",
    "    \"*SMALL PRINT!\",\n",
    "    \"More information about this book is at the top of this file.\",\n",
    "    \"tells you about restrictions in how the file may be used.\",\n",
    "    \"l'authorization à les utilizer pour preparer ce texte.\",\n",
    "    \"of the etext through OCR.\",\n",
    "    \"*****These eBooks Were Prepared By Thousands of Volunteers!*****\",\n",
    "    \"We need your donations more than ever!\",\n",
    "    \" *** START OF THIS PROJECT GUTENBERG\",\n",
    "    \"****     SMALL PRINT!\",\n",
    "    '[\"Small Print\" V.',\n",
    "    '      (http://www.ibiblio.org/gutenberg/',\n",
    "    'and the Project Gutenberg Online Distributed Proofreading Team',\n",
    "    'Mary Meehan, and the Project Gutenberg Online Distributed Proofreading',\n",
    "    '                this Project Gutenberg edition.',\n",
    ")))\n",
    "\n",
    "\n",
    "TEXT_END_MARKERS = frozenset((u(_) for _ in (\n",
    "    \"*** END OF THE PROJECT GUTENBERG\",\n",
    "    \"*** END OF THIS PROJECT GUTENBERG\",\n",
    "    \"***END OF THE PROJECT GUTENBERG\",\n",
    "    \"End of the Project Gutenberg\",\n",
    "    \"End of The Project Gutenberg\",\n",
    "    \"Ende dieses Project Gutenberg\",\n",
    "    \"by Project Gutenberg\",\n",
    "    \"End of Project Gutenberg\",\n",
    "    \"End of this Project Gutenberg\",\n",
    "    \"Ende dieses Projekt Gutenberg\",\n",
    "    \"        ***END OF THE PROJECT GUTENBERG\",\n",
    "    \"*** END OF THE COPYRIGHTED\",\n",
    "    \"End of this is COPYRIGHTED\",\n",
    "    \"Ende dieses Etextes \",\n",
    "    \"Ende dieses Project Gutenber\",\n",
    "    \"Ende diese Project Gutenberg\",\n",
    "    \"**This is a COPYRIGHTED Project Gutenberg Etext, Details Above**\",\n",
    "    \"Fin de Project Gutenberg\",\n",
    "    \"The Project Gutenberg Etext of \",\n",
    "    \"Ce document fut presente en lecture\",\n",
    "    \"Ce document fut présenté en lecture\",\n",
    "    \"More information about this book is at the top of this file.\",\n",
    "    \"We need your donations more than ever!\",\n",
    "    \"END OF PROJECT GUTENBERG\",\n",
    "    \" End of the Project Gutenberg\",\n",
    "    \" *** END OF THIS PROJECT GUTENBERG\",\n",
    ")))\n",
    "\n",
    "\n",
    "LEGALESE_START_MARKERS = frozenset((u(_) for _ in (\n",
    "    \"<<THIS ELECTRONIC VERSION OF\",\n",
    ")))\n",
    "\n",
    "\n",
    "LEGALESE_END_MARKERS = frozenset((u(_) for _ in (\n",
    "    \"SERVICE THAT CHARGES FOR DOWNLOAD\",\n",
    ")))\n",
    "\n",
    "TITLE_MARKERS = frozenset((u(_) for _ in (\n",
    "    \"Title:\",\n",
    ")))\n",
    "\n",
    "AUTHOR_MARKERS = frozenset((u(_) for _ in (\n",
    "    \"Author:\",\n",
    ")))\n",
    "DATE_MARKERS = frozenset((u(_) for _ in (\n",
    "    \"Release Date:\",\"Release Date:\"\n",
    ")))\n",
    "LANGUAGE_MARKERS = frozenset((u(_) for _ in (\n",
    "    \"Language:\",\n",
    ")))\n",
    "ENCODING_MARKERS = frozenset((u(_) for _ in (\n",
    "    \"Character set encoding:\",\n",
    ")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-memphis",
   "metadata": {},
   "source": [
    "## Importing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-selection",
   "metadata": {},
   "source": [
    "This is a very rough first draft at importing and cleansing the data. \n",
    "Solution is heavily inspired by https://gist.github.com/mbforbes/cee3fd5bb3a797b059524fe8c8ccdc2b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-cancellation",
   "metadata": {},
   "source": [
    "### Getting the content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-tennis",
   "metadata": {},
   "source": [
    "Start by downloading the repository of (english) books. This is done in bash. Only tested on Ubuntu, but mac should work the same\n",
    "\n",
    "```\n",
    "wget -m -H -nd \"http://www.gutenberg.org/robot/harvest?filetypes[]=txt&langs[]=en\"\n",
    "\n",
    "                http://www.gutenberg.org/robot/harvest?offset=40532&filetypes[]=txt&langs[]=en\n",
    "```\n",
    "Takes a few hours to run, and is stored in a folder called rawContent. \n",
    "This is then copied to another folder, and we can start to clean up the mess\n",
    "\n",
    "First we delete some dublications of the same books:\n",
    "```\n",
    "ls | grep \"\\-8.zip\" | xargs rm\n",
    "ls | grep \"\\-0.zip\" | xargs rm\n",
    "```\n",
    "We can then unzip the files, and remove the zip files\n",
    "```\n",
    "unzip \"*zip\"\n",
    "rm *.zip\n",
    "```\n",
    "\n",
    "Next we take care of some nested foldering\n",
    "```\n",
    "mv */*.txt ./\n",
    "```\n",
    "And finally, we remove all rubbish that isn't a real book:\n",
    "\n",
    "```\n",
    "ls | grep -v \"\\.txt\" | xargs rm -rf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-tucson",
   "metadata": {},
   "source": [
    "### Data Cleansing\n",
    "As the data is not given in a computer-friendly format, a lot of string operations are needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-enterprise",
   "metadata": {},
   "source": [
    "#### Read a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "realistic-cricket",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:07:34.065534Z",
     "start_time": "2021-01-26T11:07:34.051385Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "    file = open(file_name, encoding=\"ISO-8859-1\")\n",
    "    file_content = file.read()\n",
    "\n",
    "    lines = file_content.splitlines()\n",
    "    sep = str(os.linesep)\n",
    "\n",
    "    # Initialize results for single book\n",
    "    content_lines = []\n",
    "    i = 0\n",
    "    footer_found = False\n",
    "    ignore_section = False\n",
    "\n",
    "    title = \"\"\n",
    "    author = \"\"\n",
    "    date = \"\"\n",
    "    language = \"\"\n",
    "    encoding = \"\"\n",
    "    year = 0\n",
    "\n",
    "    # Reset flags for each book\n",
    "    title_found = False\n",
    "    author_found = False\n",
    "    date_found = False\n",
    "    language_found = False\n",
    "    encoding_found = False\n",
    "\n",
    "    for line in lines:\n",
    "            reset = False\n",
    "\n",
    "            #print(line)\n",
    "            if i <= 600:\n",
    "                # Shamelessly stolen\n",
    "                if any(line.startswith(token) for token in TEXT_START_MARKERS):\n",
    "                    reset = True\n",
    "\n",
    "                # Extract Metadata\n",
    "                if title_found == False:\n",
    "                    if any(line.startswith(token) for token in TITLE_MARKERS):\n",
    "                        title_found = True\n",
    "                        title = line\n",
    "                if author_found == False:\n",
    "                    if any(line.startswith(token) for token in AUTHOR_MARKERS):\n",
    "                        author_found = True\n",
    "                        author = line\n",
    "                if date_found == False:\n",
    "                    if any(line.startswith(token) for token in DATE_MARKERS):\n",
    "                        date_found = True\n",
    "                        date = line\n",
    "                        year = int(re.findall(r'\\d{4}', date)[0])\n",
    "                if language_found == False:\n",
    "                    if any(line.startswith(token) for token in LANGUAGE_MARKERS):\n",
    "                        language_found = True\n",
    "                        language = line\n",
    "                if encoding_found == False:\n",
    "                    if any(line.startswith(token) for token in ENCODING_MARKERS):\n",
    "                        encoding_found = True\n",
    "                        encoding = line\n",
    "\n",
    "                # More theft from above\n",
    "                if reset:\n",
    "                    content_lines = []\n",
    "                    continue\n",
    "\n",
    "            # I feel like a criminal by now. Guess what? Also stolen\n",
    "            if i >= 100:\n",
    "                if any(line.startswith(token) for token in TEXT_END_MARKERS):\n",
    "                    footer_found = True\n",
    "\n",
    "                if footer_found:\n",
    "                    break\n",
    "\n",
    "            if any(line.startswith(token) for token in LEGALESE_START_MARKERS):\n",
    "                ignore_section = True\n",
    "                continue\n",
    "            elif any(line.startswith(token) for token in LEGALESE_END_MARKERS):\n",
    "                ignore_section = False\n",
    "                continue\n",
    "\n",
    "            if not ignore_section:\n",
    "                if line != \"\": # Screw the blank lines\n",
    "                    content_lines.append(line.rstrip(sep))\n",
    "                i += 1\n",
    "\n",
    "            sep.join(content_lines)\n",
    "\n",
    "    # Do more cleaning\n",
    "    for token in TITLE_MARKERS:\n",
    "        title = title.replace(token, '').lstrip().rstrip()\n",
    "    for token in AUTHOR_MARKERS:\n",
    "        author = author.replace(token, '').lstrip().rstrip()\n",
    "    for token in LANGUAGE_MARKERS:\n",
    "        language = language.replace(token, '').lstrip().rstrip()\n",
    "    for token in DATE_MARKERS:\n",
    "        date = date.replace(token, '').lstrip().rstrip()\n",
    "    for token in ENCODING_MARKERS:\n",
    "        encoding = encoding.replace(token, '').lstrip().rstrip()\n",
    "    return title, author, date, year, language, encoding, content_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-coalition",
   "metadata": {},
   "source": [
    "#### Return list of all words\n",
    "Currently quite an empty function. However, I assume that some cleaning of cases etc. will be done here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "square-pontiac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:07:34.073826Z",
     "start_time": "2021-01-26T11:07:34.068321Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_words(content_lines):\n",
    "    all_text_lower = \" \".join(content_lines).lower()\n",
    "    words = re.findall(r'(\\b[A-Za-z][a-z]{2,9}\\b)', all_text_lower)\n",
    "\n",
    "    # Do more cleansing. E.g. cases and stuff\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-hazard",
   "metadata": {},
   "source": [
    "## Statistics\n",
    "We start by doing some exploratory data analysis, to see how well our scraping works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-mexican",
   "metadata": {},
   "source": [
    "### First attempt\n",
    "Trying a simple word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "color-edition",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:07:34.081923Z",
     "start_time": "2021-01-26T11:07:34.076027Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_word_frequencies(words):\n",
    "    frequency = {}\n",
    "    for word in words:\n",
    "        count = frequency.get(word,0)\n",
    "        frequency[word] = count + 1\n",
    "\n",
    "    word_count = len(words)\n",
    "    unique_word_count = 0\n",
    "    word_list = []\n",
    "    word_list_count = []\n",
    "    for key, value in reversed(sorted(frequency.items(), key = itemgetter(1))):\n",
    "        word_list.append(key)\n",
    "        word_list_count.append(value)\n",
    "        unique_word_count = unique_word_count + 1\n",
    "    \n",
    "    word_list_freq = [freq / word_count for freq in word_list_count]\n",
    "    \n",
    "    word_freq = pd.DataFrame(list(zip(word_list, word_list_count, word_list_freq))\n",
    "                             , columns = ['Word', 'count', 'freq'])\n",
    "    \n",
    "    word_freq['rank'] = word_freq['count'].rank(ascending = False, method=\"dense\")\n",
    "\n",
    "    return(word_freq, unique_word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-objective",
   "metadata": {},
   "source": [
    "### Read all files, and do preprocessing\n",
    "Well... Only ten files currently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "married-closing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:07:36.439398Z",
     "start_time": "2021-01-26T11:07:34.083323Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get all filenames\n",
    "files = [f for f in listdir(file_path) if isfile(join(file_path, f))]\n",
    "files = list(filter(lambda file: file[0].isdigit(), files))\n",
    "random.shuffle(files)\n",
    "\n",
    "\n",
    "# Do only subset\n",
    "files = files[0:10]\n",
    "\n",
    "list_of_file = []\n",
    "list_of_title = []\n",
    "list_of_author = []\n",
    "list_of_date = []\n",
    "list_of_year = []\n",
    "list_of_language = []\n",
    "list_of_encoding = []\n",
    "list_of_word_count = []\n",
    "list_of_unique_word_count = []\n",
    "list_of_word_frequencies = []\n",
    "iter_ = 0\n",
    "\n",
    "for file in files:\n",
    "    # Read in basic information from file\n",
    "    title, author, date, year, language, encoding, content_lines = read_file(file_path + \"/\" + file)\n",
    "    line_count = len(content_lines)\n",
    "\n",
    "    # Not sure if we want this for later:\n",
    "    #content_all = \" \".join(content_lines)\n",
    "    \n",
    "    # Split into words (and do various cleaning)\n",
    "    words = get_words(content_lines)\n",
    "    word_count = len(words)\n",
    "\n",
    "    # First analysis, but should do something proper\n",
    "    word_frequencies_table, unique_word_count = get_word_frequencies(words)\n",
    "    \n",
    "    # Append to results\n",
    "    list_of_file.append(file)\n",
    "    list_of_title.append(title)\n",
    "    list_of_author.append(author)\n",
    "    list_of_date.append(date)\n",
    "    list_of_year.append(year)\n",
    "    list_of_language.append(language)\n",
    "    list_of_encoding.append(encoding)\n",
    "    list_of_word_count.append(word_count)\n",
    "    list_of_unique_word_count.append(unique_word_count)\n",
    "    list_of_word_frequencies.append(word_frequencies_table)\n",
    "    \n",
    "    \n",
    "    # Show basic information\n",
    "    #print(iter_)\n",
    "    iter_ = iter_ + 1\n",
    "    #print(\"################################\")\n",
    "    #print(\"################################\")\n",
    "    #print(\"Filename: \" + str(file))\n",
    "    #print(\"Title: \" + str(title))\n",
    "    #print(\"Author(s): \" + str(author))\n",
    "    #print(\"Date: \" + str(date))\n",
    "    #print(\"Year: \" + str(year))\n",
    "    #print(\"Language: \" + str(language))\n",
    "    #print(\"Encoding: \" + str(encoding))\n",
    "    #print(\"################################\")\n",
    "    #print(\"Words in book: \" + str(word_count))\n",
    "    #print(\"Unique words in book: \" + str(unique_word_count))\n",
    "    #print(\"################################\")\n",
    "    #print(word_frequencies_table)\n",
    "\n",
    "# Feel free to change to dict? list? separate files?\n",
    "## nested dataframes works, but looks super ungly when printing\n",
    "### Fuck it - This is tooo useless killing it again\n",
    "#all_res = pd.DataFrame(list(zip(list_of_file\n",
    "#                                , list_of_title\n",
    "#                                , list_of_author\n",
    "#                                , list_of_date\n",
    "#                                , list_of_language\n",
    "#                                , list_of_encoding\n",
    "#                                , list_of_word_count\n",
    "#                                , list_of_unique_word_count\n",
    "#                                , list_of_word_frequencies\n",
    "#                                ))\n",
    "#                             , columns = ['file'\n",
    "#                                          , 'title'\n",
    "#                                          , 'author'\n",
    "#                                          , 'date'\n",
    "#                                          , 'language'\n",
    "#                                          , 'encoding'\n",
    "#                                          , 'word_count'\n",
    "#                                          , 'unique_word_count'\n",
    "#                                          , 'word_frequencies'\n",
    "#                                         ]\n",
    "#                      )\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-gilbert",
   "metadata": {},
   "source": [
    "### Compare Word ranking between titles\n",
    "This is our first attemt at seeing how the ranking of words change between titles. Idea is to see that the zipf-distribution changes as time passes buy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "nominated-christian",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:07:38.537423Z",
     "start_time": "2021-01-26T11:07:36.440713Z"
    }
   },
   "outputs": [],
   "source": [
    "list_count= []\n",
    "list_freq = []\n",
    "list_rank = []\n",
    "\n",
    "\n",
    "col_names = list_of_title.copy()\n",
    "col_names.insert(0,'Word')\n",
    "\n",
    "for df in list_of_word_frequencies:\n",
    "    list_count.append(df[['Word', 'count']])\n",
    "    list_freq.append(df[['Word', 'freq']])\n",
    "    list_rank.append(df[['Word', 'rank']])\n",
    "    \n",
    "df_count = reduce(lambda left, right: pd.merge(left, right, on=\"Word\", how='outer'), list_count)\n",
    "df_count.columns = col_names\n",
    "df_count['Sum'] = df_count.drop('Word', axis=1).apply(lambda x: x.sum(), axis=1)\n",
    "df_count = df_count.sort_values(ascending = False, by=['Sum'])\n",
    "\n",
    "df_freq = reduce(lambda left, right: pd.merge(left, right, on=\"Word\", how='outer'), list_freq)\n",
    "df_freq.columns = col_names\n",
    "df_freq['Avg'] = df_freq.drop('Word', axis=1).apply(lambda x: x.mean(), axis=1)\n",
    "df_freq = df_freq.sort_values(ascending = False, by=['Avg'])\n",
    "\n",
    "df_rank = reduce(lambda left, right: pd.merge(left, right, on=\"Word\", how='outer'), list_rank)\n",
    "df_rank.columns = col_names\n",
    "df_rank['Avg'] = df_rank.drop('Word', axis=1).apply(lambda x: x.mean(), axis=1)\n",
    "df_rank = df_rank.sort_values(by=['Avg'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "interesting-jenny",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:07:38.568737Z",
     "start_time": "2021-01-26T11:07:38.538449Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Proceedings of a Board of General Officers</th>\n",
       "      <th>Arachne, Volume 6.</th>\n",
       "      <th>Poor Jack</th>\n",
       "      <th>The Pirate Shark</th>\n",
       "      <th>Pardners</th>\n",
       "      <th>The Tale of Chloe</th>\n",
       "      <th>The Real Robert Burns</th>\n",
       "      <th>Vignettes of Manhattan; Outlines in Local Color</th>\n",
       "      <th>The Children of the Poor</th>\n",
       "      <th>A. V. Laider</th>\n",
       "      <th>Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>was</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>excellency</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8319</th>\n",
       "      <td>mart</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>his</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>for</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>andre</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>had</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12403</th>\n",
       "      <td>chloe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>with</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>10.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1071</th>\n",
       "      <td>hermon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>but</td>\n",
       "      <td>28.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>14.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>him</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12404</th>\n",
       "      <td>beamish</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>not</td>\n",
       "      <td>18.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18773</th>\n",
       "      <td>laider</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>have</td>\n",
       "      <td>10.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>20.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>they</td>\n",
       "      <td>37.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>arnold</td>\n",
       "      <td>21.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>from</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>21.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>you</td>\n",
       "      <td>13.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>this</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>her</td>\n",
       "      <td>44.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>24.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>all</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>24.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>clinton</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>were</td>\n",
       "      <td>32.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>25.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12406</th>\n",
       "      <td>caseldy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1072</th>\n",
       "      <td>demeter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Word  Proceedings of a Board of General Officers  \\\n",
       "0             the                                         1.0   \n",
       "1             and                                         2.0   \n",
       "2            that                                         3.0   \n",
       "4             was                                         5.0   \n",
       "6      excellency                                         6.0   \n",
       "8319         mart                                         NaN   \n",
       "8             his                                         8.0   \n",
       "16            for                                        14.0   \n",
       "9           andre                                         9.0   \n",
       "18            had                                        16.0   \n",
       "12403       chloe                                         NaN   \n",
       "14           with                                        12.0   \n",
       "1071       hermon                                         NaN   \n",
       "37            but                                        28.0   \n",
       "13            him                                        12.0   \n",
       "12404     beamish                                         NaN   \n",
       "20            not                                        18.0   \n",
       "18773      laider                                         NaN   \n",
       "10           have                                        10.0   \n",
       "72           they                                        37.0   \n",
       "25         arnold                                        21.0   \n",
       "12           from                                        11.0   \n",
       "15            you                                        13.0   \n",
       "21           this                                        18.0   \n",
       "261           her                                        44.0   \n",
       "61            all                                        35.0   \n",
       "30        clinton                                        25.0   \n",
       "47           were                                        32.0   \n",
       "12406     caseldy                                         NaN   \n",
       "1072      demeter                                         NaN   \n",
       "\n",
       "       Arachne, Volume 6.  Poor Jack  The Pirate Shark  Pardners  \\\n",
       "0                     1.0        1.0               1.0       1.0   \n",
       "1                     2.0        2.0               2.0       2.0   \n",
       "2                    10.0        5.0               3.0       5.0   \n",
       "4                     6.0        3.0               5.0       6.0   \n",
       "6                     NaN        NaN               NaN       NaN   \n",
       "8319                  NaN        NaN               6.0       NaN   \n",
       "8                     3.0       13.0               4.0       3.0   \n",
       "16                    9.0        6.0              11.0       9.0   \n",
       "9                     NaN        NaN               NaN       NaN   \n",
       "18                    4.0        7.0               9.0      13.0   \n",
       "12403                 NaN        NaN               NaN       NaN   \n",
       "14                    7.0       12.0              10.0       7.0   \n",
       "1071                 14.0        NaN               NaN       NaN   \n",
       "37                   15.0       11.0              14.0      12.0   \n",
       "13                    5.0       18.0              15.0       8.0   \n",
       "12404                 NaN        NaN               NaN       NaN   \n",
       "20                   13.0        9.0              23.0      49.0   \n",
       "18773                 NaN        NaN               NaN       NaN   \n",
       "10                   22.0       14.0              42.0      31.0   \n",
       "72                   42.0       20.0              13.0      10.0   \n",
       "25                    NaN        NaN               NaN       NaN   \n",
       "12                   12.0       38.0              32.0      15.0   \n",
       "15                   19.0        4.0               7.0       4.0   \n",
       "21                   16.0       33.0              26.0      24.0   \n",
       "261                   8.0        8.0              78.0      20.0   \n",
       "61                   35.0       16.0              19.0      21.0   \n",
       "30                    NaN        NaN               NaN       NaN   \n",
       "47                   21.0       15.0              24.0      44.0   \n",
       "12406                 NaN        NaN               NaN       NaN   \n",
       "1072                 26.0        NaN               NaN       NaN   \n",
       "\n",
       "       The Tale of Chloe  The Real Robert Burns  \\\n",
       "0                    1.0                    1.0   \n",
       "1                    2.0                    2.0   \n",
       "2                    6.0                    4.0   \n",
       "4                    7.0                    5.0   \n",
       "6                    NaN                    NaN   \n",
       "8319                 NaN                    NaN   \n",
       "8                    9.0                    3.0   \n",
       "16                   8.0                    7.0   \n",
       "9                    NaN                    NaN   \n",
       "18                  14.0                   12.0   \n",
       "12403               10.0                    NaN   \n",
       "14                  12.0                    8.0   \n",
       "1071                 NaN                    NaN   \n",
       "37                  18.0                   10.0   \n",
       "13                  16.0                   15.0   \n",
       "12404               15.0                    NaN   \n",
       "20                  13.0                    9.0   \n",
       "18773                NaN                    NaN   \n",
       "10                  11.0                   22.0   \n",
       "72                  19.0                   21.0   \n",
       "25                   NaN                    NaN   \n",
       "12                  22.0                   17.0   \n",
       "15                   5.0                   35.0   \n",
       "21                  31.0                   17.0   \n",
       "261                  3.0                   14.0   \n",
       "61                  33.0                   27.0   \n",
       "30                   NaN                    NaN   \n",
       "47                  30.0                   32.0   \n",
       "12406               26.0                    NaN   \n",
       "1072                 NaN                    NaN   \n",
       "\n",
       "       Vignettes of Manhattan; Outlines in Local Color  \\\n",
       "0                                                  1.0   \n",
       "1                                                  2.0   \n",
       "2                                                  4.0   \n",
       "4                                                  3.0   \n",
       "6                                                  NaN   \n",
       "8319                                               NaN   \n",
       "8                                                  7.0   \n",
       "16                                                11.0   \n",
       "9                                                  NaN   \n",
       "18                                                 6.0   \n",
       "12403                                              NaN   \n",
       "14                                                10.0   \n",
       "1071                                               NaN   \n",
       "37                                                12.0   \n",
       "13                                                14.0   \n",
       "12404                                              NaN   \n",
       "20                                                15.0   \n",
       "18773                                              NaN   \n",
       "10                                                22.0   \n",
       "72                                                16.0   \n",
       "25                                                 NaN   \n",
       "12                                                26.0   \n",
       "15                                                 9.0   \n",
       "21                                                21.0   \n",
       "261                                                8.0   \n",
       "61                                                24.0   \n",
       "30                                                 NaN   \n",
       "47                                                17.0   \n",
       "12406                                              NaN   \n",
       "1072                                               NaN   \n",
       "\n",
       "       The Children of the Poor  A. V. Laider   Avg  \n",
       "0                           1.0           1.0   1.0  \n",
       "1                           2.0           3.0   2.1  \n",
       "2                           3.0           4.0   4.7  \n",
       "4                           5.0           2.0   4.7  \n",
       "6                           NaN           NaN   6.0  \n",
       "8319                        NaN           NaN   6.0  \n",
       "8                           8.0          11.0   6.9  \n",
       "16                          4.0           8.0   8.7  \n",
       "9                           NaN           NaN   9.0  \n",
       "18                         12.0           5.0   9.8  \n",
       "12403                       NaN           NaN  10.0  \n",
       "14                          6.0          19.0  10.3  \n",
       "1071                        NaN           NaN  14.0  \n",
       "37                         19.0           9.0  14.8  \n",
       "13                         30.0          16.0  14.9  \n",
       "12404                       NaN           NaN  15.0  \n",
       "20                          9.0           7.0  16.5  \n",
       "18773                       NaN          20.0  20.0  \n",
       "10                         16.0          13.0  20.3  \n",
       "72                          7.0          20.0  20.5  \n",
       "25                          NaN           NaN  21.0  \n",
       "12                         18.0          22.0  21.3  \n",
       "15                        114.0           6.0  21.6  \n",
       "21                         24.0          10.0  22.0  \n",
       "261                        36.0          22.0  24.1  \n",
       "61                         23.0          13.0  24.6  \n",
       "30                          NaN           NaN  25.0  \n",
       "47                         21.0          19.0  25.5  \n",
       "12406                       NaN           NaN  26.0  \n",
       "1072                        NaN           NaN  26.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rank.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "disciplinary-bridal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:07:38.585462Z",
     "start_time": "2021-01-26T11:07:38.569946Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Proceedings of a Board of General Officers</th>\n",
       "      <th>Arachne, Volume 6.</th>\n",
       "      <th>Poor Jack</th>\n",
       "      <th>The Pirate Shark</th>\n",
       "      <th>Pardners</th>\n",
       "      <th>The Tale of Chloe</th>\n",
       "      <th>The Real Robert Burns</th>\n",
       "      <th>Vignettes of Manhattan; Outlines in Local Color</th>\n",
       "      <th>The Children of the Poor</th>\n",
       "      <th>A. V. Laider</th>\n",
       "      <th>Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>0.074106</td>\n",
       "      <td>0.103625</td>\n",
       "      <td>0.061332</td>\n",
       "      <td>0.078727</td>\n",
       "      <td>0.082484</td>\n",
       "      <td>0.071470</td>\n",
       "      <td>0.078031</td>\n",
       "      <td>0.090815</td>\n",
       "      <td>0.106464</td>\n",
       "      <td>0.052684</td>\n",
       "      <td>0.079974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "      <td>0.039182</td>\n",
       "      <td>0.039401</td>\n",
       "      <td>0.048454</td>\n",
       "      <td>0.039779</td>\n",
       "      <td>0.045656</td>\n",
       "      <td>0.040633</td>\n",
       "      <td>0.044380</td>\n",
       "      <td>0.042009</td>\n",
       "      <td>0.036348</td>\n",
       "      <td>0.034345</td>\n",
       "      <td>0.041019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that</td>\n",
       "      <td>0.024702</td>\n",
       "      <td>0.009161</td>\n",
       "      <td>0.019271</td>\n",
       "      <td>0.019171</td>\n",
       "      <td>0.012454</td>\n",
       "      <td>0.017099</td>\n",
       "      <td>0.017427</td>\n",
       "      <td>0.016462</td>\n",
       "      <td>0.019404</td>\n",
       "      <td>0.028343</td>\n",
       "      <td>0.018349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>was</td>\n",
       "      <td>0.015332</td>\n",
       "      <td>0.013298</td>\n",
       "      <td>0.021669</td>\n",
       "      <td>0.015273</td>\n",
       "      <td>0.012365</td>\n",
       "      <td>0.014143</td>\n",
       "      <td>0.013395</td>\n",
       "      <td>0.020291</td>\n",
       "      <td>0.012086</td>\n",
       "      <td>0.034678</td>\n",
       "      <td>0.017253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>excellency</td>\n",
       "      <td>0.015119</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.015119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>his</td>\n",
       "      <td>0.013629</td>\n",
       "      <td>0.028270</td>\n",
       "      <td>0.008873</td>\n",
       "      <td>0.018532</td>\n",
       "      <td>0.016140</td>\n",
       "      <td>0.011071</td>\n",
       "      <td>0.025328</td>\n",
       "      <td>0.011742</td>\n",
       "      <td>0.009113</td>\n",
       "      <td>0.008169</td>\n",
       "      <td>0.015087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8319</th>\n",
       "      <td>mart</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014410</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>andre</td>\n",
       "      <td>0.012990</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.012990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>had</td>\n",
       "      <td>0.007666</td>\n",
       "      <td>0.017533</td>\n",
       "      <td>0.012070</td>\n",
       "      <td>0.010288</td>\n",
       "      <td>0.006866</td>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.005625</td>\n",
       "      <td>0.013721</td>\n",
       "      <td>0.007258</td>\n",
       "      <td>0.024842</td>\n",
       "      <td>0.011311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>you</td>\n",
       "      <td>0.008731</td>\n",
       "      <td>0.005221</td>\n",
       "      <td>0.019878</td>\n",
       "      <td>0.013068</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>0.017679</td>\n",
       "      <td>0.002796</td>\n",
       "      <td>0.011361</td>\n",
       "      <td>0.000754</td>\n",
       "      <td>0.018673</td>\n",
       "      <td>0.011106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>for</td>\n",
       "      <td>0.008518</td>\n",
       "      <td>0.010146</td>\n",
       "      <td>0.012649</td>\n",
       "      <td>0.009905</td>\n",
       "      <td>0.008917</td>\n",
       "      <td>0.013853</td>\n",
       "      <td>0.010404</td>\n",
       "      <td>0.010018</td>\n",
       "      <td>0.012433</td>\n",
       "      <td>0.010837</td>\n",
       "      <td>0.010768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12403</th>\n",
       "      <td>chloe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009854</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>with</td>\n",
       "      <td>0.008944</td>\n",
       "      <td>0.012116</td>\n",
       "      <td>0.009379</td>\n",
       "      <td>0.010128</td>\n",
       "      <td>0.009749</td>\n",
       "      <td>0.009622</td>\n",
       "      <td>0.009006</td>\n",
       "      <td>0.010273</td>\n",
       "      <td>0.010758</td>\n",
       "      <td>0.004502</td>\n",
       "      <td>0.009448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>her</td>\n",
       "      <td>0.000639</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.011794</td>\n",
       "      <td>0.001214</td>\n",
       "      <td>0.004577</td>\n",
       "      <td>0.026432</td>\n",
       "      <td>0.005430</td>\n",
       "      <td>0.011460</td>\n",
       "      <td>0.003244</td>\n",
       "      <td>0.003835</td>\n",
       "      <td>0.007926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>him</td>\n",
       "      <td>0.008944</td>\n",
       "      <td>0.015366</td>\n",
       "      <td>0.005750</td>\n",
       "      <td>0.007860</td>\n",
       "      <td>0.009006</td>\n",
       "      <td>0.007130</td>\n",
       "      <td>0.005137</td>\n",
       "      <td>0.006811</td>\n",
       "      <td>0.003938</td>\n",
       "      <td>0.005502</td>\n",
       "      <td>0.007544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>not</td>\n",
       "      <td>0.007027</td>\n",
       "      <td>0.006994</td>\n",
       "      <td>0.010802</td>\n",
       "      <td>0.004984</td>\n",
       "      <td>0.001724</td>\n",
       "      <td>0.009332</td>\n",
       "      <td>0.008648</td>\n",
       "      <td>0.006203</td>\n",
       "      <td>0.008178</td>\n",
       "      <td>0.011337</td>\n",
       "      <td>0.007523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>but</td>\n",
       "      <td>0.004046</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>0.010150</td>\n",
       "      <td>0.007892</td>\n",
       "      <td>0.007253</td>\n",
       "      <td>0.006782</td>\n",
       "      <td>0.007998</td>\n",
       "      <td>0.007093</td>\n",
       "      <td>0.005703</td>\n",
       "      <td>0.010337</td>\n",
       "      <td>0.007385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12404</th>\n",
       "      <td>beamish</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007188</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>she</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>0.004137</td>\n",
       "      <td>0.010609</td>\n",
       "      <td>0.001278</td>\n",
       "      <td>0.005469</td>\n",
       "      <td>0.022838</td>\n",
       "      <td>0.002731</td>\n",
       "      <td>0.015049</td>\n",
       "      <td>0.002927</td>\n",
       "      <td>0.003334</td>\n",
       "      <td>0.006944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1071</th>\n",
       "      <td>hermon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006698</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Word  Proceedings of a Board of General Officers  \\\n",
       "0             the                                    0.074106   \n",
       "1             and                                    0.039182   \n",
       "2            that                                    0.024702   \n",
       "4             was                                    0.015332   \n",
       "6      excellency                                    0.015119   \n",
       "8             his                                    0.013629   \n",
       "8319         mart                                         NaN   \n",
       "9           andre                                    0.012990   \n",
       "18            had                                    0.007666   \n",
       "15            you                                    0.008731   \n",
       "16            for                                    0.008518   \n",
       "12403       chloe                                         NaN   \n",
       "14           with                                    0.008944   \n",
       "261           her                                    0.000639   \n",
       "13            him                                    0.008944   \n",
       "20            not                                    0.007027   \n",
       "37            but                                    0.004046   \n",
       "12404     beamish                                         NaN   \n",
       "150           she                                    0.001065   \n",
       "1071       hermon                                         NaN   \n",
       "\n",
       "       Arachne, Volume 6.  Poor Jack  The Pirate Shark  Pardners  \\\n",
       "0                0.103625   0.061332          0.078727  0.082484   \n",
       "1                0.039401   0.048454          0.039779  0.045656   \n",
       "2                0.009161   0.019271          0.019171  0.012454   \n",
       "4                0.013298   0.021669          0.015273  0.012365   \n",
       "6                     NaN        NaN               NaN       NaN   \n",
       "8                0.028270   0.008873          0.018532  0.016140   \n",
       "8319                  NaN        NaN          0.014410       NaN   \n",
       "9                     NaN        NaN               NaN       NaN   \n",
       "18               0.017533   0.012070          0.010288  0.006866   \n",
       "15               0.005221   0.019878          0.013068  0.012900   \n",
       "16               0.010146   0.012649          0.009905  0.008917   \n",
       "12403                 NaN        NaN               NaN       NaN   \n",
       "14               0.012116   0.009379          0.010128  0.009749   \n",
       "261              0.010638   0.011794          0.001214  0.004577   \n",
       "13               0.015366   0.005750          0.007860  0.009006   \n",
       "20               0.006994   0.010802          0.004984  0.001724   \n",
       "37               0.006600   0.010150          0.007892  0.007253   \n",
       "12404                 NaN        NaN               NaN       NaN   \n",
       "150              0.004137   0.010609          0.001278  0.005469   \n",
       "1071             0.006698        NaN               NaN       NaN   \n",
       "\n",
       "       The Tale of Chloe  The Real Robert Burns  \\\n",
       "0               0.071470               0.078031   \n",
       "1               0.040633               0.044380   \n",
       "2               0.017099               0.017427   \n",
       "4               0.014143               0.013395   \n",
       "6                    NaN                    NaN   \n",
       "8               0.011071               0.025328   \n",
       "8319                 NaN                    NaN   \n",
       "9                    NaN                    NaN   \n",
       "18              0.007246               0.005625   \n",
       "15              0.017679               0.002796   \n",
       "16              0.013853               0.010404   \n",
       "12403           0.009854                    NaN   \n",
       "14              0.009622               0.009006   \n",
       "261             0.026432               0.005430   \n",
       "13              0.007130               0.005137   \n",
       "20              0.009332               0.008648   \n",
       "37              0.006782               0.007998   \n",
       "12404           0.007188                    NaN   \n",
       "150             0.022838               0.002731   \n",
       "1071                 NaN                    NaN   \n",
       "\n",
       "       Vignettes of Manhattan; Outlines in Local Color  \\\n",
       "0                                             0.090815   \n",
       "1                                             0.042009   \n",
       "2                                             0.016462   \n",
       "4                                             0.020291   \n",
       "6                                                  NaN   \n",
       "8                                             0.011742   \n",
       "8319                                               NaN   \n",
       "9                                                  NaN   \n",
       "18                                            0.013721   \n",
       "15                                            0.011361   \n",
       "16                                            0.010018   \n",
       "12403                                              NaN   \n",
       "14                                            0.010273   \n",
       "261                                           0.011460   \n",
       "13                                            0.006811   \n",
       "20                                            0.006203   \n",
       "37                                            0.007093   \n",
       "12404                                              NaN   \n",
       "150                                           0.015049   \n",
       "1071                                               NaN   \n",
       "\n",
       "       The Children of the Poor  A. V. Laider       Avg  \n",
       "0                      0.106464      0.052684  0.079974  \n",
       "1                      0.036348      0.034345  0.041019  \n",
       "2                      0.019404      0.028343  0.018349  \n",
       "4                      0.012086      0.034678  0.017253  \n",
       "6                           NaN           NaN  0.015119  \n",
       "8                      0.009113      0.008169  0.015087  \n",
       "8319                        NaN           NaN  0.014410  \n",
       "9                           NaN           NaN  0.012990  \n",
       "18                     0.007258      0.024842  0.011311  \n",
       "15                     0.000754      0.018673  0.011106  \n",
       "16                     0.012433      0.010837  0.010768  \n",
       "12403                       NaN           NaN  0.009854  \n",
       "14                     0.010758      0.004502  0.009448  \n",
       "261                    0.003244      0.003835  0.007926  \n",
       "13                     0.003938      0.005502  0.007544  \n",
       "20                     0.008178      0.011337  0.007523  \n",
       "37                     0.005703      0.010337  0.007385  \n",
       "12404                       NaN           NaN  0.007188  \n",
       "150                    0.002927      0.003334  0.006944  \n",
       "1071                        NaN           NaN  0.006698  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_freq['Avg'] = df_freq.drop('Word', axis=1).apply(lambda x: x.mean(), axis=1)\n",
    "df_freq = df_freq.sort_values(ascending = False, by=['Avg'])\n",
    "\n",
    "df_freq.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "innocent-decline",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:07:38.606216Z",
     "start_time": "2021-01-26T11:07:38.586296Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Proceedings of a Board of General Officers</th>\n",
       "      <th>Arachne, Volume 6.</th>\n",
       "      <th>Poor Jack</th>\n",
       "      <th>The Pirate Shark</th>\n",
       "      <th>Pardners</th>\n",
       "      <th>The Tale of Chloe</th>\n",
       "      <th>The Real Robert Burns</th>\n",
       "      <th>Vignettes of Manhattan; Outlines in Local Color</th>\n",
       "      <th>The Children of the Poor</th>\n",
       "      <th>A. V. Laider</th>\n",
       "      <th>Sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>348.0</td>\n",
       "      <td>1052.0</td>\n",
       "      <td>6677.0</td>\n",
       "      <td>2464.0</td>\n",
       "      <td>2775.0</td>\n",
       "      <td>1233.0</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>6427.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>316.0</td>\n",
       "      <td>30748.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "      <td>184.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>5275.0</td>\n",
       "      <td>1245.0</td>\n",
       "      <td>1536.0</td>\n",
       "      <td>701.0</td>\n",
       "      <td>1365.0</td>\n",
       "      <td>2973.0</td>\n",
       "      <td>2409.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>16294.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that</td>\n",
       "      <td>116.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>2098.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>419.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>536.0</td>\n",
       "      <td>1165.0</td>\n",
       "      <td>1286.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>6778.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>was</td>\n",
       "      <td>72.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>2359.0</td>\n",
       "      <td>478.0</td>\n",
       "      <td>416.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>412.0</td>\n",
       "      <td>1436.0</td>\n",
       "      <td>801.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>6561.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>his</td>\n",
       "      <td>64.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>966.0</td>\n",
       "      <td>580.0</td>\n",
       "      <td>543.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>779.0</td>\n",
       "      <td>831.0</td>\n",
       "      <td>604.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>4894.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>you</td>\n",
       "      <td>41.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2164.0</td>\n",
       "      <td>409.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>804.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>4458.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>for</td>\n",
       "      <td>40.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>1377.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>239.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>709.0</td>\n",
       "      <td>824.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>4287.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>had</td>\n",
       "      <td>36.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>1314.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>971.0</td>\n",
       "      <td>481.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>3980.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>with</td>\n",
       "      <td>42.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>1021.0</td>\n",
       "      <td>317.0</td>\n",
       "      <td>328.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>727.0</td>\n",
       "      <td>713.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>3741.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>her</td>\n",
       "      <td>3.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>1284.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>456.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>811.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3259.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>she</td>\n",
       "      <td>5.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1155.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>394.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3183.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>but</td>\n",
       "      <td>19.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>1105.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>378.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>2987.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>not</td>\n",
       "      <td>33.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1176.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>266.0</td>\n",
       "      <td>439.0</td>\n",
       "      <td>542.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>2970.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>they</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>610.0</td>\n",
       "      <td>267.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>433.0</td>\n",
       "      <td>619.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2503.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>him</td>\n",
       "      <td>42.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>626.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>482.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2430.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>have</td>\n",
       "      <td>57.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>819.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>416.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>2146.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>were</td>\n",
       "      <td>15.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>731.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>395.0</td>\n",
       "      <td>363.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1944.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>there</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>483.0</td>\n",
       "      <td>395.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1938.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>all</td>\n",
       "      <td>12.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>283.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1854.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>from</td>\n",
       "      <td>45.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>413.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>279.0</td>\n",
       "      <td>382.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1762.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Word  Proceedings of a Board of General Officers  Arachne, Volume 6.  \\\n",
       "0      the                                       348.0              1052.0   \n",
       "1      and                                       184.0               400.0   \n",
       "2     that                                       116.0                93.0   \n",
       "4      was                                        72.0               135.0   \n",
       "8      his                                        64.0               287.0   \n",
       "15     you                                        41.0                53.0   \n",
       "16     for                                        40.0               103.0   \n",
       "18     had                                        36.0               178.0   \n",
       "14    with                                        42.0               123.0   \n",
       "261    her                                         3.0               108.0   \n",
       "150    she                                         5.0                42.0   \n",
       "37     but                                        19.0                67.0   \n",
       "20     not                                        33.0                71.0   \n",
       "72    they                                        10.0                15.0   \n",
       "13     him                                        42.0               156.0   \n",
       "10    have                                        57.0                42.0   \n",
       "47    were                                        15.0                43.0   \n",
       "339  there                                         3.0                18.0   \n",
       "61     all                                        12.0                22.0   \n",
       "12    from                                        45.0                88.0   \n",
       "\n",
       "     Poor Jack  The Pirate Shark  Pardners  The Tale of Chloe  \\\n",
       "0       6677.0            2464.0    2775.0             1233.0   \n",
       "1       5275.0            1245.0    1536.0              701.0   \n",
       "2       2098.0             600.0     419.0              295.0   \n",
       "4       2359.0             478.0     416.0              244.0   \n",
       "8        966.0             580.0     543.0              191.0   \n",
       "15      2164.0             409.0     434.0              305.0   \n",
       "16      1377.0             310.0     300.0              239.0   \n",
       "18      1314.0             322.0     231.0              125.0   \n",
       "14      1021.0             317.0     328.0              166.0   \n",
       "261     1284.0              38.0     154.0              456.0   \n",
       "150     1155.0              40.0     184.0              394.0   \n",
       "37      1105.0             247.0     244.0              117.0   \n",
       "20      1176.0             156.0      58.0              161.0   \n",
       "72       610.0             267.0     290.0              102.0   \n",
       "13       626.0             246.0     303.0              123.0   \n",
       "10       819.0              90.0      93.0              169.0   \n",
       "47       731.0             155.0      63.0               58.0   \n",
       "339      512.0             226.0     136.0               62.0   \n",
       "61       666.0             186.0     149.0               49.0   \n",
       "12       413.0             108.0     203.0               74.0   \n",
       "\n",
       "     The Real Robert Burns  Vignettes of Manhattan; Outlines in Local Color  \\\n",
       "0                   2400.0                                           6427.0   \n",
       "1                   1365.0                                           2973.0   \n",
       "2                    536.0                                           1165.0   \n",
       "4                    412.0                                           1436.0   \n",
       "8                    779.0                                            831.0   \n",
       "15                    86.0                                            804.0   \n",
       "16                   320.0                                            709.0   \n",
       "18                   173.0                                            971.0   \n",
       "14                   277.0                                            727.0   \n",
       "261                  167.0                                            811.0   \n",
       "150                   84.0                                           1065.0   \n",
       "37                   246.0                                            502.0   \n",
       "20                   266.0                                            439.0   \n",
       "72                   131.0                                            433.0   \n",
       "13                   158.0                                            482.0   \n",
       "10                   127.0                                            289.0   \n",
       "47                    94.0                                            395.0   \n",
       "339                   59.0                                            483.0   \n",
       "61                   113.0                                            283.0   \n",
       "12                   147.0                                            279.0   \n",
       "\n",
       "     The Children of the Poor  A. V. Laider      Sum  \n",
       "0                      7056.0         316.0  30748.0  \n",
       "1                      2409.0         206.0  16294.0  \n",
       "2                      1286.0         170.0   6778.0  \n",
       "4                       801.0         208.0   6561.0  \n",
       "8                       604.0          49.0   4894.0  \n",
       "15                       50.0         112.0   4458.0  \n",
       "16                      824.0          65.0   4287.0  \n",
       "18                      481.0         149.0   3980.0  \n",
       "14                      713.0          27.0   3741.0  \n",
       "261                     215.0          23.0   3259.0  \n",
       "150                     194.0          20.0   3183.0  \n",
       "37                      378.0          62.0   2987.0  \n",
       "20                      542.0          68.0   2970.0  \n",
       "72                      619.0          26.0   2503.0  \n",
       "13                      261.0          33.0   2430.0  \n",
       "10                      416.0          44.0   2146.0  \n",
       "47                      363.0          27.0   1944.0  \n",
       "339                     395.0          44.0   1938.0  \n",
       "61                      330.0          44.0   1854.0  \n",
       "12                      382.0          23.0   1762.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_count.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-strategy",
   "metadata": {},
   "source": [
    "## Second testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-finish",
   "metadata": {},
   "source": [
    "This definately needs some proper refactoring, but Was curious whether we get anything decent from reading a bunch of random books in\n",
    "\n",
    "Requires an additional folder \"decades\" in the root directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "narrow-stomach",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:07:45.785832Z",
     "start_time": "2021-01-26T11:07:38.607241Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get all filenames\n",
    "files = [f for f in listdir(file_path) if isfile(join(file_path, f))]\n",
    "\n",
    "# Do only subset\n",
    "## Is done for 5000 files already, so set down to 20 to increase performance. 5000 books are currently stored in the file\n",
    "files = files[0:20]\n",
    "\n",
    "\n",
    "counter = 0\n",
    "for file in files:\n",
    "    counter = counter + 1\n",
    "    # Read in basic information from file\n",
    "    title, author, date, year, language, encoding, content_lines = read_file(file_path + \"/\" + file)\n",
    "    #line_count = len(content_lines)\n",
    "    decade = math.floor(year / 10) * 10\n",
    "    decade_file = \"decades/\" + str(decade) + \".txt\"\n",
    "    content_all = \" \".join(content_lines)\n",
    "    \n",
    "    if os.path.exists(decade_file):\n",
    "        append_write = 'a' # append if already exists\n",
    "    else:\n",
    "        append_write = 'w' # make a new file if not\n",
    "\n",
    "    fileWriter = open(decade_file,append_write)\n",
    "    fileWriter.write(content_all + '\\n')\n",
    "    fileWriter.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-delicious",
   "metadata": {},
   "source": [
    "### Read in from the decades files, and see the distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "formed-amazon",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:08:25.354714Z",
     "start_time": "2021-01-26T11:07:45.786793Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00.txt', '0.txt', '2010.txt', '2000.txt', '2020.txt', '1990.txt']\n",
      "2020.txt\n",
      "2010.txt\n",
      "2000.txt\n",
      "1990.txt\n",
      "00.txt\n",
      "0.txt\n"
     ]
    }
   ],
   "source": [
    "# Get all filenames\n",
    "files = [f for f in listdir(\"decades\") if isfile(join(\"decades\", f))]\n",
    "print(files)\n",
    "files.sort(reverse=True)\n",
    "\n",
    "\n",
    "col_names = []\n",
    "col_names.append(\"Word\")\n",
    "\n",
    "tables = []\n",
    "\n",
    "for file_name in files:\n",
    "    print(file_name)\n",
    "    \n",
    "    file = open(\"decades/\" + file_name, encoding=\"ISO-8859-1\")\n",
    "    file_content = file.read()\n",
    "    \n",
    "    # Split into words (and do various cleaning)\n",
    "    all_text_lower = file_content.lower()\n",
    "    words = re.findall(r'(\\b[A-Za-z][a-z]{2,9}\\b)', all_text_lower)\n",
    "\n",
    "    # First analysis, but should do something proper\n",
    "    word_frequencies_table, unique_word_count = get_word_frequencies(words)\n",
    "    tables.append(word_frequencies_table)\n",
    "    col_names.append(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-sociology",
   "metadata": {},
   "source": [
    "### Preliminary Conclusion\n",
    "We see that even though the books are quite old, no decade prior to 1990s is found.\n",
    "\n",
    "This is when we found out that the \"year\" that's registered in the dataset is the upload-date. \n",
    "\n",
    "Haven gotten this far, we however decided to see if we could find a pattern in this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-paper",
   "metadata": {},
   "source": [
    "### Compare ranking between upload-decades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "gentle-deposit",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:08:26.674707Z",
     "start_time": "2021-01-26T11:08:25.355570Z"
    }
   },
   "outputs": [],
   "source": [
    "list_count= []\n",
    "list_freq = []\n",
    "list_rank = []\n",
    "\n",
    "for df in tables:\n",
    "    #list_count.append(df[['Word', 'count']])\n",
    "    #list_freq.append(df[['Word', 'freq']])\n",
    "    list_rank.append(df[['Word', 'rank']])\n",
    "    \n",
    "#df_count = reduce(lambda left, right: pd.merge(left, right, on=\"Word\", how='outer'), list_count)\n",
    "#df_count.columns = col_names\n",
    "\n",
    "#df_freq = reduce(lambda left, right: pd.merge(left, right, on=\"Word\", how='outer'), list_freq)\n",
    "#df_freq.columns = col_names\n",
    "\n",
    "df_rank = reduce(lambda left, right: pd.merge(left, right, on=\"Word\", how='outer'), list_rank)\n",
    "df_rank.columns = col_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "defined-snowboard",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:08:26.710631Z",
     "start_time": "2021-01-26T11:08:26.675653Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>2020.txt</th>\n",
       "      <th>2010.txt</th>\n",
       "      <th>2000.txt</th>\n",
       "      <th>1990.txt</th>\n",
       "      <th>00.txt</th>\n",
       "      <th>0.txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>was</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>with</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>for</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>his</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>not</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>had</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>but</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>which</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>they</td>\n",
       "      <td>13.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>from</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>were</td>\n",
       "      <td>15.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>have</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>this</td>\n",
       "      <td>17.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>are</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>she</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>all</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>their</td>\n",
       "      <td>21.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>him</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>her</td>\n",
       "      <td>23.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>its</td>\n",
       "      <td>24.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>one</td>\n",
       "      <td>25.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>there</td>\n",
       "      <td>26.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>them</td>\n",
       "      <td>27.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>what</td>\n",
       "      <td>28.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>has</td>\n",
       "      <td>29.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>been</td>\n",
       "      <td>30.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>will</td>\n",
       "      <td>31.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>would</td>\n",
       "      <td>32.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>said</td>\n",
       "      <td>33.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>when</td>\n",
       "      <td>34.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>more</td>\n",
       "      <td>35.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>who</td>\n",
       "      <td>36.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>into</td>\n",
       "      <td>37.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>out</td>\n",
       "      <td>38.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>then</td>\n",
       "      <td>39.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>other</td>\n",
       "      <td>40.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>men</td>\n",
       "      <td>41.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>only</td>\n",
       "      <td>42.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>can</td>\n",
       "      <td>43.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>upon</td>\n",
       "      <td>44.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>our</td>\n",
       "      <td>45.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>than</td>\n",
       "      <td>46.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>now</td>\n",
       "      <td>47.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>time</td>\n",
       "      <td>48.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>power</td>\n",
       "      <td>49.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>327.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>111.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>great</td>\n",
       "      <td>50.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>these</td>\n",
       "      <td>51.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>government</td>\n",
       "      <td>52.0</td>\n",
       "      <td>384.0</td>\n",
       "      <td>441.0</td>\n",
       "      <td>534.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>146.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>man</td>\n",
       "      <td>53.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>over</td>\n",
       "      <td>54.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>139.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>could</td>\n",
       "      <td>55.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>very</td>\n",
       "      <td>56.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>your</td>\n",
       "      <td>57.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>first</td>\n",
       "      <td>58.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>society</td>\n",
       "      <td>59.0</td>\n",
       "      <td>581.0</td>\n",
       "      <td>602.0</td>\n",
       "      <td>535.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>231.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>two</td>\n",
       "      <td>60.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>made</td>\n",
       "      <td>61.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>such</td>\n",
       "      <td>62.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>about</td>\n",
       "      <td>63.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>some</td>\n",
       "      <td>64.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>any</td>\n",
       "      <td>65.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>did</td>\n",
       "      <td>66.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>know</td>\n",
       "      <td>67.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>pendleton</td>\n",
       "      <td>68.0</td>\n",
       "      <td>3493.0</td>\n",
       "      <td>3821.0</td>\n",
       "      <td>773.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>same</td>\n",
       "      <td>69.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>well</td>\n",
       "      <td>70.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>under</td>\n",
       "      <td>71.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>may</td>\n",
       "      <td>72.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>general</td>\n",
       "      <td>73.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>374.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>195.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>before</td>\n",
       "      <td>74.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>most</td>\n",
       "      <td>75.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>even</td>\n",
       "      <td>76.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>much</td>\n",
       "      <td>77.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>like</td>\n",
       "      <td>78.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>stephanie</td>\n",
       "      <td>79.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>lorraine</td>\n",
       "      <td>80.0</td>\n",
       "      <td>3367.0</td>\n",
       "      <td>3682.0</td>\n",
       "      <td>766.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>383.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>those</td>\n",
       "      <td>81.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>down</td>\n",
       "      <td>82.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>back</td>\n",
       "      <td>83.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>217.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>came</td>\n",
       "      <td>84.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>236.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>see</td>\n",
       "      <td>85.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>how</td>\n",
       "      <td>86.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>way</td>\n",
       "      <td>87.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>145.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>think</td>\n",
       "      <td>88.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>little</td>\n",
       "      <td>89.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>without</td>\n",
       "      <td>90.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>here</td>\n",
       "      <td>91.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>against</td>\n",
       "      <td>92.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>people</td>\n",
       "      <td>93.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>124.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>after</td>\n",
       "      <td>94.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>must</td>\n",
       "      <td>95.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>don</td>\n",
       "      <td>95.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>274.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>where</td>\n",
       "      <td>96.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>never</td>\n",
       "      <td>97.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>own</td>\n",
       "      <td>98.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>right</td>\n",
       "      <td>99.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>161.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word  2020.txt  2010.txt  2000.txt  1990.txt  00.txt  0.txt\n",
       "0          the       1.0       1.0       1.0       1.0     1.0    1.0\n",
       "1          and       2.0       2.0       2.0       2.0     2.0    2.0\n",
       "2         that       3.0       3.0       3.0       3.0     4.0    3.0\n",
       "3          was       4.0       4.0       4.0       4.0    23.0    5.0\n",
       "4          you       5.0      10.0       8.0       5.0   163.0   19.0\n",
       "5         with       6.0       6.0       6.0       7.0     3.0    6.0\n",
       "6          for       7.0       7.0       7.0      11.0    13.0    8.0\n",
       "7          his       8.0       5.0       5.0       6.0     5.0    4.0\n",
       "8          not       9.0       8.0      11.0      12.0    11.0    9.0\n",
       "9          had      10.0       9.0       9.0      10.0    40.0   14.0\n",
       "10         but      11.0      11.0      10.0      13.0    72.0   10.0\n",
       "11       which      12.0      12.0      15.0      22.0    82.0    7.0\n",
       "12        they      13.0      17.0      14.0      16.0    58.0   21.0\n",
       "13        from      14.0      14.0      18.0      21.0    25.0   15.0\n",
       "14        were      15.0      21.0      21.0      20.0    81.0   22.0\n",
       "15        have      16.0      16.0      16.0      18.0    22.0   11.0\n",
       "16        this      17.0      13.0      17.0      15.0     8.0   13.0\n",
       "17         are      18.0      18.0      24.0      27.0    37.0   16.0\n",
       "18         she      19.0      20.0      13.0       9.0    65.0   47.0\n",
       "19         all      20.0      19.0      20.0      17.0    15.0   12.0\n",
       "20       their      21.0      24.0      25.0      26.0    21.0   30.0\n",
       "21         him      22.0      22.0      19.0      14.0    28.0   20.0\n",
       "22         her      23.0      15.0      12.0       8.0    20.0   33.0\n",
       "23         its      24.0      40.0      54.0      98.0    68.0   67.0\n",
       "24         one      25.0      23.0      22.0      25.0    17.0   28.0\n",
       "25       there      26.0      25.0      23.0      23.0    37.0   32.0\n",
       "26        them      27.0      28.0      28.0      32.0    55.0   38.0\n",
       "27        what      28.0      33.0      32.0      24.0    56.0   24.0\n",
       "28         has      29.0      36.0      46.0      46.0   176.0   40.0\n",
       "29        been      30.0      29.0      31.0      33.0    30.0   23.0\n",
       "30        will      31.0      32.0      33.0      37.0    85.0   27.0\n",
       "31       would      32.0      30.0      29.0      31.0   194.0   25.0\n",
       "32        said      33.0      31.0      26.0      19.0    61.0  141.0\n",
       "33        when      34.0      27.0      27.0      28.0   105.0   34.0\n",
       "34        more      35.0      34.0      36.0      42.0   190.0   26.0\n",
       "35         who      36.0      26.0      30.0      30.0    27.0   18.0\n",
       "36        into      37.0      37.0      37.0      39.0    97.0   63.0\n",
       "37         out      38.0      35.0      34.0      29.0    62.0   77.0\n",
       "38        then      39.0      38.0      35.0      35.0    47.0   50.0\n",
       "39       other      40.0      44.0      56.0      58.0    88.0   66.0\n",
       "40         men      41.0      72.0      78.0      81.0   108.0   53.0\n",
       "41        only      42.0      48.0      53.0      60.0   192.0   71.0\n",
       "42         can      43.0      59.0      48.0      45.0    98.0   54.0\n",
       "43        upon      44.0      51.0      57.0      80.0   121.0   89.0\n",
       "44         our      45.0      55.0      50.0      91.0   149.0   43.0\n",
       "45        than      46.0      47.0      49.0      64.0   201.0   31.0\n",
       "46         now      47.0      43.0      38.0      38.0   134.0   42.0\n",
       "47        time      48.0      42.0      43.0      49.0    96.0   57.0\n",
       "48       power      49.0     244.0     255.0     327.0   204.0  111.0\n",
       "49       great      50.0      62.0      63.0      79.0    64.0   37.0\n",
       "50       these      51.0      50.0      66.0      87.0    48.0   60.0\n",
       "51  government      52.0     384.0     441.0     534.0     NaN  146.0\n",
       "52         man      53.0      41.0      41.0      34.0   222.0   29.0\n",
       "53        over      54.0      68.0      64.0      62.0    92.0  139.0\n",
       "54       could      55.0      45.0      40.0      41.0   199.0   64.0\n",
       "55        very      56.0      46.0      42.0      50.0   159.0   65.0\n",
       "56        your      57.0      57.0      45.0      36.0   201.0   55.0\n",
       "57       first      58.0      65.0      76.0      77.0   219.0   81.0\n",
       "58     society      59.0     581.0     602.0     535.0     NaN  231.0\n",
       "59         two      60.0      52.0      59.0      70.0   124.0  100.0\n",
       "60        made      61.0      61.0      62.0      83.0   174.0   82.0\n",
       "61        such      62.0      63.0      75.0      78.0   146.0   45.0\n",
       "62       about      63.0      53.0      44.0      40.0    99.0  113.0\n",
       "63        some      64.0      39.0      39.0      44.0    70.0   49.0\n",
       "64         any      65.0      54.0      55.0      55.0   130.0   49.0\n",
       "65         did      66.0      60.0      52.0      54.0   154.0   99.0\n",
       "66        know      67.0      83.0      74.0      48.0   159.0   98.0\n",
       "67   pendleton      68.0    3493.0    3821.0     773.0     NaN    NaN\n",
       "68        same      69.0      98.0     125.0     118.0   203.0  112.0\n",
       "69        well      70.0      67.0      58.0      52.0    54.0   58.0\n",
       "70       under      71.0     103.0     120.0     153.0   129.0  103.0\n",
       "71         may      72.0      49.0      70.0      86.0   162.0   36.0\n",
       "72     general      73.0     197.0     225.0     374.0   254.0  195.0\n",
       "73      before      74.0      66.0      65.0      59.0   101.0   84.0\n",
       "74        most      75.0      81.0      88.0     120.0   243.0   46.0\n",
       "75        even      76.0      88.0     100.0     125.0    77.0   78.0\n",
       "76        much      77.0      77.0      77.0      76.0   228.0   73.0\n",
       "77        like      78.0      64.0      51.0      47.0    10.0   74.0\n",
       "78   stephanie      79.0       NaN    3800.0       NaN     NaN    NaN\n",
       "79    lorraine      80.0    3367.0    3682.0     766.0     NaN  383.0\n",
       "80       those      81.0      79.0      91.0     138.0    26.0   44.0\n",
       "81        down      82.0      76.0      69.0      63.0    63.0  126.0\n",
       "82        back      83.0      97.0      86.0      85.0   165.0  217.0\n",
       "83        came      84.0      94.0      85.0      74.0   172.0  236.0\n",
       "84         see      85.0      69.0      60.0      51.0   150.0   88.0\n",
       "85         how      86.0      84.0      73.0      57.0   152.0   56.0\n",
       "86         way      87.0      87.0      82.0      69.0   118.0  145.0\n",
       "87       think      88.0     125.0     103.0      82.0   227.0  114.0\n",
       "88      little      89.0      56.0      47.0      43.0   229.0   80.0\n",
       "89     without      90.0     107.0     107.0     107.0   105.0   79.0\n",
       "90        here      91.0      82.0      81.0      68.0    89.0   93.0\n",
       "91     against      92.0     135.0     144.0     171.0   220.0   92.0\n",
       "92      people      93.0     111.0     116.0     121.0   203.0  124.0\n",
       "93       after      94.0      58.0      61.0      53.0   103.0   95.0\n",
       "94        must      95.0      71.0      79.0      88.0   213.0   69.0\n",
       "95         don      95.0     147.0     105.0      56.0     NaN  274.0\n",
       "96       where      96.0      75.0      71.0      84.0   151.0  114.0\n",
       "97       never      97.0      89.0      83.0      75.0   172.0   83.0\n",
       "98         own      98.0      92.0      92.0      96.0   111.0   68.0\n",
       "99       right      99.0     133.0     135.0     117.0   236.0  161.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rank.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-jewelry",
   "metadata": {},
   "source": [
    "## Trying to fit models to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-memorabilia",
   "metadata": {},
   "source": [
    "### Read in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "invalid-directory",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:08:26.912975Z",
     "start_time": "2021-01-26T11:08:26.711715Z"
    }
   },
   "outputs": [],
   "source": [
    "file_contents = []\n",
    "targets = []\n",
    "\n",
    "files = [f for f in listdir(file_path) if isfile(join(file_path, f))]\n",
    "files = list(filter(lambda file: file[0].isdigit(), files))\n",
    "random.shuffle(files)\n",
    "\n",
    "targets_=['70','80','90','00','10']\n",
    "iter_ = 0\n",
    "\n",
    "for f in files[:120]:\n",
    "    file = open(\"processedData/\" + f, encoding=\"ISO-8859-1\")\n",
    "    file_contents.append(file.read())\n",
    "    iter_ = iter_+1\n",
    "    targets.append(targets_[iter_%5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-mortality",
   "metadata": {},
   "source": [
    "### Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "muslim-coating",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:08:46.733075Z",
     "start_time": "2021-01-26T11:08:26.914003Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   16.4s finished\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('kbest', SelectKBest(chi2, k=100)),\n",
    "    ('nb', MultinomialNB()),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    #'vect__max_df': [1.0),\n",
    "    # 'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    #'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    # 'tfidf__use_idf': (True, False),\n",
    "    # 'tfidf__norm': ('l1', 'l2'),\n",
    "    #'clf__max_iter': (20),\n",
    "    #'clf__alpha': (0.00001),\n",
    "    #'clf__penalty': ('l2'),\n",
    "    # 'clf__max_iter': (10, 50, 80),\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, verbose=1)\n",
    "\n",
    "grid_search.fit(file_contents, targets)\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-involvement",
   "metadata": {},
   "source": [
    "## Realisation and conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-fabric",
   "metadata": {},
   "source": [
    "At this point, we came to the conclusion that \"year\" in the Gutenberg dataset shows when the data **was published** to the project, and not the release date of the book.\n",
    "\n",
    "We searched for possible solutions to get the years for book publications, but were unable to find any free API that we could link to our current dataset.\n",
    "\n",
    "We therefore went on a search for other datasets, and to remake our hypothesis entirely.\n",
    "Thus, this part ended in a blind spot. However science is not only about the results, but also about the discoveries along the way, and therefore it is added into this file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-chapel",
   "metadata": {},
   "source": [
    "# Studying language change in Icelandic parliamentary speeches\n",
    "\n",
    "Our task involves research into **language change over the past 100 years**. Additionally we have been tasked with working out factors that influence language change. \n",
    "\n",
    "Another proposed research question could have been focused on figuring out which languages are going extinct. This particular task has been found out to be near impossible to answer given the available data. It is estimated to be very hard to come up with data that capture the amount of speakers for a large enough ranges of combinations of language and year. Furthermore, any data that are available are likely to apply a different definition of \"speaker\" (sometimes including second/third... language speakers, sometimes not) and is also likely to contain politically motivated noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-valve",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-moldova",
   "metadata": {},
   "source": [
    "Therefore, we decided to search for English language corpora containing a wide array of text documents collected over the past century for predefined dialects of English and genre of text (movie, articles, books, ...). This surprisingly turned out to be a complex endeavour as all high quality corpora were available only for a big price tag. \n",
    "\n",
    "We also looked into the material provided by the Guttenberg Project [Link](https://www.projekt-gutenberg.org/). This turned out to be promising at first sight as it appears that there is a lot of recently published material. However release date of these documents does not match the year when the documents were actually written and soon enough we figured out that all material is from before 1923. This obviously did not allow us to look much into language change of the 20th and 21st century.\n",
    "\n",
    "_Gerlach, M., & Font-Clos, F. (2020). A standardized Project Gutenberg corpus for statistical analysis of natural language and quantitative linguistics. Entropy, 22(1), 126._\n",
    "\n",
    "Theoretically one could obtain books from after 1923 and include them into the analysis. But one would quickly run into copyright/licensing issues here.\n",
    "\n",
    "Obtaining the content of these books and preprocessing them for the purposes of data analysis turned out to be quite cumbersome as well. Look at Gunnar's notebooks (first draft [here](firstDraft.ipynb), second draft [here](secondDraft.ipynb)) for the details. \n",
    "\n",
    "Finally we turned to looking for non-English corpora and **found an annotated corpus including pre-factured lemmatization of [Icelandic parlimentary speeches](https://clarin.is/en/resources/parliament/) from 1911 until 2018:**\n",
    "\n",
    "_Steingrímsson, Steinþór, Sigrún Helgadóttir, Eiríkur Rögnvaldsson, Starkaður Barkarson and Jón Guðnason. 2018. Risamálheild: A Very Large Icelandic Text Corpus. Proceedings of LREC 2018, pp. 4361-4366. Myazaki, Japan._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-framing",
   "metadata": {},
   "source": [
    "## The task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-medicare",
   "metadata": {},
   "source": [
    "In the line with our goal of analyzing the change in language over the past 100 years, we decided to train different models and assess their ability to predict whether an speech held in the Icelandic Parlament belongs to a particular decade. In the end, this is a **document classification task**  in which the input is a large set of parlament speeches and the target/class is the decade in which the speeches were held. \n",
    "\n",
    "A good performance of our proposed classifiers may support the idea that Icelandic has envolved in the years. However, the fact that the models would perform well is not enough to assert that the language has changed. It could be that what has actually changed are the topics or even the way of documenting the speeches. Anyway, for us it was really exiciting to check whether we are able to **fit a  model that predicts reasonably well the decade of an speech by only using the speech itself.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-yacht",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-sigma",
   "metadata": {},
   "source": [
    "In this section, we provide the **setup for a successful** implementation (or replication) of our experiment within this Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-washer",
   "metadata": {},
   "source": [
    "### Load required libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-naples",
   "metadata": {},
   "source": [
    "The following libraries are used during the next sections and therefore need to be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "arctic-season",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:08:46.954776Z",
     "start_time": "2021-01-26T11:08:46.733982Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import glob\n",
    "from nltk.probability import FreqDist\n",
    "import random\n",
    "from functools import reduce\n",
    "from nltk import ngrams\n",
    "# Used for building models for classifying:\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "pressing-fleet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:08:46.957443Z",
     "start_time": "2021-01-26T11:08:46.955662Z"
    }
   },
   "outputs": [],
   "source": [
    "#needed afterwards too\n",
    "namespace = \"{http://www.tei-c.org/ns/1.0}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-spectrum",
   "metadata": {},
   "source": [
    "### Get the data\n",
    "\n",
    "Data can be downloaded from here: http://www.malfong.is/index.php?dlid=81&lang=en. However, we provided already in our submission file the specifications on how to get the data of our assignment.\n",
    "\n",
    "Then extract zip folder such that a folder labelled `CC_BY` shows up in the parent folder of this notebook. *Test*: `ls ../CC_BY/althingi` should work when run from `.../IcelandicParliamentSpeeches.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-section",
   "metadata": {},
   "source": [
    "### Preprocessing helpers\n",
    "\n",
    "The data are available as XML. The text has already been preprocessed to be separated into paragraphs, sentences and words. Furthermore each word tag also includes a `lemma` attribute relating inflected/declensed forms of words to its lemma. This has been done by the authors of the original paper using Machine Learning approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-leader",
   "metadata": {},
   "source": [
    "Given a relative path to a file, pull out a list with all the words. This can be achieved by looking for all tags of type `w`, additionally also retrieve the lemma for each word.\n",
    "\n",
    "We will discard all sentences of length 3 or smaller to remove noise and to avoid that our models are able to detect year of speech just based on some short introductory/outro phrases. Furthermore the raw data appear to contain plenty of elements tagged as words that comprise of just a single letter followed by a dot. These will be removed here as well.\n",
    "\n",
    "⚠️ *Pitfall*: The namespace from above must be included when parsing out content from these XML files based on tag names.\n",
    "\n",
    "⚠️ In this kind of preprocessing we lose information about sentence boundaries as all punctuation items from the raw data are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "moved-retention",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:08:46.965584Z",
     "start_time": "2021-01-26T11:08:46.958402Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_words(path):\n",
    "    xml_tree = ET.parse(open(path, 'r', encoding=\"utf8\"))\n",
    "    words = []\n",
    "    lemmata = []\n",
    "    \n",
    "    for sentence in xml_tree.getroot().iter('{}s'.format(namespace)):\n",
    "        words_in_sent = sentence.findall('{}w'.format(namespace))\n",
    "        if len(words_in_sent) > 2:\n",
    "            for word in sentence.findall('{}w'.format(namespace)):\n",
    "                if not word.text.endswith('.'):\n",
    "                    words.append(word.text)\n",
    "                    lemmata.append(word.attrib['lemma'])\n",
    "        \n",
    "    return words, lemmata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-magnitude",
   "metadata": {},
   "source": [
    "Extract content of files separated into sentences, note that all stop items are wrapped in a `p` tag in the original documents and are not included here.\n",
    "\n",
    "Also note that some further pre-processing could be done here to exclude items such as numbers, percentages, names, abbreviations, etc. In the original documents these are also assigned to be words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "revolutionary-america",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:08:46.973518Z",
     "start_time": "2021-01-26T11:08:46.966649Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_sentences(path, lemma=False):\n",
    "    xml_tree = ET.parse(open(path, 'r', encoding=\"utf8\"))\n",
    "    sentences = []\n",
    "    \n",
    "    for sentence in xml_tree.getroot().iter('{}s'.format(namespace)):\n",
    "        sentence_cur = []\n",
    "        words_in_sent = sentence.findall('{}w'.format(namespace))\n",
    "        \n",
    "        if len(words_in_sent) > 2:\n",
    "            for word in sentence.findall('{}w'.format(namespace)):\n",
    "                if not word.text.endswith('.'):\n",
    "                    if lemma:\n",
    "                        sentence_cur.append(word.attrib['lemma'])\n",
    "                    else:\n",
    "                        sentence_cur.append(word.text)\n",
    "            \n",
    "            sentences.append(sentence_cur)\n",
    "        \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-rendering",
   "metadata": {},
   "source": [
    "Retrieve a random selection of `k` file names from the entire corpus. The files must be of type `xml`. This method does not load the entire corpus into memory and allows you to work with smaller selections for test purposes. This method samples only from the `althingi` folder so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "piano-value",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:08:46.978663Z",
     "start_time": "2021-01-26T11:08:46.974482Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_random_sample(k):\n",
    "    files = [filename for filename in glob.iglob('../CC_BY/althingi/**/*.xml', \n",
    "                                                 recursive=True)]\n",
    "    return random.sample(files, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dental-butter",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:08:47.730685Z",
     "start_time": "2021-01-26T11:08:46.979571Z"
    }
   },
   "outputs": [],
   "source": [
    "files = [filename for filename in glob.iglob('../CC_BY/althingi/**/*.xml', recursive=True)]\n",
    "#print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-castle",
   "metadata": {},
   "source": [
    "Do the same as above but choose `k` files only from a given year (range: 1911-2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "valuable-indicator",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:08:47.734846Z",
     "start_time": "2021-01-26T11:08:47.731651Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_files_for_year(year, k = None):\n",
    "    files = [filename for filename in glob.iglob('../CC_BY/althingi/{}/'.format(year) + '**/*.xml', \n",
    "                                                 recursive=True)]\n",
    "    if k == None:\n",
    "        newK = len(files)\n",
    "        res = files\n",
    "    else:\n",
    "        newK = k\n",
    "        res =  random.sample(files, min(len(files), k))\n",
    "    if len(files) != 0:\n",
    "        percentage = 100*newK/len(files)\n",
    "    else: \n",
    "        percentage = 0\n",
    "    print(\"For year \" + str(year) + \": Fetching \" + str(newK) +\" samples out of \" + str(len(files)) + \" (~\" + str(percentage) + \"%)\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-share",
   "metadata": {},
   "source": [
    "## Preliminary Data Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-darwin",
   "metadata": {},
   "source": [
    "In this section, we perform a preliminary data analysis to get a better insight of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-brass",
   "metadata": {},
   "source": [
    "### Zipf's Law\n",
    "\n",
    "First using frequency distributions of the Natural Language ToolKit (`NLTK`) to look into whether or not we can confirm [Zipf's Law](https://en.wikipedia.org/wiki/Zipf%27s_law) based on the data we have.\n",
    "\n",
    "⚠️ Note that the analysis is done based on 15 randomly selected files from the entire corpus at this point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "figured-execution",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:08:48.679552Z",
     "start_time": "2021-01-26T11:08:47.735819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Samples', ylabel='Counts'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "\n",
    "for file in get_random_sample(15):\n",
    "    words.extend(extract_words(file)[1])\n",
    "    \n",
    "fq = FreqDist(word.lower() for word in words)\n",
    "fq.plot(25, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-establishment",
   "metadata": {},
   "source": [
    "Visualizing the same data but with using the logarithm of the occurrences, this should ideally obtain a straight line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cheap-width",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:08:48.771518Z",
     "start_time": "2021-01-26T11:08:48.680873Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "freq_df = pd.DataFrame.from_dict(fq, orient='index', columns=['word_occur'])\n",
    "freq_df.sort_values(by='word_occur', inplace=True, ascending=False)\n",
    "freq_df.word_occur = np.log2(freq_df['word_occur'])\n",
    "freq_df.head(25).plot(kind='line')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-nursing",
   "metadata": {},
   "source": [
    "### Disappearing words / new words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-waters",
   "metadata": {},
   "source": [
    "Here is a description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "grateful-hands",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:08:48.841548Z",
     "start_time": "2021-01-26T11:08:48.772534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For year 1914: Fetching 25 samples out of 1306 (~1.9142419601837672%)\n",
      "For year 2014: Fetching 25 samples out of 12404 (~0.2015478877781361%)\n"
     ]
    }
   ],
   "source": [
    "words_1914 = []\n",
    "words_2014 = []\n",
    "\n",
    "for file in get_files_for_year(1914, 25):\n",
    "    words_1914.extend(extract_words(file)[1])\n",
    "    \n",
    "for file in get_files_for_year(2014, 25):\n",
    "    words_2014.extend(extract_words(file)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-funeral",
   "metadata": {},
   "source": [
    "### Development of average sentence length\n",
    "\n",
    "This is just one possible metric for the development/analysis of language complexity. There is so much more you could come up with here.\n",
    "\n",
    "Obviously our choice to discard very short sentences in the preprocessing step has an impact on the values here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "stone-edmonton",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:08:54.775145Z",
     "start_time": "2021-01-26T11:08:48.842453Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For year 1924: Fetching 20 samples out of 41 (~48.78048780487805%)\n",
      "For year 1925: Fetching 20 samples out of 23 (~86.95652173913044%)\n",
      "For year 1926: Fetching 20 samples out of 37 (~54.054054054054056%)\n",
      "For year 1927: Fetching 20 samples out of 59 (~33.898305084745765%)\n",
      "For year 1928: Fetching 20 samples out of 21 (~95.23809523809524%)\n",
      "For year 1929: Fetching 20 samples out of 96 (~20.833333333333332%)\n",
      "For year 1930: Fetching 20 samples out of 30 (~66.66666666666667%)\n",
      "For year 1931: Fetching 20 samples out of 25 (~80.0%)\n",
      "For year 1932: Fetching 20 samples out of 28 (~71.42857142857143%)\n",
      "For year 1933: Fetching 20 samples out of 52 (~38.46153846153846%)\n",
      "For year 1934: Fetching 20 samples out of 20 (~100.0%)\n",
      "For year 1935: Fetching 20 samples out of 24 (~83.33333333333333%)\n",
      "For year 1936: Fetching 20 samples out of 25 (~80.0%)\n",
      "For year 1937: Fetching 20 samples out of 1381 (~1.448225923244026%)\n",
      "For year 1938: Fetching 20 samples out of 1676 (~1.1933174224343674%)\n",
      "For year 1939: Fetching 20 samples out of 1632 (~1.2254901960784315%)\n",
      "For year 1940: Fetching 20 samples out of 1458 (~1.3717421124828533%)\n",
      "For year 1941: Fetching 20 samples out of 2066 (~0.968054211035818%)\n",
      "For year 1942: Fetching 20 samples out of 2357 (~0.8485362749257531%)\n",
      "For year 1943: Fetching 20 samples out of 3960 (~0.5050505050505051%)\n",
      "For year 1944: Fetching 20 samples out of 1072 (~1.8656716417910448%)\n",
      "For year 1945: Fetching 20 samples out of 1859 (~1.0758472296933836%)\n",
      "For year 1946: Fetching 20 samples out of 2789 (~0.7171029042667623%)\n",
      "For year 1947: Fetching 20 samples out of 2838 (~0.704721634954193%)\n",
      "For year 1948: Fetching 20 samples out of 2262 (~0.8841732979664014%)\n",
      "For year 1949: Fetching 20 samples out of 2661 (~0.7515971439308531%)\n",
      "For year 1950: Fetching 20 samples out of 3017 (~0.6629101756711966%)\n",
      "For year 1951: Fetching 20 samples out of 2453 (~0.8153281695882593%)\n",
      "For year 1952: Fetching 20 samples out of 1689 (~1.1841326228537596%)\n",
      "For year 1953: Fetching 20 samples out of 1415 (~1.4134275618374559%)\n",
      "For year 1954: Fetching 20 samples out of 1546 (~1.2936610608020698%)\n",
      "For year 1955: Fetching 20 samples out of 1433 (~1.3956734124214933%)\n",
      "For year 1956: Fetching 20 samples out of 1213 (~1.6488046166529267%)\n",
      "For year 1957: Fetching 20 samples out of 1763 (~1.1344299489506522%)\n",
      "For year 1958: Fetching 20 samples out of 1169 (~1.7108639863130881%)\n",
      "For year 1959: Fetching 20 samples out of 1326 (~1.5082956259426847%)\n",
      "For year 1960: Fetching 20 samples out of 1862 (~1.0741138560687433%)\n",
      "For year 1961: Fetching 20 samples out of 1409 (~1.4194464158978%)\n",
      "For year 1962: Fetching 20 samples out of 1475 (~1.3559322033898304%)\n",
      "For year 1963: Fetching 20 samples out of 1376 (~1.4534883720930232%)\n",
      "For year 1964: Fetching 20 samples out of 1677 (~1.1926058437686344%)\n",
      "For year 1965: Fetching 20 samples out of 1520 (~1.3157894736842106%)\n",
      "For year 1966: Fetching 20 samples out of 1404 (~1.4245014245014245%)\n",
      "For year 1967: Fetching 20 samples out of 1274 (~1.5698587127158556%)\n",
      "For year 1968: Fetching 20 samples out of 1726 (~1.1587485515643106%)\n",
      "For year 1969: Fetching 20 samples out of 1740 (~1.1494252873563218%)\n",
      "For year 1970: Fetching 20 samples out of 1921 (~1.041124414367517%)\n",
      "For year 1971: Fetching 20 samples out of 2071 (~0.9657170449058425%)\n",
      "For year 1972: Fetching 20 samples out of 2368 (~0.8445945945945946%)\n",
      "For year 1973: Fetching 20 samples out of 2329 (~0.8587376556462001%)\n",
      "For year 1974: Fetching 20 samples out of 2350 (~0.851063829787234%)\n",
      "For year 1975: Fetching 20 samples out of 2430 (~0.823045267489712%)\n",
      "For year 1976: Fetching 20 samples out of 2555 (~0.7827788649706457%)\n",
      "For year 1977: Fetching 20 samples out of 2105 (~0.9501187648456056%)\n",
      "For year 1978: Fetching 20 samples out of 2641 (~0.7572889057175313%)\n",
      "For year 1979: Fetching 20 samples out of 2165 (~0.9237875288683602%)\n",
      "For year 1980: Fetching 20 samples out of 3406 (~0.5871990604815033%)\n",
      "For year 1981: Fetching 20 samples out of 3491 (~0.5729017473503294%)\n",
      "For year 1982: Fetching 20 samples out of 2894 (~0.691085003455425%)\n",
      "For year 1983: Fetching 20 samples out of 2651 (~0.7544322897019993%)\n",
      "For year 1984: Fetching 20 samples out of 4171 (~0.4795013186286262%)\n",
      "For year 1985: Fetching 20 samples out of 4709 (~0.42471862391165854%)\n",
      "For year 1986: Fetching 20 samples out of 3432 (~0.5827505827505828%)\n",
      "For year 1987: Fetching 20 samples out of 3212 (~0.6226650062266501%)\n",
      "For year 1988: Fetching 20 samples out of 4198 (~0.47641734159123394%)\n",
      "For year 1989: Fetching 20 samples out of 5085 (~0.39331366764995085%)\n",
      "For year 1990: Fetching 20 samples out of 4662 (~0.429000429000429%)\n",
      "For year 1991: Fetching 20 samples out of 4747 (~0.4213187276174426%)\n",
      "For year 1992: Fetching 20 samples out of 8925 (~0.22408963585434175%)\n",
      "For year 1993: Fetching 20 samples out of 7412 (~0.26983270372369134%)\n",
      "For year 1994: Fetching 20 samples out of 8187 (~0.2442897276169537%)\n",
      "For year 1995: Fetching 20 samples out of 5129 (~0.38993955936829794%)\n",
      "For year 1996: Fetching 20 samples out of 7184 (~0.27839643652561247%)\n",
      "For year 1997: Fetching 20 samples out of 6960 (~0.28735632183908044%)\n",
      "For year 1998: Fetching 20 samples out of 7393 (~0.27052617340727714%)\n",
      "For year 1999: Fetching 20 samples out of 6056 (~0.33025099075297226%)\n",
      "For year 2000: Fetching 20 samples out of 7466 (~0.2678810608090008%)\n",
      "For year 2001: Fetching 20 samples out of 8210 (~0.243605359317905%)\n",
      "For year 2002: Fetching 20 samples out of 8061 (~0.24810817516437167%)\n",
      "For year 2003: Fetching 20 samples out of 5872 (~0.3405994550408719%)\n",
      "For year 2004: Fetching 20 samples out of 9466 (~0.21128248468201985%)\n",
      "For year 2005: Fetching 20 samples out of 8269 (~0.24186721489902044%)\n",
      "For year 2006: Fetching 20 samples out of 8810 (~0.22701475595913734%)\n",
      "For year 2007: Fetching 20 samples out of 7863 (~0.2543558438255119%)\n",
      "For year 2008: Fetching 20 samples out of 8764 (~0.22820629849383842%)\n",
      "For year 2009: Fetching 20 samples out of 17262 (~0.11586142973004288%)\n",
      "For year 2010: Fetching 20 samples out of 11089 (~0.18035891423933628%)\n",
      "For year 2011: Fetching 20 samples out of 13957 (~0.14329727018700295%)\n",
      "For year 2012: Fetching 20 samples out of 16356 (~0.12227928588897041%)\n",
      "For year 2013: Fetching 20 samples out of 10240 (~0.1953125%)\n",
      "For year 2014: Fetching 20 samples out of 12404 (~0.16123831022250887%)\n",
      "For year 2015: Fetching 20 samples out of 18052 (~0.11079104808331487%)\n",
      "For year 2016: Fetching 20 samples out of 8165 (~0.2449479485609308%)\n",
      "For year 2017: Fetching 20 samples out of 7270 (~0.2751031636863824%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD4CAYAAADmWv3KAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABSIElEQVR4nO29eZgcZ3Xv/31733v2RTOjGW0jWbLkRfJug7ExNiZ4IQSTYHASEkPCJYbgJDjcJPDjhjiGLIBzY8y1L9xgTJzYgI1ZDAbvsmXJkixrt6QZzYxmn+l9735/f1S91dXd1dtM9/Si83kePTOqXqampvrUqe97zvcwzjkIgiCI5kNX6x0gCIIgqgMFeIIgiCaFAjxBEESTQgGeIAiiSaEATxAE0aQYVvKHdXR08KGhoZX8kQRBEA3Pnj175jjnneW+bkUD/NDQEHbv3r2SP5IgCKLhYYyNLuV1JNEQBEE0KRTgCYIgmhQK8ARBEE3KimrwBEHUD/F4HOPj44hEIrXeFULGYrGgv78fRqOxIu9HAZ4gzlLGx8fhdDoxNDQExlitd+esh3OO+fl5jI+PY82aNRV5T5JoCOIsJRKJoL29nYJ7ncAYQ3t7e0XvqCjAE8RZDAX3+qLSfw8K8MSS4JzjiTfGEYgmar0rBEHkgQI8sSROL4Tw54/tx0/fnKz1rhAEkQcK8MSSWAzF5a+xGu8JQVSXr3zlK0Wf43A4VmBPyocCPLEkfGEpwHvkrwTRrJQS4OsVKpMkloRXDuxeCvBNwZeeOohDZ3wVfc/Nq1z4u/dvKfq8W265BWNjY4hEIrjrrruQTCZx6tQp3HfffQCA73znO9izZw+++c1v4stf/jIeeeQRDAwMoKOjA9u3b8fdd9+t+b7f+MY38MADD8BgMGDz5s34wQ9+gGAwiE9/+tM4cOAAEokEvvjFL+Lmm2/Gd77zHTz55JMIhUI4ceIEbr31Vtx33334/Oc/j3A4jPPPPx9btmzBI488UvT3+epXv4rHHnsM0WgUt956K770pS9hZGQE733ve3HllVfilVdeQV9fH3784x/DarWWd1DLhAI8sSR8EQrwRGV4+OGH0dbWhnA4jIsuugjPPvssrrjiCiXA/+d//ie+8IUvYPfu3Xj88cexd+9eJBIJXHjhhdi+fXve97333ntx6tQpmM1meDweAMDf//3f45prrsHDDz8Mj8eDiy++GO9+97sBAPv27cPevXthNpuxceNGfPrTn8a9996L+++/H/v27Svpd3nmmWdw/Phx7Nq1C5xz3HTTTXjhhRewevVqHD9+HI8++ii+/e1v40Mf+hAef/xx3H777cs6dsWgAE8sCRHYfRTgm4JSMu1q8Y1vfAM//OEPAQBjY2M4deoU1q5di1dffRUbNmzA0aNHccUVV+DrX/86br75ZiXrff/731/wfbdt24aPfOQjuOWWW3DLLbcAkALwk08+ia997WsApF6A06dPAwCuvfZauN1uAMDmzZsxOjqKgYGBsn6XZ555Bs888wwuuOACAEAgEMDx48exevVqrFmzBueffz4AYPv27RgZGSnrvZcCBXhiSfjCUnmkJ0QBnlg6zz33HH71q19h586dsNlsuPrqqxGJRHDbbbfhsccew6ZNm3DrrbeCMQbOeVnv/fTTT+OFF17Ak08+iS9/+cs4ePAgOOd4/PHHsXHjxoznvvbaazCbzcr/9Xo9EonyS4A557jnnnvwiU98ImP7yMhIzvuHw+Gy379caJGVWBIk0RCVwOv1orW1FTabDUeOHMGrr74KAPjABz6AH/3oR3j00Udx2223AQCuvPJKPPXUU4hEIggEAnj66afzvm8qlcLY2Bje9a534b777oPH40EgEMD111+Pb37zm8rFYu/evUX30Wg0Ih4v7Ty//vrr8fDDDyMQCAAAJiYmMDMzU9JrqwFl8MSSoEVWohLccMMNeOCBB7Bt2zZs3LgRl156KQCgtbUVmzdvxqFDh3DxxRcDAC666CLcdNNNOO+88zA4OIgdO3Yokko2yWQSt99+O7xeLzjn+OxnP4uWlhb8zd/8DT7zmc9g27Zt4JxjaGgIP/nJTwru45133olt27bhwgsvLLrI+p73vAeHDx/GZZddBkAqn/ze974HvV5f7qGpCKzc257lsGPHDk4TnZqDjz70Gl48PgfGgBN/fyN0Omp5bzQOHz6Mc845p9a7URaBQAAOhwOhUAjveMc78OCDD+LCCy+s9W5VFK2/C2NsD+d8R7nvRRk8sSTE4irngD+SgNtWGXtTgijEnXfeiUOHDiESieCOO+5ouuBeaSjAE0vCF0kvQHnDcQrwxIrw/e9/P2fbpz71Kbz88ssZ2+666y78wR/8QUV/9vz8PK699tqc7c8++yza29sr+rMqBQV4Ykn4wnF0u8yY9kVJh29gOOcN7yj5b//2byvyc9rb20uuh18qlZbMqYqGKBvOObzhOAZabQBoobVRsVgsmJ+fr3hQIZaGGPhhsVgq9p6UwRNlE44nkUhxrG6zYffoIgX4BqW/vx/j4+OYnZ2t9a4QMmJkX6WgAE+UjWhyGmiTMnhPmBwlGxGj0Vix0XBEfUISDVE2ImMXAZ4yeIKoT4oGeMbYAGPsN4yxw4yxg4yxu7Iev5sxxhljHdXbTaKeEF2s3S4zTHodBXiCqFNKkWgSAD7HOX+DMeYEsIcx9kvO+SHG2ACA6wCcrupeEnWFV/afcVuNcFmNZDhGEHVK0Qyecz7JOX9D/t4P4DCAPvnhfwHwlwBoGf4sQmTwLosRLTYjGY4RRJ1SlgbPGBsCcAGA1xhjNwGY4JzvL/KaOxljuxlju2m1vjkQGbvLaoTbaiSJhiDqlJIDPGPMAeBxAJ+BJNt8AcDfFnsd5/xBzvkOzvmOzs7Ope4nUUd45Soal8VAAZ4g6piSAjxjzAgpuD/COX8CwDoAawDsZ4yNAOgH8AZjrKdaO0rUD75IHHaTHga9jgI8QdQxRRdZmdTH/BCAw5zzfwYAzvkBAF2q54wA2ME5n6vSfhJ1hC8ch9sqec+4rUZl0ZUgiPqilAz+CgAfBXANY2yf/O/GKu8XUcd4w3G4VAHeH00gmaJ1doKoN4pm8JzzlwAUdCPinA9VaoeI+scXicNlSQd4QMrqW+2mWu4WQRBZUCcrUTbecCIjg5e2kUxDEPUGBXiibHzhOFxW6eavxUYBniDqFQrwRNloSTQeCvAEUXdQgCfKIpni0og+kmgIou6hAE+URUAe1UcaPEHUPxTgibIQgdxlkTR4l6qKhiCI+oICPFEWwmhMZO4Wox4Wow6eEA39IIh6gwI8URZqozEB2RUQRH1CAZ4oCxHI3RTgCaLuoQBPlIXiBU8BniDqHgrwRFn4VFbBArfVREM/CKIOoQBPlIU3HIeOAQ6zOsDT2D6CqEcowBNl4YtITpKSi7QESTQEUZ9QgCfKwhtO2xQI3FYjgrEk4slUjfaKIAgtKMATZaEe9iEgwzGCqE8owBNl4YskFCdJAdkVEER9QgGeKIt8Eo14jCCI+oECPFEWWhKNiwI8QdQlFOCJslDPYxUIDZ5KJQmivqAAT5RMJJ5ENJHKyeCVoR/U7EQQdQUFeKJk/JHcLlaANHiCqFcowBMl49VwkgQAo14Hm0lPAZ4g6gwK8ETJaBmNCdxWI0k0BFFnUIAnSkbxgrfkBvgOhxnzwehK75JCJJ7EH333dRyZ8tVsHwii3qAAT5RM2gvekPNYt8uMaV/tAvz4Ygi/OjyD/9g5WrN9IIh6gwI8UTJCgnFbTTmPdbksmPFFVnqXFMQC8DOHppFK8ZrtB0EAwI/2TuCarz1X83ORAjxRMjP+CPQ6hja7RoB3mjEfjCGWqI3hWDCaBADM+qPYO7ZYk30gCMGxaT9OzgURrdHnQUABniiZKW8UXU4z9DqW81i3ywIAmA3URqYJRNMLvL84OF2TfSAIgQjskXiypvtRNMAzxgYYY79hjB1mjB1kjN0lb/8qY+wIY+xNxtgPGWMtVd9boqbM+CPokgN5Nt0us/ScGsk0QqJZ12nHz9+aAuck0xC1I5qQAnskUecBHkACwOc45+cAuBTApxhjmwH8EsC5nPNtAI4BuKd6u0nUA9O+CHrkQJ5Nl9MiP6c2GXwwKgX4397ej9MLIRyZ8tdkPwgCACLxVMbXWlE0wHPOJznnb8jf+wEcBtDHOX+Gc56Qn/YqgP7q7SZRD0x5I4oUk43YPuOvTQYfkAP8rRf0gTHg529N1WQ/CAJoIIlGDWNsCMAFAF7LeugPAfwsz2vuZIztZoztnp2dXdJOErUnHEvCF0nkDfDtdhP0OobpWkk00QRMBh163VZcNNiGXxykAE/Ujqgc2MONEuAZYw4AjwP4DOfcp9r+BUgyziNar+OcP8g538E539HZ2bnc/SVqhAjc+QK8TsfQ5axdLXwwmoBTHgR+/bk9ODLlx+h8sCb7QhCRRsrgGWNGSMH9Ec75E6rtdwD4LQAf4bSq1dSkA7y2Bg9ItfC1yuADkQTscoB/z+ZuAKAsnqgZIoOP1rsGzxhjAB4CcJhz/s+q7TcA+CsAN3HOQ9XbRaIemPZLmXlPngwekGrhZ2qUwQeiCTjkAD/QZsNwtwOvnJivyb4QRCNp8FcA+CiAaxhj++R/NwK4H4ATwC/lbQ9Uc0eJ2jLtlTLzfGWSgGxXUOVF1vt/fRyff/zNnO2BaAIOlY3xuavcODJJlTREbYjUiQafayqSBef8JQC5nS3ATyu/O0S9Mu2LwGLU5XjBq+l2WuAJxRFNJGE26KuyH88emdG8SwhEE0qpJgBs6nXiib0TWAzG0KrReUucnfzHq6M44wnjr27YVNWfE0s0SJkkQQDAlC+CHpcFkmKnjVIquQSZ5mcHJnHr/3656C3t6fmQ5mjAQCQt0QDAOb0uAMDhSXKXJNI8c3AKT785WfWf00gSDUFgxhctKM8AQJfoZl2CTPP8sVnsPe0p+OELRBOYD8bgjyaQzDJxCkSTGRKNCPCHigT4XxycwnyN7BWIlccbjsMfqf7cAhHYG6GTlSAw7Y8UXGAF0hn8UkolT81JJY3f3TmS12bg9Hx6LT87iw9E4xkZfIfDjE6nGYcL6PDBaAKf/N4e/OD1sbL3l2hMFkMx+COJqltZREmiIRoFzrncxZq/RBJQB/jyM/iR+SAcZgPeHPdi35hH8zmnF9J17T5VFpZIphCJpzICPCBl8YUkmkA0Ac6B+UCs7P09G4glUrjsH57FU/vP1HpXKoYnFEcixaseeBUvGpJoiHrHF04gmkjlbXIStNqMMOpZ2Rl8MJrAtC+Kj102CIfZgO++MqL5vNML6QxePf9VWAXnBPgeJ96eCSCe1P4wC3uDxRAFeC2mfRFMeiM4Np17F3RiNoD/+/KpGuzV0kkkU4opXTVlmmSKI56U7hAowBN1z1SRLlYBYwxdzvIHf4zIHadbVrnxwe39ePrAJGb9uReJ0XntAO+XrYK1MvhYMoWTs9odrcKgbCFIAV4L8XfXWtR+4o1xfOmpQwjFEjmP1Svqc8ZXxQAfVenuFOCJuqeYTYGaLpcZMxrBuRAjc1LgHuqw4WOXDSKe5Hh01+mc551eCMFuksovNTN4S26AB/JX0lAGXxjxd/dFcoO4LyyOXeMMWvdkBPjqXZjU3aukwRN1j/igF1tkBaRa+HI1eJHBD7XbsbbTgXcMd+KR10ZzpJXTCyFsWeUGkBngA3ky+LWddpj0urwBXlwYKMBrM+XNn8GL47/YQHc/HtXFyF/FAB+hDJ5oJETA7iqyyAqI4dvlBfhTc0F0u8yKl8zHLh3EtC+Kl96eU56TSKYwsRjGuX25AV58WO1ZAd6o12FDtyNvqaSQaBaDjZOFriTpDD73+IhtjXRx9Kj2tZoavDqDr3UnKwV4oijTvijcViMsxuLdqV0uC3yRBMKx0k/sU3NBDLXblf9fsb4DBh3D7pEFZdukN4JEimNjjwMmvU5TonFqdNlu6nHlLZUUEk0gmsjQTQkJsVgu5Bg1IqtvpPWLlcrg1XNY695sjCBEF2spLGXwx8hcEGs60gHeatJj8yoX3hj1KNvEAuvqNjtcVmOGbCAkmuwMHgDO6XViLhDVXLQVGTyQ+eEnJKYKZPDiAttIx82TcddXmf2e8UeQyJIS1bIMNToRdc+ML1KSPAOk7YRLLZX0ReKYD8YwpArwAHDh6lbsG/MoH55RuQZ+sN0Gt9WgKdFka/AAsFleaD0ylSvTBFV3GY2Uia4U0wWqaMQiZSMdN08oBh0DdKwyGfyUN4Kr/vE3+M/dmY1yIoN3WgykwRP1z1Iy+FJ1+BG5g3VNVoDfPtiKcDypzFY9vRCCSa9Dt8sCt9VYUh08ULiSRp3BN5KWvBKI5jZAuhBmZ6npDL5xjpsnFIfbaoTDbNC8aJXL918bRTSRUpxWBULua7EZSYMn6ptkimPWHy2pRBKQqmiA0gP8qQIBHgD2jC4CkGwK+tus0OtYToAPROOwGvXQ63KN0FrtJvS4LJo6fEaAp4XWDERzW3+rFUBmxhuJJxW3xIUGk2habCY4LcZlZ/CxRArf3yVl7tlBXJRGuq1GKpMk6pv5QBQpXniSkxqX1QCTQado3idmA/j7pw8pC5rZnJoLgjFgdZstY/uqFit63RYlwI/OhzAoP8dtNWYs/GV7wWezqdepmcEHVGP+FhooE10JhP4+3O0EkKnDq79vrAw+BrfVCKfFsOw6+J8fnMKcbFIXyiooEBm8FOApgyfqmFK7WAWMMaVU8uiUH7d9aye+/eKpvPYDI3NBrHJbNSt0LhxsxZ7RRXDOMbYQUi4CuRl8UgnUWgy12zG+GM7ZHowm0CdnqI1Uz70SiL/7hm4HgMxKGrW80VgafBytNiNcFuOyF1n/Y+cIBtttWOW25FSMicqZFquJqmiI+kYslpYa4AFJptk35sGHH9wJvY7h/IEWPPzSKc3SyVPzIQx12DTeBdi+uhUTnjCOTPnhjyawWi6ldFuN8EXiSMmWwYFIXLOCRuC2GhGIJnJ05GA0iRabEU6zgTT4LISuvKErN4P3ysG+02luqAujJxyTJRrDsiSaw5M+vD6yiNsvGYTdbMjJ4EXljNtmRCyZyrG2XkkowBMFEZlcj7uMAO+yYGQ+BJvJgMc+cRnuee8mzAdjeCyr2oBzjlOzgRz9XSB0+B/unQCQlnFcViM4B/yqOnatBVaBy2pUnqdGvK7VbmqoQLUSiDWUDV0ig1dJNPL3g222xrIqCEqLrC6rUfEvWgr/b+cozAYdfmdHP2wmPUJx7QzeLZ93tZRpKMATBZnxRaBjQHsZY+8uHGzFhi4H/vMTl2Kw3Y6L17Rhx2ArHnzhZIb9wGIoDl8kkdHkpGbzKhcsRh1+vE8K8IPtaYkGSAea7GEf2Ygxg9kNO8FYAjaTFOAbabFwJZjyRdBmN6HDKa29aGnwg+12hOPJmuvMpRBPpuCPJtC6zAzeG47jR3sncPP5q9BiM8Fq0iOSo8ELiYYCPFHnTPsi6HCYYdCXfqp8/Mo1+OWfvxP9rVJAZozhU+9ajwlPGD/el/YWz1dBIzDqddjW16LIRAOt6QweSJfqZQ/7yEY8P7thJxhNwm42oNVmpAw+i2lfBN0ui+bFURz3NbK01gjylkgGWmxGJcAvZejHzhPzCMeT+J0dAwAAq1GPUDzzYiECujjvIona6fAU4ImCHJr05Q3A5XD1xk6c0+vCvz/3tqKdixr47CYnNRfKMk2X0wyr7CTpzgrwwWiyYIDPzvgFwWgCDrMebTZTQy0WrgTTvqjkD2QyQMeyMnj5OIo1kUY4dp6MAG9EMsVztPOS3ke+mPW1SIvzNlOuBh9NpGDS62CTz1fK4Im6ZCEYw8EzPlyxvmPZ78UYw59evQ4nZoP44lMHMbYQwsh8EDqWzsy1EDq8kGeA3AAfiCQKLrK6LLkZfDLFEY7LGbzd1FDlfiuBaG7T6RiclkxrCF8kAYtRh25ZvmkEuwLx9xWLrMDSulnFhUKcg1aTPreKJpGE2aBTKsNqGeDzfyqIs55XTsyBc+DKDcsP8ABw49ZePHt4Gt97dRT/8eooHGYDBtpsMBny5xkXrm4BIHnQCNQBPppIIpZMaRqNCVzWXJkhGEvbGxj1OgRjkpZciqFasxNPpjAXSDe3uayZdeNeuSO0VV6XaYgMXr4ItViN8Ial88cfiZdVPABI55xBx5Ts3GbS51bRxFMwG/V1EeApgyfy8vLbc3BaDNgmW/QuF72O4V8/fAFe/vw1+LNrNsBhNuCyte0FX9PuMONPr16H397ep2xTB/hCNgUCLQ1edLFKGrwUqBohE10JZv1RcJ4ujXXlZPBxuCxG5bg1ggYv/ratqgx+Kc1O3nAcLTYjGJO6pq0mfU4nq5LBy4lLLbtZKYMnNOGc48Xjc7hsbXtZC6yl0Ou24rPXDeOz1w2X9Py/vGFTxv9tJj0MOgZvOI5AHi94NQ6TAYxlavDqAG+ULQ4WgrGyM7pmRBnw4pYkGFdWa783LGXwLTbpwtkINg/iIuSWG52ApTlKekNxJWEApEXWWEKqdRdWGdFECmZjfUg0lMETmpxeCGF8MYyrKiTPVBLGmGxXEM87j1WNTsfgNGfKDAEl89crUkMjZKIrQfaIRkmiycrgrUYY9To4LY3RJOYNx6FjgNNsUCqDlqLBi4ubQEg16tm00XgSFoNaoqEqGqLOePG4NE2pEgus1UDYFRQa9qEm20NeZPA2kwFtFOAzEC6SPfkkmnBCCZJtdlNDHDdPSDIaE4vGQOEA/0/PHFX6L9R4w3Glvh0ArCbpOKgXWkUGb5UDfC0dJSnAn0WEY8mSJy29/PYc+lqsFSmRrAYuOcAXGvaR8XyLMSMLFV2tDpUGT7XwEtP+KIx6phwXl9WYuciqymJbGqTEdDEUUwJzuopGW6IJRhP49+dO4Kn9kzmPecKxzAxeI4hH4ym5ikZo8HUc4BljA4yx3zDGDjPGDjLG7pK3tzHGfskYOy5/ba3+7hLL4U8f2YO7frC36POSKY5XTszjivXtymJSvaFINAWGfahxWQ0ZVTTiltpuNiha8kIDaMkrwbQ3gi6nVCIJSBdH4eWTSnH4I2kdus1mbIgM3huOwy3/nW0myVpaa1IVAOweXUQixbEQzB1aIyqIBGmJJnOKk8Woh7lBNPgEgM9xzs8BcCmATzHGNgP4PIBnOecbADwr/5+oYw5M+LBrZKFoB99bE154w3FcuaFzhfasfFzlSjQ5Gbz0OrtZ31Ba8kow5YtkLDaLYxuIJhCIJZDi6Uomycen/i+MkpOkdEfCGIPDnN+uYOeJeQDI8dlJpjj80QTctrRth0UjwGdn8NF67mTlnE9yzt+Qv/cDOAygD8DNAL4rP+27AG6p0j4SFSAQTWAuEIUnFMeEJ9c6V81Lb8v6+7rCJYy1RIztK1miyaPBi8y/Vlry6fkQfvfBV+uq0WrKF8nw/1fKTMMJ5RiKSpRWW2No8GqJBkBBP5qdJ6Tzfz6QmcH7I3Fw1cUNUEk0GRp8EmaDHia9DozVfwavwBgbAnABgNcAdHPOJwHpIgCgK89r7mSM7WaM7Z6dnV3m7hJLZXQ+qHx/8Ezu8As1Lx6fxeZeF9odpQ35qAVuWRf2RxJgLP1By4eUwasanaIJ6BiUhbDWGmnJr56cx86T83h9ZHHFf3Y+ZnyZE7wUP5pIXJG5RPNYm92EUKz+Dce8obREA0Ce6qQ1azaOAxNeWIw6+CKJDHM8b1YXKyAt0gOZVTSReAoWow6MMViNuZ2uK0nJAZ4x5gDwOIDPcM4LRwgVnPMHOec7OOc7Ojvr95a/2RmdDynfFwrwnHPsG/PgkrVtK7FbS8ZtlfxEpn0ROEwGRS/Oh8tqyPCED0QTsJsMyhpDrTJ4Ycd8VGMoeC0IRBMIRBMZM3jTGXxcCXIuZZFV+lrPTWJqJ0mBK89Up9dPLSDFgWs2Sfmq+pwQv3tmFY3GIqucwQOAxahX/OFrQUkBnjFmhBTcH+GcPyFvnmaM9cqP9wKYqc4uVpf7f30cn3tsf613o+qMyBn8KrcFh8548z7PE4ojEk8pTpD1isiiJjzhovIMkJYURPVMMJrpX9Nqq42WrAT46cCK/2wtlBJJtzqDT3cCi3UMsa2tAbpZvSqjMUG+uaw7T8zDZNDh2k3dADJtGMRFTH0nIBZZs8skhf5uMejquw6eSSnOQwAOc87/WfXQkwDukL+/A8CPK7971ee1Uwt4/lhDXpvKYnQuhA6HGTuG2gpm8MqAjzImONUCEeDPeCIFveAFah0ZkBwobea0rNNqM9ZEohGTk45N5Q4FrwWiyanLqc7g014+2TKF0iS2QsfuW8+fUFxIS0UJzKrM22UxaEo0O0/O48LVLehtkX5/9TmhJdEIiS+UXQevzuDrXIO/AsBHAVzDGNsn/7sRwL0ArmOMHQdwnfz/hsMXjmMuEFMG5TYrI/NBDLXbcG6fC5PeSN5gNpXVpl6vuMrO4KXnpD3kM6dAtdpNNRleIY73idkAYjWsthCIO72BNquyTe3l48uSaITssRJDyz2hGP7hZ0fwvVdHcx776YFJ3PJvL2vq3d6wtG9qiUZrkdUTiuHQpA+Xr+tQmt+0AnwhiSYuj+gzyz40UoCv4wyec/4S55xxzrdxzs+X//2Ucz7POb+Wc75B/rqwEjtcaYQON+PLrXltJkbnQxhst2PLKsk47GAemUZklOXMYK0FIouKJVIFB24Lsg3HQjFJgxfUqpt12heB02JAIsWV4FpLjk8HYDfpFb9zINPLxxeOg8kt/wDQapf9aFZAgxeZuNYd6M/fmsK+MQ++v+t0zmNCesuVaOIZJcOvnlwA58Bl69rT54NGgFd70ZgNOuhYWqIRJZHCpsBi1NV9Bt/UiD+ayKSakXAsiSlfBEPtNmxZ5QKQf6F1SuMWvR5R3yYXa3ICVDqyasxftgYPrKz1bSyRwlwghivWSXYQR+tApjk65ceGbmdGg5vay8cXScBpTi9qt1hXTqIRF9+DZ7w5vRwHJqSE5YHnT+QEVGXYhzUzg09xIKjK+F89OQ+rUY/z+luU82E+K8Crfd4BqaZePfQjKv9ss1GdwVOArwmcc+UDP+lt3gB/ekGqoBnssKPFZkJfizVvgJdG9JkKerTXA+oAX5JEY02X+gHpaU6CdMa2cgutM37pnLtifTv0OlYXAf74jB/D3Y6c7aKPwBfOdFM0GXRwmpffJPb0m5PYN+Yp+BwRqH2RBMYX070cvkgcp+aCuGpDB2b9UTyalcUrwz7smRk8kGlXsPPEPHYMtcJk0MGo18FlMWRm8KF4xl2AQLIMlpQAMZ4vQ6Kp9yqaZiUUSyIhj4+bbuIAL279h+SpSJtXufJKNFPeSENY5jrM0ig5oHgXK6C1yJpdRSPbFaygRCMWNPvbbFjbYcfR6doG+PlAFHOBGIa7nTmPiU7gbDdFQHSzLv24jcwF8Wc/2It/f+7tgs/zqmSgtya8Od9//Mo1uGRNW04W7wnFoZfvQpTfx5rpKDkfiOLotB+XquYTtDvMORl89u8OyHNZszL4TImmjjX4Zkbdut7MGbxochqUpyJtWeXCqbmg0s2pZsoXrfsKGkC6NRZBuxSJxpE1W1RrkRXAsjpKnzk4hWu+9pxSilmMKa+07tPjsmC4x4ljNQ7wx+RSTc0AL3v5iGEfalptxmVp8P/6q2NIpjimiqyDqe8S1HegB8alAL+1z427rt2AaV8Uj+0eUx4XBmFq2Sk7gxd3DxcNpfs/WrN8drKNxgTqqU5RjQy+IRqdmhGvqnV9yle4fb+RGZkPodVmVOp3t6xyg3PgiEZzzbQvUvcLrALxYStFolHPFk0kU4gmUkoXIpCujFiqBp9IpvCVnx7GybkgDhXpFBaoS1I3djtxeiGU0RFZSTjn+MGu03kNtgBJngGAjT2FM3iR/Qpal9EkdnTKjx/vPwODjmGmyDqYWGTd0OXAW6o70AMTXvS1WNHuMOOyde24aKgV//s3J5TKOI+GtJI91Wn/mAc6Bpzb51Ke02Y3Yz6gzuAT2hm8Ka2zi6/qMslaVuid1QFe3K4bdExp8GhGRueDGGxP2/7mW2iNxJPSVKMGC/Cl1MED6dmiQZXRmMCg18FtNS5ZanjijQmMyN3CpXalTnnDMBl0aLEZMdztBOfA2zPVaXg6NOnD5584gJ+/NZX3OUen/HBZDOhy5pbIpjX43CC3HJuHf3rmKBwmA267aAAz/iiSqfxGeJ5QDC6LAdv6WzIz+AkvtspjJRlj+Oy7hzHli+CuR/chlkhJAT5rn7OHfuwb92K425lx0W+zZ/ZG+MJxuFULtQLNDF5pdKrzMslmRmTwazvtTR3gR+ZCiv4OAL1uC1ptRhycyAxEolS0uwE0eCAd4EspkwTSgysCMW2L4Ta7CQtLkBpiiRS+/uxxbOt3w2Ux4EiJi6VCDmOMKVlzqa8tFyH/FBpycXw6gOGsChqB8PLRlmhMS7Iq2D/mwTOHpvFHV63Fph4nkimOeQ2LXoEnLA3t2LLKhVl/FDO+CLyhOEbnQ9jan54bfPn6Dvzd+zfj5wen8Mnv7cGMP4IWW2Zgdqqqqjjn2D/mwfkDLRnPabObsRiKKRU7npC2RGM1GjQkGiqTrDmigma424npItlDoxJNJHHGG87I4BljOLfPjYOTmQutjdLFKnCVIdEAaZkhpJrHqqbTYcbTb57Bzfe/hH/8+ZGiVR2Cx3aPYcITxp9fN4yNZWjp06oF7dVtNpgNuqp1tB6dku4MQnnWBzjnODbjx7CGPAOkvXxCsWROkGuzS37x5TZqfe2Zo2izm/Dxq9agSz7npr0FAnwojlabEef2iV4OnyLVbM0aDP8HV6zBV27dit8cncGx6UBeicYfSWB0PgRvOJ4T4NvtJsSTkkVwPJlCUON3B+QqGjlpSEs0aQ0+keIZpmUryVkd4EUGv7Fbzh4C+U+uhWBMc1Gy3hlbCINzYKgj01tm8yoXjk0FMk68dBdrYwR4dxmLrEB6oTAQ1c7g/+G3t+J/XLMBRr0O337hJD70wM6iH8xIPIn7f/02tg+24p3DnRjuduLIlL+o5z4g+67LgU2vY9jQ7ahaJc1x+X0DeTT+Wb9kJT3clVsiCSAja3dlBTmRHYsF6rGFUNGsdc/oIl48PodPvnMtHGaDchymC+jwnlAMbpsJ5/RKF6GDZ7xK/Xt2gAeA37tkNb76wfOgY0BnluxkNUpDP/yROPaPewAA52UFeLUNg0/Dz0ZgM+ZKNKKKxlrjoR9ndYAXC04b5KqBQpU0H33oNXzpqYMrsl+VRKmgUWXwALC514VYMoUTs2nNt1G6WAWKRFOqBi9n8GkNPvN16zod+PPrhvHff3I5/ua3NiOWTGUsxGvx6K7TmPJF8LnrhsEYw6YeJ/yRRNGqLM55zmCNjd2uqlXSiAtHviRFqaDJm8GrA3yutAUA/7VnHL/zwCu46r7f4OvPHi+4Pw88fwItNiNuv3QQQPqcK9Rw6AlLGbzTYsRQuw1vTfhwYNyL/larEoyz+eD2fvzk01fhT9+5PmM7Y0yxK9g35oHVqMeGrItbuz3d7CRq8PNm8PGsRiclgxdj+yiDX3G84TicZgP6W6W27HwfSs45Ts4GsWe0fjy7S0Us/A1pBHgAGRUfU74IrEa9sgBV75RTRQOkFwoDikST30O+FBvcRDKFb79wEhcPteFyeTj5xh7puBZrWvKE4oglUhkX0409Dkz7ohUf/hGMphuDxMUtG3EB0CqRBJBxTmgtsgLAV39xVBkWsr+AvPX2jB+/PDSNj102pCxqdjhM0DEUrKRRL5ZukSXGNyc82Nafm72r2bzKleEAmf6dJLuC/WMebO1zw6DPDIfqDF7LaExgM6VLISNZi6y1Htt3Vgd4XzgBl9WofMjy3R4GY0mE40mcnAtWrYytWozOB+G0GJRGHsGaDjtMBh0OT2YG+B63pW7nsGYz3O2A02LIuf3Oh8tiRDCWVG63C0k72bKDFr84OI0z3gj++B1rlW0bu0tbLNVa7xDBtdIdrcdVlTn5Mvjj03602U3oyDPkJSODz1pkvWB1C+58x1o8dMcOPHf3u3D1cFfB3+Fbz5+ExajDHZcNKtsMeh06HOa8GXwyxeGLxJW/y5ZVLowthDG2EFY0+XJxWgxYCMXx1hkfzhvIfQ91Bq8EeC2JxiTp7LFESqPRSfpaq1LJszrAe+W263a7CUY9y5vBi6yC8/rwCymHkfkQhtrtOUHboNdhU48Th1QBftqbOaqt3nnXxi7s/9v3lKXBA+k7NXVJXDatJWTwD710EoPtNmU4BCAFgB6XpajUouXaKSpp1BfdSiAWbjscJgTzJCjHprUtCgTqoJ6dxVqMevz1jefg2nO6oddJFUHzwRhm/blrWlPeCH60bwIf2jGQMzGsx23J2+wkVbuk76zOXZUOyNv6WvLudyGcFgP2ji4ilkjl6O9AVgavYTkssKjG9uU0OhlIoqkZUsmXZJzU7bLkzeBnVCfqoQp/+KqNVAOvPbzjnB4XDk+mFwTVi36NAGOs6CQnNSJITXoluaJgBi/XO3vyaPB7Ty/ijdMe/P7lQ9Bn7cPGHmfRDF5rvaPHZcHqNht+fbSyoy2PTfthNuiwqcelDBtXwzlXSiTzodbdsxdZs9nUk/9O5OGXTyHFgT++am3OY11OS16JRjRStSjNeumGJHVzUjk4LUb45Tua8/pbch63m/QwGXRYKCrRSMcmHE8iGk+CMcAkyz1aE59WkrM7wKuMk3pcFuWDn406E9HKrkqpmKgF4VgS44vh/AG+14mFYAwz/ig459IszgapoFkK4m99xhuBjqUXwLQQxlT5JJqHXx6B02zA7+wYyHlsU48TJ2YCBStwtFw7GWN479YevPL2XEV1+KPTfmzodsBlNWhKNJPeCPzRhFJsoEUhiSYbsVCbXRHkDcfx/ddO431bezHQlntO9rjzSzSKI6Qs0bQ7zOh1SxfE7Br3UhGL8+12k7IOp4Yxhna7KVOiyaPBA5IFtTTsQ6fcMVtIg68dPpV5UI/bkrfZSWTww92OnDb0PaML2PbFZzRf+5M3z+Dnb01WeK9L50f7JpBMcbxzWHMeOjbLt7mHzviwEIwhlkw1VAZfLmKhcFIeElJorcFpNkCvY5oSzaQ3jJ8emMRtFw1o3gVs7HEilkwVnDyUz7XzfVt7kUhx/PLQdKm/VlGOTfsx3OWE3WTQrIMXgXhjgQAvPOFNel3BCyMAdDjM6HCYcjp6n9w3gUA0gTvfkZu9A0C30yKPjNQY2hHKHbbx8SvX4A+vGCq4L4UQF6rzBlryngvSKEcpwNtNehj1ub+7VQnwyYxpToDUyQqQRFMTfJGE8kfudVsw5YtoZuOz/iiMeobL13XgyJQ/oyHqyX1n4I8mlHJENQ++cBLfeuFk9X6BAnDO8d1XRnBOrwsXDbVqPmeTXE98aNLXcE1OS0FkoVPeSFHdnjEmWRdoZNL/b+coOOe44/Ihzdcqi6UFdPgpr7bnz9Y+N/parPhZAUuBcvCG4pj2RTHc44TdbNA0QjuuVNDk1+CFJ7zLWvjCKNjY48yRaF5+ex79rda8i6Li7lFr+I74O6inMv3RVWvx+1esKbov+RAZvJY8I2h3SBm8J6TtJAmo5rLKE8HMqot2ukySMvgVJZFMIRBN+2p0uyyIxLXrnmf8EXQ6zNjc60IollSCOeccv5H1Uq0J7b5wPMOsaCV59eQCjkz58fuXD+b9QLosRgy0WXFo0qesP5wNEo0/yyo4Hy02o6YG/1+7x3Hd5m5NmQEA1nc5ivq753PtZIzhxq09ePH4bEFjsFI5NpPOzu1mPYKxZE4SMzIfQpvdVFTqcFmNRfV3gVTTH0BKToZSKY6dJ+dx+br2vK9Rqtn8uXfD4k5Kq9FoqSgBXqOCRtBqk4zUvOE43HmOj3ouqzRwW5XBG0mDL8rzx2bxf16sbCYsPDnE4lGvO38t/Kw/ik6XBZvlhZ3Dk9KH5uRcUBmmoTXA1xdJFOyOrSbffWUELTYjbj6/r+DzpIVWX4Z1bbOiruUuKcBbjRke5ICUic0FothWIOuzGPUYarcVXGid9uX33X/v1l7EkxzPHl6+TCOqeUQGn0xxpdJD4A3nmnFp4bIYi+rvgo09DoTjSeXzcWjSB284jsvl6VVaiHNPS+70hGLSqMASf34pnNPrQq/bggtWa9/hArI/USAmy7na54yykBpLIprIzuDlMkkK8Pl57ugM/uWXxyq6mJm9aCI+bFqLPLP+KLqcZqzvcsCgYzgke7j85siM8hxfVqbHOYc/Epdq6FfYD3rCE8Yzh6Zw20UDGdmEFuf0St7wI/NBMI2W7mbCbkoPCXEUaHIStNhybXDn5At2h6Nwtrupx5U3gy/m2nl+fwt63RY8/ebyZZpjU344zAascluUGbTZC63+SALOEgL8VRs6cOX6/AFajdLwJV9gdp6YByDNO82HKNHVqmbzyOtl2RVLy+GqDZ3Yec+1eaUXQArw/mgCs4FoAYlGVNEkEImnlCYngDpZS6K/1YZgLLkkx7p8iNtfkZEoAV4je5jxR9HpNMNi1GN9V3qh9fljs4pLY7ZLXySeQjwpXZAKOeRVAzF1/qOXDhZ5ptTlxznwwrFZdDjMmotIzYLwhAcK18ALWmzGnHNuTpbc8jUECYYL+LsXc+3U6RhuOLcHLxyf1bwzLIdj0wFs6HaAMabctWR3s/rC8ZK6l++58Rzcff3Gkn7ucLcDjKVLJV85MYd1nfaCNhhuqxFmg047wGtY/q4EwoZhbCFUVIMPyRm8xZAr0ZAGXwBRwqSew7hcsjvTupxmMJYr0cSTKSwEY4pH9jm9Lhya9CEYTeC1kwu4bnM3LEZdjl6q/v/cCurwkXgSP9h1Gu8+pxv9rdoasRphWXBkyt/U8oxASHKlNEe1WE055YpzfpHBFw7womlJeLyoKWVB+8atvYglUvi16i5xKRyb9ivVMeKuJXuh1a9hAbxcbCYDVrfZcHTKj3gyhV2nFgrKM4C0/iD1o2gvsi61HHI5iG7WRIrn/fkZjU5ZGbxRr4Nex2o2l7UhAnxfiwjwoYq9pxj2IU5so16HToc5ZzaruCUX0sXmXhemfVE8/eYkYskUrt7YJfuMZ35o1JLNSurwvz4yg8VQHB+7bKik5/e3WhU/9UYxGVsO4u9dyIdG0GqTrA3UNriKRFNEyhKOh1rTnUpx7dy+uhVdTjOeObh0HX4uEMV8MKbUt4u7luy7Cl8kUbJhWzlIzpo+vDnuRTCWLLjAKuhxWTRlUm9Ye+B1tVGbmBWtooklEUkkM8okAambNRwjiSYvA3ImOuGpQgav+qP1uC2YzDq5RJOTaEgRC60PPH8CdpMeO4Za4bIa4Y9mZ/DpD9FKVtK8dnIeVqMel6xtK/5kSFnTOXIWr26bb1bKMSgTAUVdWSUCfHse90LB6jYbWm1G7D2da1BXimunTsewrb8lw+2zXI5l1beL31kzg6+C/LGpx4mR+RCeOyrdhagHWuej263dzboYimWUSK4U6r9zvmNk1Otg1DOE4lIGn90nYDXpKYMvhMtqgNNsqKhEo2jwqpXxHpcFU1ndrEIv7VRJNIBUQXPF+g6YDXo4LYbcDF4t0aygBr9rZBEXDraUpaWLbPOskGjkDN5Rggbv1jAcmwvE4LQYii5eM8awfbAVezQC/BlvuCTXzl63ZVnD4MX4vw1yfbtDQ4OPJpKIxFNVcRDdKE9penTXGDb3uvJa+qrpdpo1+1EK1aFXE/U+F1oDsMrDtbMbnQBpuhNp8AVgjKGv1VpRicYbjsOoZ0oNKyA3O2V9oGaUDF4K8G12E3rlW+t3ySZTwnZUTaZEszIZvDccx5EpX8Zk+FJIZ/C57drNhrigl5LBK4Zjqr/lbCCKziL6u+CC1a04ORvMmfP6xugitqxyFW0Y6m2xwBuOL9nBdHQ+BKtRr5y7QpZSV9GI4oBKlh8KhCfNXCBakjwDSHfRkXgq4w44kUzBH0nURKJpsRoh/kyFLjA2kwGhWCKn0QmQKmmiVEVTmP5Wa2Uz+LC0sKT+kHW7LfJQ5vTJNauxqCYC4tUbOwHIPuORXF0TkK7sK6XBvzG6CM6Bi8sM8BevaYPFqMswcGpWlAy+xEVWABkBes4fLbrAKtg+KNVX7x1LZ/GeUAxvTnhx5Ybi5Yar5AvuGc/SsvjR+RBWt9mUc1wpk4zlBvjsIR6VYLDdrphuXb6+tADfpWHdLSSyWkg0Yhg7UDjAW+XB29mNToC0CFu3GTxj7GHG2Axj7C3VtvMZY68yxvYxxnYzxi6u7m5KpZITi+GK1cJ7w7m3fGIoxsnZtO3AjD+CVpsxwzPktosG8PuXDynNUZJEk5nBi4x+qMO+YlU0u0YWYNCxgo0bWqztdODw/3eDcuFqZsqZ49qikcHPBaLocJYWaLb1u6HXsYxBMa+cmAfnKKmeXNwp5jPBK8bYQgirVUZz6TLJdIAX563TXPns2KjXYZ3c1VvqXaXW6L600djKZ/BAulSy0M+3ykE8u9FJPFbPnazfAXBD1rb7AHyJc34+gL+V/19V+lut8EcTOVr3UvFpNHcIj2kx5xGQu1izKiau39KDL960Rfm/JNEkMi4+vnACJr0OfS0WZWGu2rx+agHn9rmVzrpyaJQhH8tFaM22khqd5EXWUGbJa6kZvM1kwOZeF94Y9SjbXnp7Dg6zQdN/PJtVcvXY5BIyeM45Ti9IGbzAZNDBpNdlWAanM/jqBM/rt3Tj/dt6S5aARLOTWir1KFbBK5/BA+mF1sISjR7BaFJudGqgDJ5z/gKAhezNAES65wZwpsL7lYMolRyrkA6vlcEPtFnhshiUSe2ApMGrLV21cFoMiCVTGS3gvkgcLqsBHQ4z5oPVz+Aj8STeHPfi4jXlyTNnGyKQlSLROMwGGHRM6WaNJSSvolIDPCDJNPvGPEjI1sEvHZ/DpWvbS1oE73ZZwJi0KFsus/4owvFkjlW0zazP0PS1ig0qyWfePYx//fAFJT9fVBapZzB4NJwkVxIhDRW6SFlNekVK0tLgG62T9TMAvsoYGwPwNQD3VGyP8iCadiqlw/s1uvcYYzi3z423sjL4riI1zyJoqCtn/LJTZbvDhIVgTDFdqhb7xzyIJVNlL7CebWzscaLFZsRgHqMwNYyxDMMx0ZFcToC/YHULwvEkjkz5cXo+hNMLIVxZoh5tMshj7JZQSSM8YFZn/Z52U6ajpJASq7HIuhQsRj3cVmNWBl87DR4AulxmtNoK2yTYTHrlTiM7wJuNjVcm+ScAPss5HwDwWQAP5XsiY+xOWaffPTu79Ek1optVXQsfTSRx49dfxC8Olu/ZoZXBA5Jd65FJqfuOc64p0WQjLhRq+cgXjsNpkTL4ZIprulRWktdHpJusHYPl6e9nG1tWubHvb9+jLOYVw60yHJvzC5uC0gONWGh94/QiXnp7DgBw5YbOkl/f67bgzBIC/Oi8doB3mA1ZGrxo+KufQes9WdPVxB2U1jzUleBPrl6Pb310R8HnWI16LMrnSc4iq0GPyAr7UQmWGuDvAPCE/P1/Aci7yMo5f5BzvoNzvqOzs/QTO5sWmxF2kz6jVPLAuBeHJn14rswRZ5xzWULJPWG29LkRS6ZwbNoPbziOWDJVQoDPzeDF+4u5k9XW4XeNLGK421FSrTFROmrDsVK7WNX0tVjR5TRjz+giXn57Dj0uC9Z12kt+fa/bgsklNPiNLoSgY8ixq7CZ9Rl18L5IHIylK2zqgS6XOaeKRsegdFyvNH0t1qLSp9VkUBZScxZZTTpEEo0l0ZwB8E75+2sAHK/M7uQnXQufPtlfH5GqE44XGXCcjTACy5fBA8BbE16lRLJogJf1S7XhmCjD7JADbjUraZIpjjdGF0meqQKtKsOxWWFbUYZEIxqedo8s4uUTc7hyQ0dZC9q9buuSmp3GFkLodVtzJkY5zIacMkmn2VDWbNtq0+OyYMKTrpgTPjT1tI/Z2FSFDTmLrPXc6MQYexTATgAbGWPjjLGPA/hjAP/EGNsP4CsA7qzubkr0t9oyAvxuWZY4Nu0vq3xSyCVaBkuDbTY4zAa8NeFTNTkVW2SVM/hwlgZvNSgZfDUdJQ9P+hCIJmiBtQq4VYZjaavg8iwdtg+2YsIThicUx1Ul1L+rWdViQSCaKHv4x+h8MEeeAaRMPbtMsl70d8GOoVbMBWI4KPv41MpJshzUAd6Ss8gqBfhazG4upYrmdznnvZxzI+e8n3P+EOf8Jc75ds75eZzzSzjne1ZiZ/tbrZiQJZpUimP36CLMBh18kUTGqnsxxIdFK4PX6Ri2rHLhQDkZvPwBycjgI9IHp13Wa6vZzbrrlHShowy+8rSqFlnn/DHYTfqyy1DVfQnFHBWzUQbRlFkqeXohpDls3W42ZEk0iaqVSC6V92zugV7H8JM3pXnGtTIaKwdroQzeqEOKQ7EPX0kappMVkLQwXyQBbziO4zMBeMNxvP+8VQBQcDxaNkoGn6c0bGufG4cnfUp5WperNIlGXDjU/h6tNhN0LNNRctIbxnX//HzBoczlcPCMD11Os1I3TVSOFptR8fmWmpzKN2Q7t88Fk16HTT3OsgeqrGopv9kpGE1gLhDLaHIS2M36jCoaX6Q0L/iVpNVuwuXr2vHTA5PgnNfMKrgc1JYnuWWStRvb11ABXiwYTSyGlaqR37tkNYC0c14pCCklX+PCuX1uRBMp7DwxD7NBV3Rxx2rUQ69jSsmZunlEr2Nos5swp6qFf+XteRyfCWQ0VC2HsUXtbI1YPsJwzBuKSwG+THkGkMymPnn1OnzynevKfm1PgVGS+chXIglIGXwoW4OvM4kGAN63tRenF0I4eMbXeBKNRqMTUJuxfQ0W4NO+8LtHFtDpNOOCgRa02004rjFYIR+FNHgAytT3104uoMtlLrooxhiDS+UomTZwki4M7XazMigCgNJIVSmPmvGFUN4B0MTyUBuOSQF+aZnkn183jFsuKDwfV4tupxk6hrIqaUSJ5GBbbrWOw2xAPMkRleuyfeF41ZqclsN7tkgyzdMHJqUAX+8ZvKoKKV8GX4tmp4YK8H2qWvjXRxZx8VAbGGPY0O1QpseXQrEMfm2HHXaTHrFkqugCq0AyHItnvL+4gLQ7TBndrGLxaKECHa6xRAqTvojimU9UFrXhWDk2BZXCoNehy1leLfyYyOA17upEpil0+GpMc6oEbbJM89T+MwhEa+MkWQ62ghKNPJe1Bs1ODRXg2+0mWIw6vD6ygAlPGDuGpMWr4W4njk8HSl6l9oYzM+xsdDqmDPYotSTOaTEomXu6/VsEeLOSradSHIflAD9XgQB/xhMG56AMvkqIwDIfjGExtPIBHpBsg8vR4EcXgnBbjZoJjNpwLJXi8EcTdafBC963tVepmmut9wBfSKIxpCc+rTQNFeAZY+hvteFXh6UJMaJqZEO3E4FoouQsxxeJw27Sw1DAD0TINMUWWAXS2D6RwWeOA+xwmJQqmtMLIfjlRa6FClTWCL11oJUWWKuBCPAnZwPgvLwmp0qxqsxa+NH5/GsyytCPWAKBWAKcV89obLkImQZIr4XUKxZT8UXWWtTCN1SAByQdPpZIwW7SKwMFxEiyUhda89kUqBHOkkvJ4NP+HtKHqcNhhj8qDQMQ8ozbaqxIbbwwX6MMvjoI7VdMR+pcoga/HKRu1twpR/kYy3KRVKPO4LPXiuoNIdMAjZXBZ5dJWk1CoiENvijCVfLCwVYlAx+WR5KV2tEqLSwVPmHOX90i/bwSM2OXRaXBZ0s0cjfrQjCGg2e8MOgYLlnTVpHa+LGFMIx6dlYMzK4FdpMeRj3DcTnA10Ki6XFbEI4nS/IzSiRTGF8M5w/wciAKRJM5a0X1yG9t6wVQvBel1tiM6YtkdqPTYLsd935gKzZ0OVZ6t1Cfl+4CiFJJdVNPi82ETqcZx0qspPGWEODXdTrw35+8DNv6W0p6T5c1U6LRsfSHSe1H89YZH9Z3OdDrtuC1U9kuzOUzthhCX4u1oNMdsXQYY3BbTcrw61oEeNHfcMYTQYvNhFAsgW89fxJ/dNWanBLHSW8EiRTPK9GIDD4UTcAvZ5r1WCYp+OD2AfS32rCpp76H0YhGJ72O5Ui/HQ4zPnzx6lrsVuNl8OLEze7aHO52lCzR+GQr32LsGGrL8fLIh9NiQDCWRCKZUozGRHml6GadC0Rx6IwX5/a50e4wwxuOI55c3m0blUhWnxabUSlxq4UGnz3Z6f++PIKvP3scPz0wmfPctIuktqGZ0OAD0UQ6g6/DMkmBXsdwRQnTr2qNkGiy9fdaU197UwLXbe7Gtz66HZeuzQzwG7qkSppSfNd9JWjw5SIuGAFZ21TrmkLHPzzpx1wghi2rXMoYsOyBzOUythimAF9lhP5rMeqUu7KVRMngvREEown8nxdPAkib7ak5XaBEElCXSSbgj9aXF3wjIxZSsytoak3DBXijXofrt/TkNB8NdzsRjicz/OLz4QnFKp61iIDujyQUJ0mByOCfl22Nt6xyKw0zy3GZDEQTWAjGqAa+yrjlWvgOR/Gmt2rQ4TDDoGOY8obxyGujWAzFMdRuU7q51YwuBGHS65TZptkoi6yxZF16wTcqeh2D2aCjDL5abOyRFjCKyTS+SBzBWFK57a0UQtP3huOyv0c6wNtMBliNerxxWsq4zul1os0uZfXLaXYSDS0DbVQiWU1EqWQt9HdACh7dLgtOzQXx4AuncOX6DnzkkkGMzocw48ssn3x7OoCBtvxrMmaDDgYdQ1Al0VAGXxlsJj0F+GqxvkuUShZeaD0jZ/h9LZXNetVDP7IlGkDK4hMpjjUd9kyXyWWUSioBnjL4qtJa4wAPSKZjP39rCnOBKD59zXpcJFtDq2WacCwpec4X0KwZY/KA6AT80QQsRl3J60xEYWwmA0k01cJtNaLHZSmawU/InXHCpa9S5Eg0WRq/qKQRHbIddlFZs4wMXv5dSIOvLqIWvtNZu2abHrcVKQ5cPNSGS9a2Y8sqF6xGfYZM8/yxWUTiKVy/pafgeznMBqVMsp5LJBsNax1m8E0lvg33OIvaBisZfIU7P8WirS8c16zSEQ0yW+QA77IaYNAxLCwzg7eb9HXfBNLo1FqiAYBVsqT46WvXA5DWoi5Y3aLMAgCAZw5OwW01Fh38IhwldYzVbZNTI2I16mE21FcG31R/3Y3dDrx6ch6JZCqvDcG4JwyTXqdk0JVCBHRvOI5ANJGziNsu/zzRIcuYZCO8nGan8UWpRLIWC39nEy2qRdZa8aGLBtDpNGfILxcNteEbvz4OXyQOq1GPXx2exrs3dxe04ACkAC884evVpqARecdwB4xFjv1K01wBvseFWCKFkfkQ1ufpGjvjiaC3xVLx+Y4OORMSVTzZC1cdzswMHpBaseeXtchKJZIrQT1k8Os6HVjXmXlOX7ymDZwDb4wuwqCTJpsVk2cAaehHMJpAkqPufdYbib+4flOtdyGHpgrwwpvm6JS/QIAPK3YHlUSvY3CYDYr7XXbp2YcvWo21HQ5FiwekgLFUT3jOOU4vhBqiCaTR2drvxnvP7am7mbcXrG6BQcfw+sgCfGFpwfQdGzqLvs5uMmDOH0M8mSKTuianqQL8+i4HdAw4Ou3H+9Cr+ZyJxTCuLHPwcak4LQZlETf71negzZaTbbfZTRiXzcLKZT4YQziepBLJFcBlMeLfb99e693IwWYyYEufG7tOLeD0QgjvHO4saV6sw2xAMCaZ35FE09zUl2C0TCxGPYba7Tg65dN8PJ5MYdofqdrsUpfFqEg0pVQntDuWrsFTiSQBABcNtuL1kUVM+6IlyTMAYJMlGp9GOS/RXDRVgAeAjQUqaaa8EXAO9FcrwFsNiuNfKR8cYSMcXcKkFyqRJAAo9fAGHcO1m7pLeo3dLJ2nsUSKyiSbnKYL8MPdTowuhDSnp4jsuloZvHphtRSvmzaVjXC5iAy+nzTUsxphunfp2na4SyyXdZgMEJZNZFPQ3DTdX3dTjxOcA8dn/DlWv0Ifr3QNvED9YSlJopED/Hwghl63tE+HJ3342YFJzAaimPVHYTbq8a+3nZ9TfjW+GEK73aR4ixBnJ212E/7i+o1lLQCrzxnS4JubposOG+VKmiNTuQFeNDlV2odGoM7gHSVkRmm7gnQG/7+ePoRXTsyjw2GGUcdwxhvB3e/ZiDUdmfavpxdC6Cd5hgDwqXetL+v5dnN6IZYkmuam6SSawXY7zAYdjmno8Ge8YXQ4TFXzixDNTQ6zoaQBHKL5SZRKJlMc+8e8uP2SQbz+hXfj3t/elvG4monFMMkzxJJQZ/C0yNrcNF2A1+sYNnQ7cFTDk2Z8sTo18AKRDZWqayoZvFxJc3I2gEA0gfMGWjIen9MI8LP+KLrqfIwZUZ+QRHP20HQBHgA2drtwRCuD94SrtsAKpCWaUj80DrMBJr1OkWj2jnkAAOfLAb7DoW1IFo4lEYwl635OJVGf2E2UwZ8tFA3wjLGHGWMzjLG3srZ/mjF2lDF2kDF2X/V2sXw29Tgx649mVKdwzjFRpS5WgZBoSv3QMMbkWngpQ9835oHTYsBaWW9vtWVm+AKR0deydZ5oXEiDP3soJYP/DoAb1BsYY+8CcDOAbZzzLQC+VvldWzrDKssCwWIojkg8tTIZfBkfmja7SbkQ7R/z4Lz+FsUnx2TQwW015njGz8oBvpMCPLEExFxWvY4pI/yI5qRogOecvwAgezbYnwC4l3MelZ8zU4V9WzJpT5p0R+uZKtfAA2ntvRxds91hxlwwhnAsiSNTfpw34M56PLfbdc5PGTyxdGym9J0mOZE2N0vV4IcBXMUYe40x9jxj7KJ8T2SM3ckY280Y2z07O7vEH1ceXU4zWmzGjIVWYQJWzcoTEdjLaR5pt5uwEIzi4BkvkimO8wdaMx7vsJtzFlmFJt9RwwEUROMiMniSZ5qfpQZ4A4BWAJcC+AsAj7E8qQDn/EHO+Q7O+Y7OzuJOd5WAMYaN3ZmWBSuRwQvtvZwZl+2yJ/w+eYE1O4PvcOZaCouA315hT3vi7MBi1EHHaIH1bGCpAX4cwBNcYheAFIC68q0VnjSRuGRZMOEJw2LUVXX6UYvVhA6HGRu6ta2KtWh3mBGKJbHzxDz6WqzocmY2YbXbcy2F5wJRuK1GmqVJLAnGGOxmA2XwZwFLjRA/AnANADDGhgGYAMxVaJ8qwo1bexGMJXH/r98GkPaBr6bmaDLosOuvr8VN560q+TXCruDlE3NKeWTG4w4TFkNxJJIpZdtcIIoOB8kzxNKxmwyUwZ8FlFIm+SiAnQA2MsbGGWMfB/AwgLVy6eQPANzBOefV3dXyuHRtOz5wQR++9cIJvD3jx0SVa+AFOh0r6yIimpki8VSOPCM9LskwC6G0TDPnj9ECK7EsPri9H+/dWpq9MNG4FL2Ec85/N89Dt1d4XyrOX7/vHDx7ZAZ//cRbmFgMZ4zLqxeEoySAnAVWAOhQGZIJ+WY2EMXmOvxdiMbh7us31noXiBWgqUXcDocZ97x3E3aNLGA+GMMqd/15t4hMXK9jOLcvN2iLDF5dKjnnj1INPEEQRWnqAA8AH9oxgIuGpMx4JSSachEZ/HC3U6lPVpN2nJQWWiPxJPzRBGnwBEEUpekDvE7H8A8f2Iotq1zYPpgrgdQam0kPp8WAC1e3aD7eYc/0oyGbAoIgSuWsWEZf3+XE0392Va13QxPGGL7/R5fmHULishpg0DGlVFJpcqIATxBEEc6KAF/vbO3PrZ4RCEMykbkrNgXkJEkQRBGaXqJpBqRmp2yJhjR4giAKQwG+AWh3mDAXJA2eIIjyoADfAHQ4zBkavNNsqNrYQYIgmgcK8A1Ah8oyeDYQJf2dIIiSoADfALQ7zAjHkwjFEpjzkw8NQRClQQG+AWhX2RVIRmOUwRMEURwK8A1Aevh2FHMBMhojCKI0KMA3AMKuYMobgTccpwBPEERJUIBvAIThmBhBSKP6CIIoBQrwDYDQ4MUIQsrgCYIoBQrwDYDFqIfDbKAATxBEWVCAbxDaHSaMzAcBgLzgCYIoCQrwDUK73YSUPBSRNHiCIEqBAnyDIBZabSa95mAQgiCIbCjANwiie5X0d4IgSoUCfIMgAjvZFBAEUSoU4BsEUSpJGTxBEKVCAb5BEBo8OUkSBFEqFOAbhHbS4AmCKBMK8A2CCOydpMETBFEiFOAbhHWdDvzp1etw/ZaeWu8KQRANAhVUNwh6HcNf3rCp1rtBEEQDQRk8QRBEk1I0wDPGHmaMzTDG3tJ47G7GGGeMdVRn9wiCIIilUkoG/x0AN2RvZIwNALgOwOkK7xNBEARRAYoGeM75CwAWNB76FwB/CYBXeqcIgiCI5bMkDZ4xdhOACc75/hKeeydjbDdjbPfs7OxSfhxBEASxBMoO8IwxG4AvAPjbUp7POX+Qc76Dc76js7Oz3B9HEARBLJGlZPDrAKwBsJ8xNgKgH8AbjDEq0CYIgqgjyq6D55wfANAl/i8H+R2c87kK7hdBEASxTBjnhddIGWOPArgaQAeAaQB/xzl/SPX4CEoM8IyxWQCj8nvRBYGOA0DHAKBjIKDjkP8YDHLOy9a4iwb4asAY280537HiP7jOoONAxwCgYyCg41D5Y0CdrARBEE0KBXiCIIgmpVYB/sEa/dx6g44DHQOAjoGAjkOFj0FNNHiCIAii+pBEQxAE0aRQgCcIgmhSKhLgtSyFGWPnMcZ2MsYOMMaeYoy55O3XMcb2yNv3MMauUb1mu7z9bcbYNxhjrBL7t1KUcxxUj69mjAUYY3ertjXscSj3GDDGtsmPHZQft8jbG/YYAGV/JoyMse/K2w8zxu5RvaZhjwNjbIAx9hv5dzrIGLtL3t7GGPslY+y4/LVV9Zp75N/1KGPsetX2hjwO5R6DisdHzvmy/wF4B4ALAbyl2vY6gHfK3/8hgC/L318AYJX8/bmQTMvEa3YBuAwAA/AzAO+txP6t1L9yjoPq8ccB/BeAu5vhOJR5LhgAvAngPPn/7QD0jX4MlnAcfg/AD+TvbQBGAAw1+nEA0AvgQvl7J4BjADYDuA/A5+Xtnwfwj/L3mwHsB2CGZIdyotHPhyUcg4rGx0r+IkNZJ7MP6UXcAQCHNF7DAMzLf9BeAEdUj/0ugG/V+g9UzeMA4BYAXwXwRcgBvhmOQ6nHAMCNAL6n8fqGPwZlHoffBfAUpAteuxwE2prlOKj2/8eQZkgcBdCr+lsflb+/B8A9quf/Qg5oTXMcih2DrOcuOz5WU4N/C8BN8ve/A+mEzua3AezlnEcB9AEYVz02Lm9rdDSPA2PMDuCvAHwp6/nNeBzynQvDADhj7BeMsTcYY38pb2/GYwDkPw7/DSAIYBLSAJ2vcc4X0ETHgTE2BCk7fQ1AN+d8EgDkr8Lbqg/AmOpl4vdtiuNQ4jFQs+z4WM0A/4cAPsUY2wPp1iSmfpAxtgXAPwL4hNik8R7NUMOZ7zh8CcC/cM4DWc9vxuOQ7xgYAFwJ4CPy11sZY9eiOY8BkP84XAwgCWAVJGnic4yxtWiS48AYc0CSIj/DOfcVeqrGNl5ge8NQxjEQz69IfCzbTbJUOOdHALwHABhjwwDeJx5jjPUD+CGAj3HOT8ibxyFZDwv6AZyp1v6tFAWOwyUAPsgYuw9AC4AUYywC6SRoquNQ4BiMA3iey0Z1jLGfQtKtv4cmOwZAwePwewB+zjmPA5hhjL0MYAeAF9Hgx4ExZoR0Tj/COX9C3jzNGOvlnE8yxnoBzMjbx5F5py9+34aODWUeg4rGx6pl8IyxLvmrDsD/BPCA/P8WAE9D0tpeFs+Xb1P8jLFL5dXhj0HSqxqafMeBc34V53yIcz4E4F8BfIVzfn8zHod8xwCSxrqNMWZjjBkAvBOSLt10xwAoeBxOA7iGSdgBXApJb23o4yDv80MADnPO/1n10JMA7pC/vwPp3+lJAB9mjJkZY2sAbACwq5GPQ7nHoOLxsUILB49C0g/jkK40HwdwF6TFomMA7kV6cel/QtIb96n+dcmP7YCkU54AcL94TaP8K+c4ZL3ui8isomnY41DuMQBwO4CD8u97XzMcg3KPAwAHpEqqgwAOAfiLZjgOkGQ3DqlSSnzWb4S0kPwsgOPy1zbVa74g/65HoaoSadTjUO4xqHR8JKsCgiCIJoU6WQmCIJoUCvAEQRBNCgV4giCIJoUCPEEQRJNCAZ4gCKJJoQBPEATRpFCAJwiCaFL+f80yntpg6vcWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def avg_sentence_length_year(year, k):\n",
    "    sentence_len = []\n",
    "    for file in get_files_for_year(year, k):\n",
    "        sentences = extract_sentences(file)\n",
    "        sentence_len.extend([len(s) for s in sentences])\n",
    "    \n",
    "    return reduce(lambda a, b: a + b, sentence_len) / len(sentence_len)\n",
    "        \n",
    "sentence_len_years = []\n",
    "\n",
    "for year in range(1924, 2018):\n",
    "    sentence_len_years.append(avg_sentence_length_year(year, 20))\n",
    "    \n",
    "avg_df = pd.DataFrame(sentence_len_years, index=range(1924, 2018), columns=['avg_sent_len'])\n",
    "avg_df.plot(kind='line')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earlier-recycling",
   "metadata": {},
   "source": [
    "### n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-necklace",
   "metadata": {},
   "source": [
    "Here is a description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "distinguished-oliver",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:08:55.908427Z",
     "start_time": "2021-01-26T11:08:54.776042Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def most_common_ngrams(n, top_k, sample):\n",
    "    file_contents = []\n",
    "\n",
    "    for file in get_random_sample(sample):\n",
    "        file_contents.extend(extract_words(file)[1])\n",
    "    \n",
    "    fq_ngr = FreqDist(ngrams(file_contents, n))\n",
    "    fq_ngr.plot(top_k, cumulative=False)\n",
    "    \n",
    "most_common_ngrams(n=3, top_k=25, sample=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-bolivia",
   "metadata": {},
   "source": [
    "## Building model for classifying speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-penny",
   "metadata": {},
   "source": [
    "This section is the main core of the modelling task of this assignment. It is organized as follows:\n",
    "\n",
    "* Firstly, the data is splitted into **training and test sets** according to the criteria that in our opinion fits better to the nature of data and task.\n",
    "* Secondly, we describe the **methods** that are going to be used for **feature extraction** from our documents.\n",
    "* After that, we describe the **classifiers chosen** to be trained and why they were selected.\n",
    "* Then, we **train 7 models** combining the feature extraction techniques described and the classifiers selected. This is done through a cross-validated grid search in which many hyperparameters are combined. The goal of this search is to find the **best hyperparameter combination of each of the 7 models.**\n",
    "* Finally, we **compare the results from the training within and between the models.**\n",
    "* Evaluation on test data will be performed in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-poster",
   "metadata": {},
   "source": [
    "### Constructing training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-priority",
   "metadata": {},
   "source": [
    "Our whole dataset contains **380285 speeches** hold in the Icelandic parliament from 1911 to 2017. In order to perform our train-test split, we took into account the following considerations:\n",
    "\n",
    "* Documents are **classified in directories by year and month instead of decade.**\n",
    "* Decades (classes) are highly **unbalanced.** There are much more documents from laterdecades as from the earlier ones. As an example, 1912 has only 14 documents while 2011 has 13957. This may introduce bias in the training of the models if not dealt. \n",
    "\n",
    "To solve the first problem, we use the help function get_files_for_year() created above, which takes n documents from an specified year. After that, for each of the documents, the year is substituted by the decade as shown in the next two sections. This can be done iteratively through a list of years. In this way, **we obtain a dataset with a bunch of corpora labelled by decade.**\n",
    "\n",
    "To solve the problem of unbalance within classes, we **limit the number of documents to be extracted from each year to 200 for the training set.** This way, we ensure that there will not be too big differences within the number of documents sampled within the years (maximum of 200 vs minimum of 14) and neither within the decades. We choose 200 since we consider it to be a good balance for **undersampling the majority classes but not loosing as much information as we would keep it to minimum of 14**.\n",
    "\n",
    "Note that, in order to make the runtimes of our notebook shorter (feasible) we **skip intermediate decades from our classification task.** This would simulate that there were not speeches held in some decades. We like to imagine it as weird regime which combines a decade of democracy followed by a decade of dictatorship. In summary:\n",
    "\n",
    "- 1910s, 1930s, 1950s, 1970s, 1990s, and 2010s are considered.\n",
    "- Whereas 1920s, 1940s, 1960s, 1980s, and 2000s are discarded.\n",
    "\n",
    "We will perform a train/test split of the approximate proportion 80/20. We will see why it will be approximate in the next two sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-rotation",
   "metadata": {},
   "source": [
    "#### Train data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-hometown",
   "metadata": {},
   "source": [
    "**8 years out of the 10 years** that form a decade are chosen for each of the 6 decades considered for the train set. **The other 2 are left for the test set**. The selection of the years was completly random. For each of the decades a maximum of 1600 documents are chosen. However, this will not be equal for all the decandes, since, as explained above, not all the years have at least 200 documents. \n",
    "\n",
    "Note that for the last decade, we just have documents until 2017. The split will be 6 years (train) vs 1 (test) in this case. Same applies for first decade (in this case, 7 vs. 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "extended-jewelry",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:09:16.914303Z",
     "start_time": "2021-01-26T11:08:55.909494Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For year 1911: Fetching 200 samples out of 125 (~160.0%)\n",
      "For year 1912: Fetching 200 samples out of 14 (~1428.5714285714287%)\n",
      "For year 1914: Fetching 200 samples out of 1306 (~15.313935681470138%)\n",
      "For year 1915: Fetching 200 samples out of 1383 (~14.461315979754158%)\n",
      "For year 1916: Fetching 200 samples out of 0 (~0%)\n",
      "For year 1918: Fetching 200 samples out of 0 (~0%)\n",
      "For year 1919: Fetching 200 samples out of 0 (~0%)\n",
      "For year 1931: Fetching 200 samples out of 25 (~800.0%)\n",
      "For year 1932: Fetching 200 samples out of 28 (~714.2857142857143%)\n",
      "For year 1933: Fetching 200 samples out of 52 (~384.61538461538464%)\n",
      "For year 1934: Fetching 200 samples out of 20 (~1000.0%)\n",
      "For year 1935: Fetching 200 samples out of 24 (~833.3333333333334%)\n",
      "For year 1936: Fetching 200 samples out of 25 (~800.0%)\n",
      "For year 1937: Fetching 200 samples out of 1381 (~14.48225923244026%)\n",
      "For year 1938: Fetching 200 samples out of 1676 (~11.933174224343675%)\n",
      "For year 1951: Fetching 200 samples out of 2453 (~8.153281695882592%)\n",
      "For year 1952: Fetching 200 samples out of 1689 (~11.841326228537596%)\n",
      "For year 1953: Fetching 200 samples out of 1415 (~14.134275618374557%)\n",
      "For year 1955: Fetching 200 samples out of 1433 (~13.956734124214934%)\n",
      "For year 1956: Fetching 200 samples out of 1213 (~16.488046166529266%)\n",
      "For year 1957: Fetching 200 samples out of 1763 (~11.344299489506524%)\n",
      "For year 1958: Fetching 200 samples out of 1169 (~17.108639863130882%)\n",
      "For year 1959: Fetching 200 samples out of 1326 (~15.082956259426847%)\n",
      "For year 1970: Fetching 200 samples out of 1921 (~10.41124414367517%)\n",
      "For year 1971: Fetching 200 samples out of 2071 (~9.657170449058427%)\n",
      "For year 1972: Fetching 200 samples out of 2368 (~8.445945945945946%)\n",
      "For year 1973: Fetching 200 samples out of 2329 (~8.587376556462%)\n",
      "For year 1974: Fetching 200 samples out of 2350 (~8.51063829787234%)\n",
      "For year 1975: Fetching 200 samples out of 2430 (~8.23045267489712%)\n",
      "For year 1978: Fetching 200 samples out of 2641 (~7.5728890571753125%)\n",
      "For year 1979: Fetching 200 samples out of 2165 (~9.237875288683602%)\n",
      "For year 1990: Fetching 200 samples out of 4662 (~4.29000429000429%)\n",
      "For year 1991: Fetching 200 samples out of 4747 (~4.213187276174426%)\n",
      "For year 1992: Fetching 200 samples out of 8925 (~2.2408963585434174%)\n",
      "For year 1993: Fetching 200 samples out of 7412 (~2.698327037236913%)\n",
      "For year 1995: Fetching 200 samples out of 5129 (~3.899395593682979%)\n",
      "For year 1996: Fetching 200 samples out of 7184 (~2.7839643652561246%)\n",
      "For year 1997: Fetching 200 samples out of 6960 (~2.8735632183908044%)\n",
      "For year 1999: Fetching 200 samples out of 6056 (~3.3025099075297226%)\n",
      "For year 2010: Fetching 200 samples out of 11089 (~1.8035891423933628%)\n",
      "For year 2011: Fetching 200 samples out of 13957 (~1.4329727018700293%)\n",
      "For year 2012: Fetching 200 samples out of 16356 (~1.2227928588897041%)\n",
      "For year 2013: Fetching 200 samples out of 10240 (~1.953125%)\n",
      "For year 2014: Fetching 200 samples out of 12404 (~1.6123831022250887%)\n",
      "For year 2016: Fetching 200 samples out of 8165 (~2.449479485609308%)\n",
      "For year 2017: Fetching 200 samples out of 7270 (~2.751031636863824%)\n"
     ]
    }
   ],
   "source": [
    "#set seed for reproducibility\n",
    "random.seed(123)\n",
    "\n",
    "file_contents = []\n",
    "targets = []\n",
    "\n",
    "\n",
    "for year in [1911, 1912, 1914, 1915, 1916, 1918, 1919,\n",
    "             1920, 1921, 1922, 1924, 1925, 1926, 1928, 1929,\n",
    "             1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, \n",
    "             1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, \n",
    "             1951, 1952, 1953, 1955, 1956, 1957, 1958, 1959,\n",
    "             1961, 1962, 1963, 1965, 1966, 1967, 1968, 1969,\n",
    "             1970, 1971, 1972, 1973, 1974, 1975, 1978, 1979,\n",
    "             1980, 1981, 1982, 1983, 1984, 1985, 1988, 1989,\n",
    "             1990, 1991, 1992, 1993, 1995, 1996, 1997, 1999,\n",
    "             2000, 2001, 2002, 2003, 2005, 2006, 2007, 2009,\n",
    "             2010, 2011, 2012, 2013, 2014, 2016, 2017]:\n",
    "    for file in get_files_for_year(year, 200):\n",
    "        file_contents.append(extract_words(file)[1])\n",
    "        targets.append(year - year%10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "actual-validity",
   "metadata": {},
   "source": [
    "Let's randomly choose a fixed number of documents (here currently: 200) from various different decades. Then passing (document, decade) pairs to the model below. The decade is computed by subtracting `mod(<year>, 10)` from `<year>`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-borough",
   "metadata": {},
   "source": [
    "#### Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-angel",
   "metadata": {},
   "source": [
    "Choose the other **2 years that were not selected** within the decades in the train set. In this case, **we do not have to limit the number of documents for year.** It doesn't make sense to undersample the test set since it represents \"unseen\" data. And, unseen data should be as close to reality as possible. That means, that it is normal that there are much more documents from later decades than from earlier. \n",
    "\n",
    "So, instead of 200, we will put there a very large number to be sure that all the documents from every year are selected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "large-balloon",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:11:51.654785Z",
     "start_time": "2021-01-26T11:09:16.915305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For year 1913: Fetching 2037 samples out of 2037 (~100.0%)\n",
      "For year 1917: Fetching 0 samples out of 0 (~0%)\n",
      "For year 1930: Fetching 30 samples out of 30 (~100.0%)\n",
      "For year 1939: Fetching 1632 samples out of 1632 (~100.0%)\n",
      "For year 1950: Fetching 3017 samples out of 3017 (~100.0%)\n",
      "For year 1954: Fetching 1546 samples out of 1546 (~100.0%)\n",
      "For year 1976: Fetching 2555 samples out of 2555 (~100.0%)\n",
      "For year 1977: Fetching 2105 samples out of 2105 (~100.0%)\n",
      "For year 1994: Fetching 8187 samples out of 8187 (~100.0%)\n",
      "For year 2013: Fetching 10240 samples out of 10240 (~100.0%)\n",
      "For year 2015: Fetching 18052 samples out of 18052 (~100.0%)\n"
     ]
    }
   ],
   "source": [
    "#seed for reproducibility\n",
    "random.seed(123)\n",
    "\n",
    "file_contents_test = []\n",
    "targets_test = []\n",
    "\n",
    "for year in [1913, 1917,1930, 1939, 1950, 1954, 1976, 1977, 1994, 2013,2015,\n",
    "             1923, 1927,1940, 1949, 1960, 1964, 1986, 1987, 2004]:\n",
    "    for file in get_files_for_year(year):\n",
    "        file_contents_test.append(extract_words(file)[1])\n",
    "        targets_test.append(year - year%10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-horse",
   "metadata": {},
   "source": [
    "#### See classes distribution within train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "subjective-chicken",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:11:51.665579Z",
     "start_time": "2021-01-26T11:11:51.655722Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([1910, 1930, 1950, 1970, 1990, 2010])\n",
      "dict_values([539, 574, 1600, 1600, 1600, 1400])\n",
      "dict_keys([1910, 1930, 1950, 1970, 1990, 2010])\n",
      "dict_values([2037, 1662, 4563, 4660, 8187, 28292])\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(Counter(targets).keys()) \n",
    "print(Counter(targets).values()) \n",
    "\n",
    "print(Counter(targets_test).keys()) \n",
    "print(Counter(targets_test).values()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-oasis",
   "metadata": {},
   "source": [
    "We see that although there are some differences within the classes for the train split, it is acceptable to perform the classification task. Maximum within the classes for training is 1600.\n",
    "\n",
    "Test set is expected to have much more class imbalance. However, our model should dealt with it thanks to the undersampling that was performed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-swiss",
   "metadata": {},
   "source": [
    "### Text feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-assurance",
   "metadata": {},
   "source": [
    "We have considered 3 different methods for text feature extraction: Tf-idf, word2vec and doc2vec. All of them will be implemented through the corresponding functions from *sklearn* library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-vinyl",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-concentrate",
   "metadata": {},
   "source": [
    "Helper function to transform the data so that it is in the right format for the tfidfVectorizer() function that will be used later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "substantial-adoption",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:11:51.677339Z",
     "start_time": "2021-01-26T11:11:51.666684Z"
    }
   },
   "outputs": [],
   "source": [
    "class JoinElement(object):\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        #joins the elements of a list (which represents a document) into a single string \n",
    "        #with a blank space separation between each word\n",
    "        return [' '.join(X[i]) for i in range(len(X))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-elements",
   "metadata": {},
   "source": [
    "More information about it: [sklearn documentation.](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-illinois",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-rover",
   "metadata": {},
   "source": [
    "<ins>Original paper</ins>:\n",
    "Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed representations of words and phrases and their compositionality. Advances in neural information processing systems, 26, 3111-3119.\n",
    "\n",
    "With this model every word is assigned a unique vector of configurable cardinality such that the dot product of two randomly chosen vectors should be proportional to the semantic similarity for the associated words. This happens during the training step using logistic regression and sliding windows. Personally I found that this video delivers a solid explanation of the concepts: https://www.youtube.com/watch?v=QyrUentbkvw\n",
    "\n",
    "However, since we are working with entire documents as training items we have to somehow aggregate the vectors for every word in a given document. This can be done e.g. by taking the mean and/or summing up the vectors (see `MeanEmbeddingVectorizer`), optionally weighted by TF-IDF (see `MeanEmbeddingVectorizerTfidf`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "exact-conditions",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:11:51.682702Z",
     "start_time": "2021-01-26T11:11:51.678251Z"
    }
   },
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(BaseEstimator):\n",
    "    def fit(self, X, y):\n",
    "        self.word2vec = Word2Vec(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec.wv[w] for w in words if w in self.word2vec.wv.vocab]\n",
    "                    or [np.zeros(self.word2vec.vector_size)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "intimate-ottawa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:11:51.690301Z",
     "start_time": "2021-01-26T11:11:51.683680Z"
    }
   },
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizerTfidf(BaseEstimator):\n",
    "    def fit(self, X, y):\n",
    "        self.word2vec = Word2Vec(X)\n",
    "        self.X_joined = [' '.join(X[i]) for i in range(len(X))]\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.transformed = self.vectorizer.fit_transform(self.X_joined)\n",
    "        self.transformed = pd.DataFrame.sparse.from_spmatrix(self.transformed)\n",
    "        return self\n",
    "    \n",
    "    def tfidf(self, w, docid):\n",
    "        if w in self.vectorizer.vocabulary_:\n",
    "            return self.transformed[self.vectorizer.vocabulary_[w]][docid]\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec.wv[w] * self.tfidf(w, i) for w in words if w in self.word2vec.wv.vocab]\n",
    "                    or [np.zeros(self.word2vec.vector_size)], axis=0)\n",
    "            for i, words in enumerate(X)\n",
    "        ])\n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        self = self.fit(X, y)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-recall",
   "metadata": {},
   "source": [
    "#### Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-designer",
   "metadata": {},
   "source": [
    "Finally we are attempting to build a model using _Doc2Vec_. After training this model with our training corpus we receive a vector of configurable cardinality for each document.\n",
    "\n",
    "<ins>Original paper</ins>: Le, Quoc, and Tomas Mikolov. \"Distributed representations of sentences and documents.\" International conference on machine learning. 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "amended-conviction",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:11:51.695574Z",
     "start_time": "2021-01-26T11:11:51.691334Z"
    }
   },
   "outputs": [],
   "source": [
    "class Doc2Vectorizer(BaseEstimator):\n",
    "    def __init__(self, window=2, vector_size=100):\n",
    "        self.window = window\n",
    "        self.vector_size = vector_size\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        docs = [TaggedDocument(X[i], [y[i]]) for i in range(len(X))]\n",
    "        self.doc_vec = Doc2Vec(docs, vector_size=self.vector_size, window=self.window, min_count=1, workers=4)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return [self.doc_vec.infer_vector(X[i]) for i in range(len(X))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-prediction",
   "metadata": {},
   "source": [
    "**BERT** (*Bidirectional Encoder Representations from Transformers*) is also interesting to look at, but we'll skip this here because we predict training a model from scratch would use up too many resources. Given more time however you could search for pretrained networks that roughly serve the purpose of classification of documents according to publication year.\n",
    "\n",
    "<ins>Paper</ins>: Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-adapter",
   "metadata": {},
   "source": [
    "### Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-fetish",
   "metadata": {},
   "source": [
    "3 different classifiers are going to be trained: Multinomial Naive Bayes, Support Vector Machines and Random Forest Classifier. All of them will be implemented using sklearn library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-drama",
   "metadata": {},
   "source": [
    "#### Multinominal Naive Bayes (MNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-rating",
   "metadata": {},
   "source": [
    "MNB is a common method for document classification due to its good balance between computational efficiency and predictive performance [(Eibe, 2006)](https://www.cs.waikato.ac.nz/~eibe/pubs/FrankAndBouckaertPKDD06new.pdf). Therefore, we decided to choose it as one of our classifiers. \n",
    "\n",
    "Details on the algorithm implementation can be found in the [sklearn documentation.](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html).\n",
    "\n",
    "The set of hyperparameters chosen to perform the grid search cross-validation during the training are based on the recommendations[ from this article. ](https://medium.com/@kocur4d/hyper-parameter-tuning-with-pipelines-5310aff069d6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-journey",
   "metadata": {},
   "source": [
    "#### Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-garage",
   "metadata": {},
   "source": [
    "Support vector machines are widely used for classification purposes. What is more, it improves Multinominal Naive Bayes in terms of performance in most of the classification taks. Thus, it was also chosen as one of our classifiers to be trained.\n",
    "\n",
    "Details on the algorithm implementation can be found in [sklearn documentation.](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n",
    "\n",
    "The set of hyperparameters chosen to perform the grid search cross-validation during the training are based on the recommendations [ from this article. ](https://www.vebuso.com/2020/03/svm-hyperparameter-tuning-using-gridsearchcv/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-surgery",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-sharing",
   "metadata": {},
   "source": [
    "Random Forest Classifier is one of the best methods according to the literature for classification tasks. However, the runtime may be extremly large (specially when increasing the size of the forest within grid search CV setups). \n",
    "\n",
    "Details on the algorithm implementation can be found in [sklearn documentation.](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html).\n",
    "\n",
    "The set of hyperparameters chosen to perform the grid search cross-validation during the training are based on the recommendations [ from this article. ](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-captain",
   "metadata": {},
   "source": [
    "### Train models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-discipline",
   "metadata": {},
   "source": [
    "Since there are 3 methods for feature extraction and 3 classifiers, we should train 9 kind of models with their different combinations of hyperparameters. However, multinomial naive bayes does not take negative values produced by Word2Vec and Doc2Vec. Therefore, we have 7.\n",
    "\n",
    "For each model, **a grid search is performed with different combinations of hyperparameters** for the classifiers and the text extraction methods. Afterwards, the most relevant results of each of the models are stored in a pandas data frame.\n",
    "\n",
    "The goal of this grid search is to find the best combination of hyperparameters for each of our 7 combinations.\n",
    "\n",
    "Note that **ideally we should perform a random search prior to the grid search to limit the scope of the best hyperparameters** to be used and then perform a more accurate search. However, this would lead to a tedious notebook and extremly large runtimes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-conspiracy",
   "metadata": {},
   "source": [
    "#### Model 1: TFIDF vectorizer, select K best and Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-situation",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-26T11:07:33.617Z"
    }
   },
   "outputs": [],
   "source": [
    "#choose parameters for the different steps in the pipeline\n",
    "parameters_model_1 = {\n",
    "    \n",
    "    #select KBest\n",
    "    #k number of top features to select (default 10)\n",
    "    \"k_best__k\": [10, 500],\n",
    "    #score function to be used (default f_classif)\n",
    "    \"k_best__score_func\": [chi2],\n",
    "    \n",
    "    #MultinomialNaiveBayes\n",
    "    #alpha is a parameter for smoothing (default value is 1)\n",
    "    \"MNB__alpha\": np.linspace(0.5, 1.5, 4), \n",
    "    #whether to learn class prior probabilities or not (dafult value is True)\n",
    "    \"MNB__fit_prior\": [True,False],\n",
    "    \n",
    "    #TFIDF Vectorizer\n",
    "    #Whether the feature should be made of word or character n-grams (default word)\n",
    "    \"tfidf__analyzer\": [\"word\"],\n",
    "    #Smooth idf weights by adding one to document frequencies, \n",
    "    #as if an extra document was seen containing every term in the collection exactly once.\n",
    "    #Prevents zero divisions (default True)\n",
    "    \"tfidf__smooth_idf\": [True, False]\n",
    "    \n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "#build a pipeline\n",
    "model_1_pipeline = Pipeline([\n",
    "        #joins list into a single string\n",
    "        ('join', JoinElement()),\n",
    "        #tfidf vectorizer\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        #select 1000 best from word vectors\n",
    "        ('k_best', SelectKBest()),\n",
    "        #apply naive bayes\n",
    "        ('MNB', MultinomialNB())\n",
    "    ])\n",
    "\n",
    "#design grid search\n",
    "grid_search_model_1 = GridSearchCV(\n",
    "    #pipeline to be followed\n",
    "    model_1_pipeline,\n",
    "    #parameters\n",
    "    param_grid=parameters_model_1,\n",
    "    #number of folds for CV\n",
    "    cv=5,\n",
    "    #scoring to be considered for the cv\n",
    "    scoring = \"accuracy\",\n",
    "    #parallelize if possible\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#fit the grid search for training data\n",
    "grid_search_model_1.fit(file_contents, targets)\n",
    "\n",
    "#save results of cross validation\n",
    "cv_results_model_1 = pd.DataFrame(grid_search_model_1.cv_results_)\n",
    "\n",
    "#filter columns to be kept in the dataframe\n",
    "filter_col = [col for col in cv_results_model_1 if (col.startswith(\"param_\") or col.startswith(\"mean_\") or col.startswith(\"rank\"))]\n",
    "\n",
    "#save results with only filtered columns\n",
    "cv_results_model_1 = cv_results_model_1[filter_col]\n",
    "\n",
    "#save name of the model for later comparison\n",
    "cv_results_model_1.insert(loc=0, column=\"Model\", value= \"1\")\n",
    "\n",
    "#round mean_test_score\n",
    "cv_results_model_1[\"mean_test_score\"] = cv_results_model_1[\"mean_test_score\"].round(2)\n",
    "\n",
    "#show best 5 sorted by mean_test_score\n",
    "display(cv_results_model_1.sort_values(by=\"mean_test_score\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-leadership",
   "metadata": {},
   "source": [
    "#### Model 2: TFIDF vectorizer, select K best and SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-bandwidth",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-26T11:07:33.622Z"
    }
   },
   "outputs": [],
   "source": [
    "#choose parameters for the different steps in the pipeline\n",
    "parameters_model_2 = {\n",
    "\n",
    "    #select KBest\n",
    "    #k number of top features to select (default 10)\n",
    "    \"k_best__k\": [10, 500],\n",
    "    #score function to be used (default f_classif)\n",
    "    \"k_best__score_func\": [chi2],\n",
    "    \n",
    "    #SVC\n",
    "    #Specifies the kernel type to be used in the algorithm\n",
    "    \"SVC__kernel\" : [\"linear\", \"poly\", \"sigmoid\"],\n",
    "    #Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid'\n",
    "    \"SVC__gamma\": [1,0.1,0.001],\n",
    "    #Regularization parameter. The strength of the regularization is inversely proportional to C. \n",
    "    #Must be strictly positive. The penalty is a squared l2 penalty.\n",
    "    \"SVC__C\": [0.1,1, 10, 100],\n",
    "    \n",
    "    #TFIDF Vectorizer\n",
    "    #Whether the feature should be made of word or character n-grams (default word)\n",
    "    \"tfidf__analyzer\": [\"word\"],\n",
    "    #Smooth idf weights by adding one to document frequencies, \n",
    "    #as if an extra document was seen containing every term in the collection exactly once.\n",
    "    #Prevents zero divisions (default True)\n",
    "    \"tfidf__smooth_idf\": [True, False]\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "#build a pipeline\n",
    "model_2_pipeline = Pipeline([\n",
    "        #joins list into a single string\n",
    "        ('join', JoinElement()),\n",
    "        #tfidf vectorizer\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        #select 1000 best from word vectors\n",
    "        ('k_best', SelectKBest()),\n",
    "        #apply naive bayes\n",
    "        ('SVC', SVC())\n",
    "    ])\n",
    "\n",
    "#design grid search\n",
    "grid_search_model_2 = GridSearchCV(\n",
    "    #pipeline to be followed\n",
    "    model_2_pipeline,\n",
    "    #parameters\n",
    "    param_grid=parameters_model_2,\n",
    "    #number of folds for CV\n",
    "    cv=5,\n",
    "    #scoring to be considered for the cv\n",
    "    scoring = \"accuracy\",\n",
    "    #parallelize if possible\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#fit the grid search for training data\n",
    "grid_search_model_2.fit(file_contents, targets)\n",
    "\n",
    "#save results of cross validation\n",
    "cv_results_model_2 = pd.DataFrame(grid_search_model_2.cv_results_)\n",
    "\n",
    "#filter columns to be kept in the dataframe\n",
    "filter_col = [col for col in cv_results_model_2 if (col.startswith(\"param_\") or col.startswith(\"mean_\") or col.startswith(\"rank\"))]\n",
    "\n",
    "#save results with only filtered columns\n",
    "cv_results_model_2 = cv_results_model_2[filter_col]\n",
    "\n",
    "#save name of the model for later comparison\n",
    "cv_results_model_2.insert(loc=0, column=\"Model\", value= \"2\")\n",
    "\n",
    "#round mean_test_score\n",
    "cv_results_model_2[\"mean_test_score\"] = cv_results_model_2[\"mean_test_score\"].round(2)\n",
    "\n",
    "#show best 5 sorted by mean_test_score\n",
    "display(cv_results_model_2.sort_values(by=\"mean_test_score\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-peace",
   "metadata": {},
   "source": [
    "#### Model 3: TFIDF vectorizer, select K best and Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-kernel",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-26T11:07:33.626Z"
    }
   },
   "outputs": [],
   "source": [
    "#choose parameters for the different steps in the pipeline\n",
    "parameters_model_3 = {\n",
    "    \n",
    "    #select KBest\n",
    "    #k number of top features to select (default 10)\n",
    "    \"k_best__k\": [10, 500],\n",
    "    #score function to be used (default f_classif)\n",
    "    \"k_best__score_func\": [chi2],\n",
    "    \n",
    "    #RF classifier\n",
    "    #nThe number of trees in the forest (default is 100)\n",
    "    \"clf__n_estimators\" : [10,100,200],\n",
    "    #The minimum number of samples required to split an internal node (default is 2 but it is a large dataset)\n",
    "    \"clf__min_samples_split\": [10, 40, 80],\n",
    "    #The number of features to consider when looking for the best split (default \"auto\" but sparse dataset)\n",
    "    'clf__max_features': [\"auto\", 10],\n",
    "    #maximum depth of the tree (default None)\n",
    "    'clf__max_depth': [10, None],\n",
    "\n",
    "    \n",
    "    \n",
    "    #TFIDF Vectorizer\n",
    "    #Whether the feature should be made of word or character n-grams (default word)\n",
    "    \"tfidf__analyzer\": [\"word\"],\n",
    "    #Smooth idf weights by adding one to document frequencies, \n",
    "    #as if an extra document was seen containing every term in the collection exactly once.\n",
    "    #Prevents zero divisions (default True)\n",
    "    \"tfidf__smooth_idf\": [True, False]\n",
    "\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "#build a pipeline\n",
    "model_3_pipeline = Pipeline([\n",
    "        #joins list into a single string\n",
    "        ('join', JoinElement()),\n",
    "        #tfidf vectorizer\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        #select 1000 best from word vectors\n",
    "        ('k_best', SelectKBest()),\n",
    "        #apply naive bayes\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "#design grid search\n",
    "grid_search_model_3 = GridSearchCV(\n",
    "    #pipeline to be followed\n",
    "    model_3_pipeline,\n",
    "    #parameters\n",
    "    param_grid=parameters_model_3,\n",
    "    #number of folds for CV\n",
    "    cv=5,\n",
    "    #scoring to be considered for the cv\n",
    "    scoring = \"accuracy\",\n",
    "    #parallelize if possible\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#fit the grid search for training data\n",
    "grid_search_model_3.fit(file_contents, targets)\n",
    "\n",
    "#save results of cross validation\n",
    "cv_results_model_3 = pd.DataFrame(grid_search_model_3.cv_results_)\n",
    "\n",
    "#filter columns to be kept in the dataframe\n",
    "filter_col = [col for col in cv_results_model_3 if (col.startswith(\"param_\") or col.startswith(\"mean_\") or col.startswith(\"rank\"))]\n",
    "\n",
    "#save results with only filtered columns\n",
    "cv_results_model_3 = cv_results_model_3[filter_col]\n",
    "\n",
    "#save name of the model for later comparison\n",
    "cv_results_model_3.insert(loc=0, column=\"Model\", value= \"3\")\n",
    "\n",
    "#round mean_test_score\n",
    "cv_results_model_3[\"mean_test_score\"] = cv_results_model_3[\"mean_test_score\"].round(2)\n",
    "\n",
    "#show best 5 sorted by mean_test_score\n",
    "display(cv_results_model_3.sort_values(by=\"mean_test_score\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-morgan",
   "metadata": {},
   "source": [
    "#### Model 4: Word2Vec and SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-booking",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-26T11:07:33.631Z"
    }
   },
   "outputs": [],
   "source": [
    "#choose parameters for the different steps in the pipeline\n",
    "parameters_model_4 = {\n",
    "    \n",
    "    #SVC\n",
    "    #Specifies the kernel type to be used in the algorithm\n",
    "    \"SVC__kernel\" : [\"linear\", \"poly\", \"sigmoid\"],\n",
    "    #Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid'\n",
    "    \"SVC__gamma\": [1,0.1,0.001],\n",
    "    #Regularization parameter. The strength of the regularization is inversely proportional to C. \n",
    "    #Must be strictly positive. The penalty is a squared l2 penalty.\n",
    "    \"SVC__C\": [0.1,1, 10, 100]\n",
    "    \n",
    "    #defaults for Word2Vec\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "#build a pipeline\n",
    "model_4_pipeline = Pipeline([\n",
    "        #tfidf vectorizer\n",
    "        ('word2vec', MeanEmbeddingVectorizer()),\n",
    "        #apply naive bayes\n",
    "        ('SVC', SVC())\n",
    "    ])\n",
    "\n",
    "#design grid search\n",
    "grid_search_model_4 = GridSearchCV(\n",
    "    #pipeline to be followed\n",
    "    model_4_pipeline,\n",
    "    #parameters\n",
    "    param_grid=parameters_model_4,\n",
    "    #number of folds for CV\n",
    "    cv=5,\n",
    "    #scoring to be considered for the cv\n",
    "    scoring = \"accuracy\",\n",
    "    #parallelize if possible\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#fit the grid search for training data\n",
    "grid_search_model_4.fit(file_contents, targets)\n",
    "\n",
    "#save results of cross validation\n",
    "cv_results_model_4 = pd.DataFrame(grid_search_model_4.cv_results_)\n",
    "\n",
    "#filter columns to be kept in the dataframe\n",
    "filter_col = [col for col in cv_results_model_4 if (col.startswith(\"param_\") or col.startswith(\"mean_\") or col.startswith(\"rank\"))]\n",
    "\n",
    "#save results with only filtered columns\n",
    "cv_results_model_4 = cv_results_model_4[filter_col]\n",
    "\n",
    "#save name of the model for later comparison\n",
    "cv_results_model_4.insert(loc=0, column=\"Model\", value= \"4\")\n",
    "\n",
    "#round mean_test_score\n",
    "cv_results_model_4[\"mean_test_score\"] = cv_results_model_4[\"mean_test_score\"].round(2)\n",
    "\n",
    "#show best 5 sorted by mean_test_score\n",
    "display(cv_results_model_4.sort_values(by=\"mean_test_score\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-wiring",
   "metadata": {},
   "source": [
    "#### Model 5: Word2Vec and Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-message",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-26T11:07:33.635Z"
    }
   },
   "outputs": [],
   "source": [
    "#choose parameters for the different steps in the pipeline\n",
    "parameters_model_5 = {\n",
    "    \n",
    "    #RF classifier\n",
    "    #nThe number of trees in the forest (default is 100)\n",
    "    \"clf__n_estimators\" : [10,100,200],\n",
    "    #The minimum number of samples required to split an internal node (default is 2 but it is a large dataset)\n",
    "    \"clf__min_samples_split\": [10, 40, 80],\n",
    "    #The number of features to consider when looking for the best split (default \"auto\" but sparse dataset)\n",
    "    'clf__max_features': [\"auto\", 10],\n",
    "    #maximum depth of the tree (default None)\n",
    "    'clf__max_depth': [10, None]\n",
    "    \n",
    "    #defaults word2vec\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "#build a pipeline\n",
    "model_5_pipeline = Pipeline([\n",
    "        #tfidf vectorizer\n",
    "        ('word2vec', MeanEmbeddingVectorizer()),\n",
    "        #apply naive bayes\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "#design grid search\n",
    "grid_search_model_5 = GridSearchCV(\n",
    "    #pipeline to be followed\n",
    "    model_5_pipeline,\n",
    "    #parameters\n",
    "    param_grid=parameters_model_5,\n",
    "    #number of folds for CV\n",
    "    cv=5,\n",
    "    #scoring to be considered for the cv\n",
    "    scoring = \"accuracy\",\n",
    "    #parallelize if possible\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#fit the grid search for training data\n",
    "grid_search_model_5.fit(file_contents, targets)\n",
    "\n",
    "#save results of cross validation\n",
    "cv_results_model_5 = pd.DataFrame(grid_search_model_5.cv_results_)\n",
    "\n",
    "#filter columns to be kept in the dataframe\n",
    "filter_col = [col for col in cv_results_model_5 if (col.startswith(\"param_\") or col.startswith(\"mean_\") or col.startswith(\"rank\"))]\n",
    "\n",
    "#save results with only filtered columns\n",
    "cv_results_model_5 = cv_results_model_5[filter_col]\n",
    "\n",
    "#save name of the model for later comparison\n",
    "cv_results_model_5.insert(loc=0, column=\"Model\", value= \"5\")\n",
    "\n",
    "#round mean_test_score\n",
    "cv_results_model_5[\"mean_test_score\"] = cv_results_model_5[\"mean_test_score\"].round(2)\n",
    "\n",
    "#show best 5 sorted by mean_test_score\n",
    "display(cv_results_model_5.sort_values(by=\"mean_test_score\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-vacation",
   "metadata": {},
   "source": [
    "#### Model 6: Doc2Vec and Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-tribune",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-26T11:07:33.640Z"
    }
   },
   "outputs": [],
   "source": [
    "#choose parameters for the different steps in the pipeline\n",
    "parameters_model_6 = {\n",
    "    \n",
    "    #SVC\n",
    "    #Specifies the kernel type to be used in the algorithm\n",
    "    \"SVC__kernel\" : [\"linear\", \"poly\", \"sigmoid\"],\n",
    "    #Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid'\n",
    "    \"SVC__gamma\": [1,0.1,0.001],\n",
    "    #Regularization parameter. The strength of the regularization is inversely proportional to C. \n",
    "    #Must be strictly positive. The penalty is a squared l2 penalty.\n",
    "    \"SVC__C\": [0.1,1, 10, 100]\n",
    "    \n",
    "    #defaults doc2vec\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "#build a pipeline\n",
    "model_6_pipeline = Pipeline([\n",
    "        #tfidf vectorizer\n",
    "        ('doc2vec', Doc2Vectorizer()),\n",
    "        #apply naive bayes\n",
    "        ('SVC', SVC())\n",
    "    ])\n",
    "\n",
    "#design grid search\n",
    "grid_search_model_6 = GridSearchCV(\n",
    "    #pipeline to be followed\n",
    "    model_6_pipeline,\n",
    "    #parameters\n",
    "    param_grid=parameters_model_6,\n",
    "    #number of folds for CV\n",
    "    cv=5,\n",
    "    #scoring to be considered for the cv\n",
    "    scoring = \"accuracy\",\n",
    "    #parallelize if possible\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#fit the grid search for training data\n",
    "grid_search_model_6.fit(file_contents, targets)\n",
    "\n",
    "#save results of cross validation\n",
    "cv_results_model_6 = pd.DataFrame(grid_search_model_6.cv_results_)\n",
    "\n",
    "#filter columns to be kept in the dataframe\n",
    "filter_col = [col for col in cv_results_model_6 if (col.startswith(\"param_\") or col.startswith(\"mean_\") or col.startswith(\"rank\"))]\n",
    "\n",
    "#save results with only filtered columns\n",
    "cv_results_model_6 = cv_results_model_6[filter_col]\n",
    "\n",
    "#save name of the model for later comparison\n",
    "cv_results_model_6.insert(loc=0, column=\"Model\", value= \"6\")\n",
    "\n",
    "#round mean_test_score\n",
    "cv_results_model_6[\"mean_test_score\"] = cv_results_model_6[\"mean_test_score\"].round(2)\n",
    "\n",
    "#show best 5 sorted by mean_test_score\n",
    "display(cv_results_model_6.sort_values(by=\"mean_test_score\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-soldier",
   "metadata": {},
   "source": [
    "#### Model 7: Doc2Vec and Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-freight",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-26T11:07:33.643Z"
    }
   },
   "outputs": [],
   "source": [
    "#choose parameters for the different steps in the pipeline\n",
    "parameters_model_7 = {\n",
    "    \n",
    "    \n",
    "    #RF classifier\n",
    "    #nThe number of trees in the forest (default is 100)\n",
    "    \"clf__n_estimators\" : [10,100,200],\n",
    "    #The minimum number of samples required to split an internal node (default is 2 but it is a large dataset)\n",
    "    \"clf__min_samples_split\": [10, 40, 80],\n",
    "    #The number of features to consider when looking for the best split (default \"auto\" but sparse dataset)\n",
    "    'clf__max_features': [\"auto\", 10],\n",
    "    #maximum depth of the tree (default None)\n",
    "    'clf__max_depth': [10, None]\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "#build a pipeline\n",
    "model_7_pipeline = Pipeline([\n",
    "        #tfidf vectorizer\n",
    "        ('doc2vec', Doc2Vectorizer()),\n",
    "        #apply naive bayes\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "#design grid search\n",
    "grid_search_model_7 = GridSearchCV(\n",
    "    #pipeline to be followed\n",
    "    model_7_pipeline,\n",
    "    #parameters\n",
    "    param_grid=parameters_model_7,\n",
    "    #number of folds for CV\n",
    "    cv=5,\n",
    "    #scoring to be considered for the cv\n",
    "    scoring = \"accuracy\",\n",
    "    #parallelize if possible\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#fit the grid search for training data\n",
    "grid_search_model_7.fit(file_contents, targets)\n",
    "\n",
    "#save results of cross validation\n",
    "cv_results_model_7 = pd.DataFrame(grid_search_model_7.cv_results_)\n",
    "\n",
    "#filter columns to be kept in the dataframe\n",
    "filter_col = [col for col in cv_results_model_7 if (col.startswith(\"param_\") or col.startswith(\"mean_\") or col.startswith(\"rank\"))]\n",
    "\n",
    "#save results with only filtered columns\n",
    "cv_results_model_7 = cv_results_model_7[filter_col]\n",
    "\n",
    "#save name of the model for later comparison\n",
    "cv_results_model_7.insert(loc=0, column=\"Model\", value= \"7\")\n",
    "\n",
    "#round mean_test_score\n",
    "cv_results_model_7[\"mean_test_score\"] = cv_results_model_7[\"mean_test_score\"].round(2)\n",
    "\n",
    "#show best 5 sorted by mean_test_score\n",
    "display(cv_results_model_7.sort_values(by=\"mean_test_score\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-congo",
   "metadata": {},
   "source": [
    "### Compare CV results from trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-relief",
   "metadata": {},
   "source": [
    "In this section, the results from CV are compared within the trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-friendship",
   "metadata": {},
   "source": [
    "#### Raw results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-springfield",
   "metadata": {},
   "source": [
    "⚠️ Export results from grid serach. This allows us to experiment with visualiazations and results from CV without having to rerun the whole script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-lyric",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-26T11:07:33.651Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_results.to_csv(\"cv_results.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-franklin",
   "metadata": {},
   "source": [
    "A dataframe showing the best models according to the **mean accuracy within the test folds** used for cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-account",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-26T11:07:33.655Z"
    }
   },
   "outputs": [],
   "source": [
    "#merge cv results into 1 that keeps the relevant information\n",
    "\n",
    "#empty dataframe that will keep all the results\n",
    "cv_results = pd.DataFrame()\n",
    "\n",
    "#loop over cv results\n",
    "for i in [cv_results_model_1, cv_results_model_2, cv_results_model_3, cv_results_model_4,\n",
    "          cv_results_model_5, cv_results_model_6, cv_results_model_7]:\n",
    "    \n",
    "    #select relevant columns\n",
    "    selected = i[[\"Model\",\"mean_fit_time\",\"mean_score_time\",\"mean_test_score\"]]\n",
    "    \n",
    "    #append to cv results\n",
    "    cv_results = cv_results.append(selected)\n",
    "    \n",
    "#show models with best scores\n",
    "display(cv_results.sort_values(by=\"mean_test_score\", ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-klein",
   "metadata": {},
   "source": [
    "⚠️ *This table could be improved by also indicating the parameters used in each model but I thought it would be a bit overwhelming*\n",
    "\n",
    "⚠️⚠️ Models x y seem to achieve a better accuracy since they appear more often within the first positions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heated-diameter",
   "metadata": {},
   "source": [
    "#### Tradeoff score vs mean fit time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-parent",
   "metadata": {},
   "source": [
    "A plot to check if there is some kind of tradeoff between accuracy and runtime of the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-calendar",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-26T11:07:33.661Z"
    }
   },
   "outputs": [],
   "source": [
    "#group by model\n",
    "groups = cv_results.groupby(\"Model\")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "for name, group in groups:\n",
    "    ax.plot(group.mean_fit_time, group.mean_test_score, marker='o', linestyle='', ms=12, label=\"Model %s\" %name)\n",
    "ax.legend(loc = 1)\n",
    "plt.xlabel(\"Mean fit time\")\n",
    "plt.ylabel(\"Mean test Score (accuracy)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-paint",
   "metadata": {},
   "source": [
    "⚠️⚠️ Clear relation between runtime of the models and accuracy within test folds?\n",
    "\n",
    "⚠️⚠️ Some pre-processing or classifier takes more time to be run?\n",
    "\n",
    "⚠️⚠️ Further analysis on what it increases runtime of the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-venice",
   "metadata": {},
   "source": [
    "#### Best estimator from each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-nashville",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-26T11:07:33.667Z"
    }
   },
   "outputs": [],
   "source": [
    "#group by model and take best mean_test_score for each type of model\n",
    "best_models = cv_results.groupby(\"Model\")[[\"mean_test_score\"]].max()\n",
    "\n",
    "#plot\n",
    "plt.bar(best_models.index, best_models[\"mean_test_score\"])\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Mean_test_score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-legend",
   "metadata": {},
   "source": [
    "⚠️⚠️ As expected best models are x and y\n",
    "\n",
    "⚠️⚠️ Worse models are x and y\n",
    "\n",
    "⚠️⚠️ Are they stable to changes in hyperparameters? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-month",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-26T11:07:33.672Z"
    }
   },
   "outputs": [],
   "source": [
    "#mean of mean scores within the folds for each model\n",
    "stability = cv_results.groupby(\"Model\")[[\"mean_test_score\"]].mean()\n",
    "#standard deviation of the same\n",
    "stability[\"standard_deviation\"] = cv_results.groupby(\"Model\")[[\"mean_test_score\"]].std()\n",
    "\n",
    "#show\n",
    "stability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-lottery",
   "metadata": {},
   "source": [
    "⚠️⚠️ Model x is not very stable to the change of hyperparameters. What is influencing it so much?\n",
    "\n",
    "⚠️⚠️ Show an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-failure",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-26T11:07:33.676Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(cv_results_model_1[\"param_MNB__alpha\"], cv_results_model_1[\"mean_test_score\"], marker=\"o\")\n",
    "plt.xlabel(\"Parameter alpha\")\n",
    "plt.ylabel(\"Mean_test_score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-louisville",
   "metadata": {},
   "source": [
    "#### Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-andorra",
   "metadata": {},
   "source": [
    "This grid searches helped us to see which combinations of models and hyperparameters are expected to be the best, how stable they are and other insights. However, **we cannot draw strong conclusions on this since we are still dealing with train data.** This step is only helping us to understand the models better and choose the ones with which we want to test (or validate). To proceed further we decided to select **the best combination of hyperparameters for each of the 7 models, predict on test data**, evaluate and draw conclusions. That is done in the next section of this notebook. We know that this are not strictly the best 7 models (see raw results), but we wanted to include more diversity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-hearts",
   "metadata": {},
   "source": [
    "## Evaluation and model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-marks",
   "metadata": {},
   "source": [
    "**Predict on test data** using the best combinations of hyperparameters used in the models obtained in the training phase and evaluate using different metrics.\n",
    "\n",
    "⚠️ *This simulates predictions on unseen data. However, since it is done for many models and then we will choose the best model out of them, it behaves more like a validation set that would help us choose which model we would apply to actually unseen data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-roads",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-26T11:07:33.683Z"
    }
   },
   "outputs": [],
   "source": [
    "#add models to be evaluated\n",
    "models = [\n",
    "    grid_search_model_1,\n",
    "    grid_search_model_2,\n",
    "    grid_search_model_3,\n",
    "    grid_search_model_4,\n",
    "    grid_search_model_5,\n",
    "    grid_search_model_6,\n",
    "    grid_search_model_7\n",
    "]\n",
    "\n",
    "evaluation = pd.DataFrame(columns=[\"model\"\n",
    "                                   , \"mean_fit_time\", \"accuracy\"\n",
    "                                   , \"recall_macro\", \"recall_micro\"\n",
    "                                   , \"precision_macro\", \"precision_micro\"\n",
    "                                   , \"f1_macro\", \"f1_micro\"\n",
    "                                   , \"model_definition\"\n",
    "                                  ])\n",
    "\n",
    "i = -1 # Ensure that first item is index 0 in the loop\n",
    "for model_ in models:\n",
    "    # Yucky method of finding mean fit times:\n",
    "    i = i +1\n",
    "    mean_fit_time = cv_results.groupby(\"Model\")[\"mean_fit_time\"].mean()[i]\n",
    "    \n",
    "    # Predict\n",
    "    preds = model_.best_estimator_.predict(file_contents_test)\n",
    "    model = cv_results.iloc[model_.best_index_,0]\n",
    "\n",
    "    # Calculate metrix\n",
    "    to_append = [\n",
    "            \"Model \" + str(i+1),\n",
    "            mean_fit_time, \n",
    "            accuracy_score(y_true=targets_test,y_pred=preds),\n",
    "            #choose micro or macro according to criteria\n",
    "            recall_score(y_true=targets_test,y_pred=preds, average=\"macro\"),\n",
    "            recall_score(y_true=targets_test,y_pred=preds, average=\"micro\"),\n",
    "            precision_score(y_true=targets_test,y_pred=preds, average=\"macro\"),\n",
    "            precision_score(y_true=targets_test,y_pred=preds, average=\"micro\"),\n",
    "            f1_score(y_true=targets_test,y_pred=preds, average=\"macro\"),\n",
    "            f1_score(y_true=targets_test,y_pred=preds, average=\"micro\"),\n",
    "            model_\n",
    "            ]\n",
    "    \n",
    "    #Append Metrics\n",
    "    evaluation_length = len(evaluation)\n",
    "    evaluation.loc[evaluation_length] = to_append\n",
    "    \n",
    "    # Print results and Confusion Matrix for each model\n",
    "    print(\"####################################################################\")\n",
    "    print(\"####################################################################\")\n",
    "    print(\"                        Model \"+ str(i+1) + \":\")\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    \n",
    "    print(evaluation.loc[i, 'mean_fit_time':'f1_macro'])\n",
    "    print(\"\\n\")\n",
    "    print(\"Pipeline: \")\n",
    "    print(model_.best_estimator_)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"Confusion Matrix: \")\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    plot_confusion_matrix(estimator=model_.best_estimator_\n",
    "                          , X=file_contents_test\n",
    "                          , y_true=targets_test\n",
    "                          , ax=ax\n",
    "                         )\n",
    "    plt.show()\n",
    "    \n",
    "# Print table of the models compared and sorted:\n",
    "evaluation.sort_values(by=\"accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-multimedia",
   "metadata": {},
   "source": [
    "The **best estimator we found after everything is:** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-listing",
   "metadata": {},
   "source": [
    "Main **conclusion**: for unseen data, we would choose to use the estimator from above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-waters",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-niger",
   "metadata": {},
   "source": [
    "**Draw conclusions from above. but add more evaulations and stuff....**\n",
    "**Should probably be done after proper train/test is defined, and full model is tried**\n",
    "\n",
    "Some observations:\n",
    "* Model 1,2,3 (TFIDF) seems to work best overall, no matter training size\n",
    "* Model 5,6,7 (Doc2Vec) takes by far the most time, but their accuracy greatly increased when increasing training set\n",
    "\n",
    "\n",
    "As often is the case in the fields of science, not all research leads to useable results. We ended up having to remodel our plans several times during this project, including a complete pivot of the datasets.\n",
    "\n",
    "This did however give us some insight into how larger projects are managed. This also lead us to an interesting path of looking at a relatively obscure language.\n",
    "\n",
    "Although further works is possible, we reached the conclusion that there is a change in the Icelandic spoken language throughout time, and it is therefore possible to train models that estimates which decade a given speech is from. However, take into account what explained in section 3.2: it may be also due to other factors (for instance, topic used).\n",
    "\n",
    "Overall we did work with Data-Oriented Programming best practices. We were able to develop a scientific workflow. From the given data, we managed to train a model for prediction on test data with **decent results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-crash",
   "metadata": {},
   "source": [
    "# Further Works\n",
    "As we drilled down this dataset, we kept getting new ideas that we would like to experiment with, and try to gain better insight.\n",
    "Specifically, our next steps would be:\n",
    "\n",
    "## Predict Different Sources\n",
    "As it currently stands, we are trying to estimate a decade of speeches from \"Althingi\". However, the dataset has several other sources of Icelandic; both written and spoken (TV scripts, cinema and others).\n",
    "\n",
    "We would like to see if it was possible to extend our model to be able to classify the source.\n",
    "\n",
    "## Treating years as Contious Variables\n",
    "We are currently treating decades as a class. By discretizing results from a regression algorithm, we think it should be possible to keep some nominal knowledge of the ordering of the years, and thus improving our predictions. It would be also interesting to see if we can achieve also decent prediction by narrowing a bit the intervals for the years (instead of decades, lustrums). And of course, it would be interesting to rerun the model in a more powerful machine using all the decades instead of discarding the intermediate ones.\n",
    "\n",
    "## Gaining insight into Explanatory Variables\n",
    "From our results, it is clear that it is somewhat possible to predict the decades. However, we are still treating the algorithms as \"Black Boxes\". \n",
    "We would like to dive deeper into the decision treas/boundaries, to see if we can locate what it is that makes the predictions possible. It might be new words introduced, semantic changes, or something entirely different.\n",
    "\n",
    "\n",
    "## Additional Feature Extraction and Classifiers\n",
    "We would like to extend the list to include more classifiers, as well as trying to develop some additional feature extractions.\n",
    "E.g. \"Glove Embedding\"\n",
    "E.g. \"Neural networks\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "105px",
    "width": "242px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "421.771px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

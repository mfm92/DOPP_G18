{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction-to-Assignment\" data-toc-modified-id=\"Introduction-to-Assignment-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction to Assignment</a></span><ul class=\"toc-item\"><li><span><a href=\"#First-draft\" data-toc-modified-id=\"First-draft-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>First draft</a></span><ul class=\"toc-item\"><li><span><a href=\"#Topic-and-Questions-to-answer\" data-toc-modified-id=\"Topic-and-Questions-to-answer-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Topic and Questions to answer</a></span></li><li><span><a href=\"#Justification-For-Limit-Of-Scope\" data-toc-modified-id=\"Justification-For-Limit-Of-Scope-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Justification For Limit Of Scope</a></span></li><li><span><a href=\"#Workflow-plan-&amp;-Project-management\" data-toc-modified-id=\"Workflow-plan-&amp;-Project-management-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Workflow plan &amp; Project management</a></span></li><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-1.1.4\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;</span>Data</a></span></li></ul></li><li><span><a href=\"#Second-Draft\" data-toc-modified-id=\"Second-Draft-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Second Draft</a></span></li><li><span><a href=\"#Pivoting-Point\" data-toc-modified-id=\"Pivoting-Point-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Pivoting Point</a></span></li><li><span><a href=\"#Language-change-in-Icelandic-Parliamentary-Speeches\" data-toc-modified-id=\"Language-change-in-Icelandic-Parliamentary-Speeches-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Language change in Icelandic Parliamentary Speeches</a></span></li></ul></li><li><span><a href=\"#Estimating-publication-year-from-Project-Gutenberg\" data-toc-modified-id=\"Estimating-publication-year-from-Project-Gutenberg-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Estimating publication year from Project Gutenberg</a></span><ul class=\"toc-item\"><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-packages\" data-toc-modified-id=\"Import-packages-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Import packages</a></span></li><li><span><a href=\"#Define-Constants\" data-toc-modified-id=\"Define-Constants-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Define Constants</a></span></li></ul></li><li><span><a href=\"#Importing-the-data\" data-toc-modified-id=\"Importing-the-data-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Importing the data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Getting-the-content\" data-toc-modified-id=\"Getting-the-content-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Getting the content</a></span></li><li><span><a href=\"#Data-Cleansing\" data-toc-modified-id=\"Data-Cleansing-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Data Cleansing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Read-a-single-file\" data-toc-modified-id=\"Read-a-single-file-2.2.2.1\"><span class=\"toc-item-num\">2.2.2.1&nbsp;&nbsp;</span>Read a single file</a></span></li><li><span><a href=\"#Return-list-of-all-words\" data-toc-modified-id=\"Return-list-of-all-words-2.2.2.2\"><span class=\"toc-item-num\">2.2.2.2&nbsp;&nbsp;</span>Return list of all words</a></span></li></ul></li></ul></li><li><span><a href=\"#Statistics\" data-toc-modified-id=\"Statistics-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Statistics</a></span><ul class=\"toc-item\"><li><span><a href=\"#First-attempt\" data-toc-modified-id=\"First-attempt-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>First attempt</a></span></li><li><span><a href=\"#Read-all-files,-and-do-preprocessing\" data-toc-modified-id=\"Read-all-files,-and-do-preprocessing-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Read all files, and do preprocessing</a></span></li><li><span><a href=\"#Compare-Word-ranking-between-titles\" data-toc-modified-id=\"Compare-Word-ranking-between-titles-2.3.3\"><span class=\"toc-item-num\">2.3.3&nbsp;&nbsp;</span>Compare Word ranking between titles</a></span></li></ul></li><li><span><a href=\"#Second-testing\" data-toc-modified-id=\"Second-testing-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Second testing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Read-in-from-the-decades-files,-and-see-the-distributions\" data-toc-modified-id=\"Read-in-from-the-decades-files,-and-see-the-distributions-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Read in from the decades files, and see the distributions</a></span></li><li><span><a href=\"#Preliminary-Conclusion\" data-toc-modified-id=\"Preliminary-Conclusion-2.4.2\"><span class=\"toc-item-num\">2.4.2&nbsp;&nbsp;</span>Preliminary Conclusion</a></span></li><li><span><a href=\"#Compare-ranking-between-upload-decades\" data-toc-modified-id=\"Compare-ranking-between-upload-decades-2.4.3\"><span class=\"toc-item-num\">2.4.3&nbsp;&nbsp;</span>Compare ranking between upload-decades</a></span></li></ul></li><li><span><a href=\"#Trying-to-fit-models-to-predict\" data-toc-modified-id=\"Trying-to-fit-models-to-predict-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Trying to fit models to predict</a></span><ul class=\"toc-item\"><li><span><a href=\"#Read-in-files\" data-toc-modified-id=\"Read-in-files-2.5.1\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;</span>Read in files</a></span></li><li><span><a href=\"#Train-models\" data-toc-modified-id=\"Train-models-2.5.2\"><span class=\"toc-item-num\">2.5.2&nbsp;&nbsp;</span>Train models</a></span></li></ul></li><li><span><a href=\"#Realisation-and-conclusion\" data-toc-modified-id=\"Realisation-and-conclusion-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Realisation and conclusion</a></span></li></ul></li><li><span><a href=\"#Studying-language-change-in-Icelandic-parliamentary-speeches\" data-toc-modified-id=\"Studying-language-change-in-Icelandic-parliamentary-speeches-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Studying language change in Icelandic parliamentary speeches</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#The-task\" data-toc-modified-id=\"The-task-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>The task</a></span></li><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-required-libraries\" data-toc-modified-id=\"Load-required-libraries-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Load required libraries</a></span></li><li><span><a href=\"#Get-the-data\" data-toc-modified-id=\"Get-the-data-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Get the data</a></span></li><li><span><a href=\"#Preprocessing-helpers\" data-toc-modified-id=\"Preprocessing-helpers-3.3.3\"><span class=\"toc-item-num\">3.3.3&nbsp;&nbsp;</span>Preprocessing helpers</a></span></li></ul></li><li><span><a href=\"#Preliminary-Data-Analysis\" data-toc-modified-id=\"Preliminary-Data-Analysis-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Preliminary Data Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Zipf's-Law\" data-toc-modified-id=\"Zipf's-Law-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>Zipf's Law</a></span></li><li><span><a href=\"#Disappearing-words-/-new-words\" data-toc-modified-id=\"Disappearing-words-/-new-words-3.4.2\"><span class=\"toc-item-num\">3.4.2&nbsp;&nbsp;</span>Disappearing words / new words</a></span></li><li><span><a href=\"#Development-of-average-sentence-length\" data-toc-modified-id=\"Development-of-average-sentence-length-3.4.3\"><span class=\"toc-item-num\">3.4.3&nbsp;&nbsp;</span>Development of average sentence length</a></span></li><li><span><a href=\"#n-grams\" data-toc-modified-id=\"n-grams-3.4.4\"><span class=\"toc-item-num\">3.4.4&nbsp;&nbsp;</span>n-grams</a></span></li></ul></li><li><span><a href=\"#Building-model-for-classifying-speeches\" data-toc-modified-id=\"Building-model-for-classifying-speeches-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Building model for classifying speeches</a></span><ul class=\"toc-item\"><li><span><a href=\"#Constructing-training-and-test-data\" data-toc-modified-id=\"Constructing-training-and-test-data-3.5.1\"><span class=\"toc-item-num\">3.5.1&nbsp;&nbsp;</span>Constructing training and test data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Train-data\" data-toc-modified-id=\"Train-data-3.5.1.1\"><span class=\"toc-item-num\">3.5.1.1&nbsp;&nbsp;</span>Train data</a></span></li><li><span><a href=\"#Test-data\" data-toc-modified-id=\"Test-data-3.5.1.2\"><span class=\"toc-item-num\">3.5.1.2&nbsp;&nbsp;</span>Test data</a></span></li><li><span><a href=\"#See-classes-distribution-within-train-and-test-sets\" data-toc-modified-id=\"See-classes-distribution-within-train-and-test-sets-3.5.1.3\"><span class=\"toc-item-num\">3.5.1.3&nbsp;&nbsp;</span>See classes distribution within train and test sets</a></span></li></ul></li><li><span><a href=\"#Text-feature-extraction\" data-toc-modified-id=\"Text-feature-extraction-3.5.2\"><span class=\"toc-item-num\">3.5.2&nbsp;&nbsp;</span>Text feature extraction</a></span><ul class=\"toc-item\"><li><span><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-3.5.2.1\"><span class=\"toc-item-num\">3.5.2.1&nbsp;&nbsp;</span>TF-IDF</a></span></li><li><span><a href=\"#Word2Vec\" data-toc-modified-id=\"Word2Vec-3.5.2.2\"><span class=\"toc-item-num\">3.5.2.2&nbsp;&nbsp;</span>Word2Vec</a></span></li><li><span><a href=\"#Doc2Vec\" data-toc-modified-id=\"Doc2Vec-3.5.2.3\"><span class=\"toc-item-num\">3.5.2.3&nbsp;&nbsp;</span>Doc2Vec</a></span></li></ul></li><li><span><a href=\"#Classifiers\" data-toc-modified-id=\"Classifiers-3.5.3\"><span class=\"toc-item-num\">3.5.3&nbsp;&nbsp;</span>Classifiers</a></span><ul class=\"toc-item\"><li><span><a href=\"#Multinominal-Naive-Bayes-(MNB)\" data-toc-modified-id=\"Multinominal-Naive-Bayes-(MNB)-3.5.3.1\"><span class=\"toc-item-num\">3.5.3.1&nbsp;&nbsp;</span>Multinominal Naive Bayes (MNB)</a></span></li><li><span><a href=\"#Support-Vector-Machines\" data-toc-modified-id=\"Support-Vector-Machines-3.5.3.2\"><span class=\"toc-item-num\">3.5.3.2&nbsp;&nbsp;</span>Support Vector Machines</a></span></li><li><span><a href=\"#Random-Forest-Classifier\" data-toc-modified-id=\"Random-Forest-Classifier-3.5.3.3\"><span class=\"toc-item-num\">3.5.3.3&nbsp;&nbsp;</span>Random Forest Classifier</a></span></li></ul></li><li><span><a href=\"#Train-models\" data-toc-modified-id=\"Train-models-3.5.4\"><span class=\"toc-item-num\">3.5.4&nbsp;&nbsp;</span>Train models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model-1:-TFIDF-vectorizer,-select-K-best-and-Multinomial-Naive-Bayes\" data-toc-modified-id=\"Model-1:-TFIDF-vectorizer,-select-K-best-and-Multinomial-Naive-Bayes-3.5.4.1\"><span class=\"toc-item-num\">3.5.4.1&nbsp;&nbsp;</span>Model 1: TFIDF vectorizer, select K best and Multinomial Naive Bayes</a></span></li><li><span><a href=\"#Model-2:-TFIDF-vectorizer,-select-K-best-and-SVC\" data-toc-modified-id=\"Model-2:-TFIDF-vectorizer,-select-K-best-and-SVC-3.5.4.2\"><span class=\"toc-item-num\">3.5.4.2&nbsp;&nbsp;</span>Model 2: TFIDF vectorizer, select K best and SVC</a></span></li><li><span><a href=\"#Model-3:-TFIDF-vectorizer,-select-K-best-and-Random-Forest-Classifier\" data-toc-modified-id=\"Model-3:-TFIDF-vectorizer,-select-K-best-and-Random-Forest-Classifier-3.5.4.3\"><span class=\"toc-item-num\">3.5.4.3&nbsp;&nbsp;</span>Model 3: TFIDF vectorizer, select K best and Random Forest Classifier</a></span></li><li><span><a href=\"#Model-4:-Word2Vec-and-SVC\" data-toc-modified-id=\"Model-4:-Word2Vec-and-SVC-3.5.4.4\"><span class=\"toc-item-num\">3.5.4.4&nbsp;&nbsp;</span>Model 4: Word2Vec and SVC</a></span></li><li><span><a href=\"#Model-5:-Word2Vec-and-Random-Forest-Classifier\" data-toc-modified-id=\"Model-5:-Word2Vec-and-Random-Forest-Classifier-3.5.4.5\"><span class=\"toc-item-num\">3.5.4.5&nbsp;&nbsp;</span>Model 5: Word2Vec and Random Forest Classifier</a></span></li><li><span><a href=\"#Model-6:-Doc2Vec-and-Support-Vector-Machines\" data-toc-modified-id=\"Model-6:-Doc2Vec-and-Support-Vector-Machines-3.5.4.6\"><span class=\"toc-item-num\">3.5.4.6&nbsp;&nbsp;</span>Model 6: Doc2Vec and Support Vector Machines</a></span></li><li><span><a href=\"#Model-7:-Doc2Vec-and-Random-Forest-Classifier\" data-toc-modified-id=\"Model-7:-Doc2Vec-and-Random-Forest-Classifier-3.5.4.7\"><span class=\"toc-item-num\">3.5.4.7&nbsp;&nbsp;</span>Model 7: Doc2Vec and Random Forest Classifier</a></span></li></ul></li><li><span><a href=\"#Compare-CV-results-from-trained-models\" data-toc-modified-id=\"Compare-CV-results-from-trained-models-3.5.5\"><span class=\"toc-item-num\">3.5.5&nbsp;&nbsp;</span>Compare CV results from trained models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Raw-results\" data-toc-modified-id=\"Raw-results-3.5.5.1\"><span class=\"toc-item-num\">3.5.5.1&nbsp;&nbsp;</span>Raw results</a></span></li><li><span><a href=\"#Tradeoff-score-vs-mean-fit-time\" data-toc-modified-id=\"Tradeoff-score-vs-mean-fit-time-3.5.5.2\"><span class=\"toc-item-num\">3.5.5.2&nbsp;&nbsp;</span>Tradeoff score vs mean fit time</a></span></li><li><span><a href=\"#Best-estimator-from-each-model\" data-toc-modified-id=\"Best-estimator-from-each-model-3.5.5.3\"><span class=\"toc-item-num\">3.5.5.3&nbsp;&nbsp;</span>Best estimator from each model</a></span></li><li><span><a href=\"#Next-steps\" data-toc-modified-id=\"Next-steps-3.5.5.4\"><span class=\"toc-item-num\">3.5.5.4&nbsp;&nbsp;</span>Next steps</a></span></li></ul></li></ul></li><li><span><a href=\"#Evaluation-and-model-selection\" data-toc-modified-id=\"Evaluation-and-model-selection-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Evaluation and model selection</a></span></li></ul></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Conclusion</a></span></li><li><span><a href=\"#Further-Works\" data-toc-modified-id=\"Further-Works-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Further Works</a></span><ul class=\"toc-item\"><li><span><a href=\"#Predict-Different-Sources\" data-toc-modified-id=\"Predict-Different-Sources-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Predict Different Sources</a></span></li><li><span><a href=\"#Treating-years-as-Contious-Variables\" data-toc-modified-id=\"Treating-years-as-Contious-Variables-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Treating years as Contious Variables</a></span></li><li><span><a href=\"#Gaining-insight-into-Explanatory-Variables\" data-toc-modified-id=\"Gaining-insight-into-Explanatory-Variables-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Gaining insight into Explanatory Variables</a></span></li><li><span><a href=\"#Additional-Feature-Extraction-and-Classifiers\" data-toc-modified-id=\"Additional-Feature-Extraction-and-Classifiers-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Additional Feature Extraction and Classifiers</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the third Exercise of **188.995 Data-Oriented Programming Paradigms**\n",
    "\n",
    "We are group 18, and consist of:\n",
    "* Guillermo Alamán Requena, Matr. Nr: 11937906\n",
    "* Michael Ferdinand Moser, Matr. Nr: 01123077 \n",
    "* Paul Joe Maliakel, Matr. Nr: 12012422\n",
    "* Gunnar Sjúrðarson Knudsen, Matr. Nr: 12028205\n",
    "\n",
    "In this task we were asked to choose one vaguely worded question, and then narrow the scope, figuring out how to get the data, before finally solving the question at hand.\n",
    "We chose **Question 21**, which contains:\n",
    "* How does the use of various communication languages in countries change over time?\n",
    "* Which languages grow and which disappear,  and what are their characteristics?\n",
    "* Are there other factors that correlate with the appearance or disappearance of languages?\n",
    "\n",
    "\n",
    "We soon realized that the question as stated is far too broad, and we therefore had to limit it.\n",
    "\n",
    "After having discussed amoung our groups, we came to the following plan:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First draft\n",
    "### Topic and Questions to answer\n",
    "We've selected question 21, which is regarding how communication languages in countries change over time.\n",
    "\n",
    "After having discussed the data available, and planned a workflow, we've decided to try to answer the questions:\n",
    "* How has the English language changed in the past 100 years based on word frequencies, sentence length, ...?\n",
    "* Can we find parallel developments between different genres of text?\n",
    "* Can the publication year of a movie/article/whatever be predicted based on the text and its characteristics?\n",
    "\n",
    "\n",
    "### Justification For Limit Of Scope\n",
    "The sample questions stated in the task description are too broad, to be answered in a single 160 hour project.\n",
    "* Lot's of issues, such as: \n",
    "* Lack of census data; \n",
    "* other changes such as phonetic, semantic and syntactic meanings; \n",
    "* High correlation with e.g:\n",
    "    * country population\n",
    "    * age of speakers\n",
    "    * ...\n",
    "* What counts as a language? \n",
    "    * dialect? \n",
    "    * Mutually Intelligible?\n",
    "* Political dimensions\n",
    "* Multilingual people\n",
    "* How do we check accuracy of the available data?\n",
    "* ...\n",
    "\n",
    "\n",
    "Historical data for language use is likely not available for most languages, as it's topics for great research to estimate merely historical populations - especially before 1850 or so. \n",
    "The evolution of languages are much less documented. \n",
    "Lack of census data overall, but other changes are even harder to gauge, such as phonetic, semantic, and syntactic meanings. Highly correlated with population of countries, but also with \"hidden\" correlations, such as age of speakers, ... \n",
    "Even dead languages can be revived. \n",
    "\n",
    "What constitutes a language? Dialect? Mutually Intelligible? Also do not forget the political dimension, e.g. Croatian/Serbian really are just dialects of the same language but they want to keep separate. On the other end of this scheme the variant of Chinese spoken in Beijing may be drastically different from the Chinese spoken in other regions of the country, but still falls under the same \"Chinese\" umbrella to communicate unity. \n",
    "\n",
    "How much is spoken? Should we consider people who studied a language as their second, third... language? If so, how well should be the command over the language for the person to count? A1/B1/C2 level?\n",
    "%How do we check accuracy of the available data?\n",
    "\n",
    "### Workflow plan & Project management\n",
    "* Outline the plan\n",
    "    * Get, understand and clean data: articles/movie scripts/video transcripts over the years (see next section)\n",
    "    * Train-test split: keeping proportion of publication years within the splits.\n",
    "    * Preprocessing: text feature extraction, feature selection, scaling, etc. (Come back here if necessary)\n",
    "    * Visualization: evolution of words over the years, word-clouds and other relevant characteristics.\n",
    "    * Define evaluation metrics, train different models/parameters using CV and select best one for predictions.\n",
    "    * Predict, conclude, report and publish notebook in Kaggle Kernel.\n",
    "* How the work will be divided up between group members\n",
    "    * Acquisition, cleaning and prepossessing of the data will be done commonly.\n",
    "    * Each member of the group will train a model and report results using same evaluation metrics. \n",
    "    * Jointly choose the best model and conclude.\n",
    "    * Presentation, report and publishing will be also split. \n",
    "* Timeline: To be defined after review meeting    \n",
    "\n",
    "\n",
    "### Data\n",
    "\n",
    "Our goal is to get a dataset similar to:\n",
    "\n",
    "\n",
    "\n",
    "| **Corpus** | **Year Published** | **Type**               | ... |\n",
    "|------------|--------------------|------------------------|-----|\n",
    "| Text1      | 1976               | News                   | ... |\n",
    "| Text2      | 1976               | Movie Script           | ... |\n",
    "| ...        | ...                | ...                    | ... |\n",
    "| TextN      | 2009               | Scientific Article     | ... |\n",
    "\n",
    "Feature extraction from texts will be performed to obtain appropriate features for modeling. To build a dataset like this one, we will rely on the following kind sources: \n",
    "\n",
    "* \\url{https://www.kaggle.com/asad1m9a9h6mood/news-articles} - News articles from 2015 until date.\n",
    "* \\url{https://www.kaggle.com/snapcrack/all-the-news} - 143000 articles from 15 American Publications. \n",
    "* NLTK\n",
    "* ...\n",
    "\n",
    "\n",
    "## Second Draft\n",
    "After having a preliminary meeting with Univ.Prof. Dr. Hanbury and Dipl.-Ing. Dr. Piroi, who gave great input, we decided to further limit out goal to only use Project Gutenberg as a datasource, and setting our hypothesis to see whether it was possible to generate a model that predicted the publication year/decade for a set of books.\n",
    "\n",
    "## Pivoting Point\n",
    "After having done a decent portion of work, we reached to the conclusion that our dataset was not suitable to solve the question we had original set out, and we were forced to pivot.\n",
    "\n",
    "We discussed whether we wanted to change the goal from classifying, but as we were all quite interrested in a classification algorithm, and wanted to do proper NLP, we instead searched for another dataset.\n",
    "\n",
    "## Language change in Icelandic Parliamentary Speeches\n",
    "We found the dataset with all icelandic parliamentary speeches going back a century. This is further described in section 3. \n",
    "With this great dataset, our goal was to develop a model that could try to predict which decade a speech is from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating publication year from Project Gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was the attempt at our first hypothesis. \n",
    "We import a large corpus of books from Project Gutenberg, and cleanse the data, so it's ready for machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We start by setting up all packages needed for the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from builtins import str\n",
    "import os\n",
    "from six import u\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from operator import itemgetter    \n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "import random\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import math\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Constants\n",
    "Constant that are used in this part is also set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"processedData\"\n",
    "\n",
    "TEXT_START_MARKERS = frozenset((u(_) for _ in (\n",
    "    \"*END*THE SMALL PRINT\",\n",
    "    \"*** START OF THE PROJECT GUTENBERG\",\n",
    "    \"*** START OF THIS PROJECT GUTENBERG\",\n",
    "    \"This etext was prepared by\",\n",
    "    \"E-text prepared by\",\n",
    "    \"Produced by\",\n",
    "    \"Distributed Proofreading Team\",\n",
    "    \"Proofreading Team at http://www.pgdp.net\",\n",
    "    \"http://gallica.bnf.fr)\",\n",
    "    \"      http://archive.org/details/\",\n",
    "    \"http://www.pgdp.net\",\n",
    "    \"by The Internet Archive)\",\n",
    "    \"by The Internet Archive/Canadian Libraries\",\n",
    "    \"by The Internet Archive/American Libraries\",\n",
    "    \"public domain material from the Internet Archive\",\n",
    "    \"Internet Archive)\",\n",
    "    \"Internet Archive/Canadian Libraries\",\n",
    "    \"Internet Archive/American Libraries\",\n",
    "    \"material from the Google Print project\",\n",
    "    \"*END THE SMALL PRINT\",\n",
    "    \"***START OF THE PROJECT GUTENBERG\",\n",
    "    \"This etext was produced by\",\n",
    "    \"*** START OF THE COPYRIGHTED\",\n",
    "    \"The Project Gutenberg\",\n",
    "    \"http://gutenberg.spiegel.de/ erreichbar.\",\n",
    "    \"Project Runeberg publishes\",\n",
    "    \"Beginning of this Project Gutenberg\",\n",
    "    \"Project Gutenberg Online Distributed\",\n",
    "    \"Gutenberg Online Distributed\",\n",
    "    \"the Project Gutenberg Online Distributed\",\n",
    "    \"Project Gutenberg TEI\",\n",
    "    \"This eBook was prepared by\",\n",
    "    \"http://gutenberg2000.de erreichbar.\",\n",
    "    \"This Etext was prepared by\",\n",
    "    \"This Project Gutenberg Etext was prepared by\",\n",
    "    \"Gutenberg Distributed Proofreaders\",\n",
    "    \"Project Gutenberg Distributed Proofreaders\",\n",
    "    \"the Project Gutenberg Online Distributed Proofreading Team\",\n",
    "    \"**The Project Gutenberg\",\n",
    "    \"*SMALL PRINT!\",\n",
    "    \"More information about this book is at the top of this file.\",\n",
    "    \"tells you about restrictions in how the file may be used.\",\n",
    "    \"l'authorization à les utilizer pour preparer ce texte.\",\n",
    "    \"of the etext through OCR.\",\n",
    "    \"*****These eBooks Were Prepared By Thousands of Volunteers!*****\",\n",
    "    \"We need your donations more than ever!\",\n",
    "    \" *** START OF THIS PROJECT GUTENBERG\",\n",
    "    \"****     SMALL PRINT!\",\n",
    "    '[\"Small Print\" V.',\n",
    "    '      (http://www.ibiblio.org/gutenberg/',\n",
    "    'and the Project Gutenberg Online Distributed Proofreading Team',\n",
    "    'Mary Meehan, and the Project Gutenberg Online Distributed Proofreading',\n",
    "    '                this Project Gutenberg edition.',\n",
    ")))\n",
    "\n",
    "\n",
    "TEXT_END_MARKERS = frozenset((u(_) for _ in (\n",
    "    \"*** END OF THE PROJECT GUTENBERG\",\n",
    "    \"*** END OF THIS PROJECT GUTENBERG\",\n",
    "    \"***END OF THE PROJECT GUTENBERG\",\n",
    "    \"End of the Project Gutenberg\",\n",
    "    \"End of The Project Gutenberg\",\n",
    "    \"Ende dieses Project Gutenberg\",\n",
    "    \"by Project Gutenberg\",\n",
    "    \"End of Project Gutenberg\",\n",
    "    \"End of this Project Gutenberg\",\n",
    "    \"Ende dieses Projekt Gutenberg\",\n",
    "    \"        ***END OF THE PROJECT GUTENBERG\",\n",
    "    \"*** END OF THE COPYRIGHTED\",\n",
    "    \"End of this is COPYRIGHTED\",\n",
    "    \"Ende dieses Etextes \",\n",
    "    \"Ende dieses Project Gutenber\",\n",
    "    \"Ende diese Project Gutenberg\",\n",
    "    \"**This is a COPYRIGHTED Project Gutenberg Etext, Details Above**\",\n",
    "    \"Fin de Project Gutenberg\",\n",
    "    \"The Project Gutenberg Etext of \",\n",
    "    \"Ce document fut presente en lecture\",\n",
    "    \"Ce document fut présenté en lecture\",\n",
    "    \"More information about this book is at the top of this file.\",\n",
    "    \"We need your donations more than ever!\",\n",
    "    \"END OF PROJECT GUTENBERG\",\n",
    "    \" End of the Project Gutenberg\",\n",
    "    \" *** END OF THIS PROJECT GUTENBERG\",\n",
    ")))\n",
    "\n",
    "\n",
    "LEGALESE_START_MARKERS = frozenset((u(_) for _ in (\n",
    "    \"<<THIS ELECTRONIC VERSION OF\",\n",
    ")))\n",
    "\n",
    "\n",
    "LEGALESE_END_MARKERS = frozenset((u(_) for _ in (\n",
    "    \"SERVICE THAT CHARGES FOR DOWNLOAD\",\n",
    ")))\n",
    "\n",
    "TITLE_MARKERS = frozenset((u(_) for _ in (\n",
    "    \"Title:\",\n",
    ")))\n",
    "\n",
    "AUTHOR_MARKERS = frozenset((u(_) for _ in (\n",
    "    \"Author:\",\n",
    ")))\n",
    "DATE_MARKERS = frozenset((u(_) for _ in (\n",
    "    \"Release Date:\",\"Release Date:\"\n",
    ")))\n",
    "LANGUAGE_MARKERS = frozenset((u(_) for _ in (\n",
    "    \"Language:\",\n",
    ")))\n",
    "ENCODING_MARKERS = frozenset((u(_) for _ in (\n",
    "    \"Character set encoding:\",\n",
    ")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very rough first draft at importing and cleansing the data. \n",
    "Solution is heavily inspired by https://gist.github.com/mbforbes/cee3fd5bb3a797b059524fe8c8ccdc2b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by downloading the repository of (english) books. This is done in bash. Only tested on Ubuntu, but mac should work the same\n",
    "\n",
    "```\n",
    "wget -m -H -nd \"http://www.gutenberg.org/robot/harvest?filetypes[]=txt&langs[]=en\"\n",
    "\n",
    "                http://www.gutenberg.org/robot/harvest?offset=40532&filetypes[]=txt&langs[]=en\n",
    "```\n",
    "Takes a few hours to run, and is stored in a folder called rawContent. \n",
    "This is then copied to another folder, and we can start to clean up the mess\n",
    "\n",
    "First we delete some dublications of the same books:\n",
    "```\n",
    "ls | grep \"\\-8.zip\" | xargs rm\n",
    "ls | grep \"\\-0.zip\" | xargs rm\n",
    "```\n",
    "We can then unzip the files, and remove the zip files\n",
    "```\n",
    "unzip \"*zip\"\n",
    "rm *.zip\n",
    "```\n",
    "\n",
    "Next we take care of some nested foldering\n",
    "```\n",
    "mv */*.txt ./\n",
    "```\n",
    "And finally, we remove all rubbish that isn't a real book:\n",
    "\n",
    "```\n",
    "ls | grep -v \"\\.txt\" | xargs rm -rf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleansing\n",
    "As the data is not given in a computer-friendly format, a lot of string operations are needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "    file = open(file_name, encoding=\"ISO-8859-1\")\n",
    "    file_content = file.read()\n",
    "\n",
    "    lines = file_content.splitlines()\n",
    "    sep = str(os.linesep)\n",
    "\n",
    "    # Initialize results for single book\n",
    "    content_lines = []\n",
    "    i = 0\n",
    "    footer_found = False\n",
    "    ignore_section = False\n",
    "\n",
    "    title = \"\"\n",
    "    author = \"\"\n",
    "    date = \"\"\n",
    "    language = \"\"\n",
    "    encoding = \"\"\n",
    "    year = 0\n",
    "\n",
    "    # Reset flags for each book\n",
    "    title_found = False\n",
    "    author_found = False\n",
    "    date_found = False\n",
    "    language_found = False\n",
    "    encoding_found = False\n",
    "\n",
    "    for line in lines:\n",
    "            reset = False\n",
    "\n",
    "            #print(line)\n",
    "            if i <= 600:\n",
    "                # Shamelessly stolen\n",
    "                if any(line.startswith(token) for token in TEXT_START_MARKERS):\n",
    "                    reset = True\n",
    "\n",
    "                # Extract Metadata\n",
    "                if title_found == False:\n",
    "                    if any(line.startswith(token) for token in TITLE_MARKERS):\n",
    "                        title_found = True\n",
    "                        title = line\n",
    "                if author_found == False:\n",
    "                    if any(line.startswith(token) for token in AUTHOR_MARKERS):\n",
    "                        author_found = True\n",
    "                        author = line\n",
    "                if date_found == False:\n",
    "                    if any(line.startswith(token) for token in DATE_MARKERS):\n",
    "                        date_found = True\n",
    "                        date = line\n",
    "                        year = int(re.findall(r'\\d{4}', date)[0])\n",
    "                if language_found == False:\n",
    "                    if any(line.startswith(token) for token in LANGUAGE_MARKERS):\n",
    "                        language_found = True\n",
    "                        language = line\n",
    "                if encoding_found == False:\n",
    "                    if any(line.startswith(token) for token in ENCODING_MARKERS):\n",
    "                        encoding_found = True\n",
    "                        encoding = line\n",
    "\n",
    "                # More theft from above\n",
    "                if reset:\n",
    "                    content_lines = []\n",
    "                    continue\n",
    "\n",
    "            # I feel like a criminal by now. Guess what? Also stolen\n",
    "            if i >= 100:\n",
    "                if any(line.startswith(token) for token in TEXT_END_MARKERS):\n",
    "                    footer_found = True\n",
    "\n",
    "                if footer_found:\n",
    "                    break\n",
    "\n",
    "            if any(line.startswith(token) for token in LEGALESE_START_MARKERS):\n",
    "                ignore_section = True\n",
    "                continue\n",
    "            elif any(line.startswith(token) for token in LEGALESE_END_MARKERS):\n",
    "                ignore_section = False\n",
    "                continue\n",
    "\n",
    "            if not ignore_section:\n",
    "                if line != \"\": # Screw the blank lines\n",
    "                    content_lines.append(line.rstrip(sep))\n",
    "                i += 1\n",
    "\n",
    "            sep.join(content_lines)\n",
    "\n",
    "    # Do more cleaning\n",
    "    for token in TITLE_MARKERS:\n",
    "        title = title.replace(token, '').lstrip().rstrip()\n",
    "    for token in AUTHOR_MARKERS:\n",
    "        author = author.replace(token, '').lstrip().rstrip()\n",
    "    for token in LANGUAGE_MARKERS:\n",
    "        language = language.replace(token, '').lstrip().rstrip()\n",
    "    for token in DATE_MARKERS:\n",
    "        date = date.replace(token, '').lstrip().rstrip()\n",
    "    for token in ENCODING_MARKERS:\n",
    "        encoding = encoding.replace(token, '').lstrip().rstrip()\n",
    "    return title, author, date, year, language, encoding, content_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Return list of all words\n",
    "Currently quite an empty function. However, I assume that some cleaning of cases etc. will be done here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(content_lines):\n",
    "    all_text_lower = \" \".join(content_lines).lower()\n",
    "    words = re.findall(r'(\\b[A-Za-z][a-z]{2,9}\\b)', all_text_lower)\n",
    "\n",
    "    # Do more cleansing. E.g. cases and stuff\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics\n",
    "We start by doing some exploratory data analysis, to see how well our scraping works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First attempt\n",
    "Trying a simple word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_frequencies(words):\n",
    "    frequency = {}\n",
    "    for word in words:\n",
    "        count = frequency.get(word,0)\n",
    "        frequency[word] = count + 1\n",
    "\n",
    "    word_count = len(words)\n",
    "    unique_word_count = 0\n",
    "    word_list = []\n",
    "    word_list_count = []\n",
    "    for key, value in reversed(sorted(frequency.items(), key = itemgetter(1))):\n",
    "        word_list.append(key)\n",
    "        word_list_count.append(value)\n",
    "        unique_word_count = unique_word_count + 1\n",
    "    \n",
    "    word_list_freq = [freq / word_count for freq in word_list_count]\n",
    "    \n",
    "    word_freq = pd.DataFrame(list(zip(word_list, word_list_count, word_list_freq))\n",
    "                             , columns = ['Word', 'count', 'freq'])\n",
    "    \n",
    "    word_freq['rank'] = word_freq['count'].rank(ascending = False, method=\"dense\")\n",
    "\n",
    "    return(word_freq, unique_word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read all files, and do preprocessing\n",
    "Well... Only ten files currently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all filenames\n",
    "files = [f for f in listdir(file_path) if isfile(join(file_path, f))]\n",
    "files = list(filter(lambda file: file[0].isdigit(), files))\n",
    "random.shuffle(files)\n",
    "\n",
    "\n",
    "# Do only subset\n",
    "files = files[0:10]\n",
    "\n",
    "list_of_file = []\n",
    "list_of_title = []\n",
    "list_of_author = []\n",
    "list_of_date = []\n",
    "list_of_year = []\n",
    "list_of_language = []\n",
    "list_of_encoding = []\n",
    "list_of_word_count = []\n",
    "list_of_unique_word_count = []\n",
    "list_of_word_frequencies = []\n",
    "iter_ = 0\n",
    "\n",
    "for file in files:\n",
    "    # Read in basic information from file\n",
    "    title, author, date, year, language, encoding, content_lines = read_file(file_path + \"/\" + file)\n",
    "    line_count = len(content_lines)\n",
    "\n",
    "    # Not sure if we want this for later:\n",
    "    #content_all = \" \".join(content_lines)\n",
    "    \n",
    "    # Split into words (and do various cleaning)\n",
    "    words = get_words(content_lines)\n",
    "    word_count = len(words)\n",
    "\n",
    "    # First analysis, but should do something proper\n",
    "    word_frequencies_table, unique_word_count = get_word_frequencies(words)\n",
    "    \n",
    "    # Append to results\n",
    "    list_of_file.append(file)\n",
    "    list_of_title.append(title)\n",
    "    list_of_author.append(author)\n",
    "    list_of_date.append(date)\n",
    "    list_of_year.append(year)\n",
    "    list_of_language.append(language)\n",
    "    list_of_encoding.append(encoding)\n",
    "    list_of_word_count.append(word_count)\n",
    "    list_of_unique_word_count.append(unique_word_count)\n",
    "    list_of_word_frequencies.append(word_frequencies_table)\n",
    "    \n",
    "    \n",
    "    # Show basic information\n",
    "    #print(iter_)\n",
    "    iter_ = iter_ + 1\n",
    "    #print(\"################################\")\n",
    "    #print(\"################################\")\n",
    "    #print(\"Filename: \" + str(file))\n",
    "    #print(\"Title: \" + str(title))\n",
    "    #print(\"Author(s): \" + str(author))\n",
    "    #print(\"Date: \" + str(date))\n",
    "    #print(\"Year: \" + str(year))\n",
    "    #print(\"Language: \" + str(language))\n",
    "    #print(\"Encoding: \" + str(encoding))\n",
    "    #print(\"################################\")\n",
    "    #print(\"Words in book: \" + str(word_count))\n",
    "    #print(\"Unique words in book: \" + str(unique_word_count))\n",
    "    #print(\"################################\")\n",
    "    #print(word_frequencies_table)\n",
    "\n",
    "# Feel free to change to dict? list? separate files?\n",
    "## nested dataframes works, but looks super ungly when printing\n",
    "### Fuck it - This is tooo useless killing it again\n",
    "#all_res = pd.DataFrame(list(zip(list_of_file\n",
    "#                                , list_of_title\n",
    "#                                , list_of_author\n",
    "#                                , list_of_date\n",
    "#                                , list_of_language\n",
    "#                                , list_of_encoding\n",
    "#                                , list_of_word_count\n",
    "#                                , list_of_unique_word_count\n",
    "#                                , list_of_word_frequencies\n",
    "#                                ))\n",
    "#                             , columns = ['file'\n",
    "#                                          , 'title'\n",
    "#                                          , 'author'\n",
    "#                                          , 'date'\n",
    "#                                          , 'language'\n",
    "#                                          , 'encoding'\n",
    "#                                          , 'word_count'\n",
    "#                                          , 'unique_word_count'\n",
    "#                                          , 'word_frequencies'\n",
    "#                                         ]\n",
    "#                      )\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Word ranking between titles\n",
    "This is our first attemt at seeing how the ranking of words change between titles. Idea is to see that the zipf-distribution changes as time passes buy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_count= []\n",
    "list_freq = []\n",
    "list_rank = []\n",
    "\n",
    "\n",
    "col_names = list_of_title.copy()\n",
    "col_names.insert(0,'Word')\n",
    "\n",
    "for df in list_of_word_frequencies:\n",
    "    list_count.append(df[['Word', 'count']])\n",
    "    list_freq.append(df[['Word', 'freq']])\n",
    "    list_rank.append(df[['Word', 'rank']])\n",
    "    \n",
    "df_count = reduce(lambda left, right: pd.merge(left, right, on=\"Word\", how='outer'), list_count)\n",
    "df_count.columns = col_names\n",
    "df_count['Sum'] = df_count.drop('Word', axis=1).apply(lambda x: x.sum(), axis=1)\n",
    "df_count = df_count.sort_values(ascending = False, by=['Sum'])\n",
    "\n",
    "df_freq = reduce(lambda left, right: pd.merge(left, right, on=\"Word\", how='outer'), list_freq)\n",
    "df_freq.columns = col_names\n",
    "df_freq['Avg'] = df_freq.drop('Word', axis=1).apply(lambda x: x.mean(), axis=1)\n",
    "df_freq = df_freq.sort_values(ascending = False, by=['Avg'])\n",
    "\n",
    "df_rank = reduce(lambda left, right: pd.merge(left, right, on=\"Word\", how='outer'), list_rank)\n",
    "df_rank.columns = col_names\n",
    "df_rank['Avg'] = df_rank.drop('Word', axis=1).apply(lambda x: x.mean(), axis=1)\n",
    "df_rank = df_rank.sort_values(by=['Avg'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>The Communist Manifesto</th>\n",
       "      <th>Anne Of The Island</th>\n",
       "      <th>Anne Of Avonlea</th>\n",
       "      <th>Alexander's Bridge and The Barrel Organ</th>\n",
       "      <th>The Narrative of the Life of Frederick Douglass</th>\n",
       "      <th>On the Duty of Civil Disobedience</th>\n",
       "      <th>The Song Of Hiawatha</th>\n",
       "      <th>The War of the Worlds</th>\n",
       "      <th>The Oedipus Trilogy</th>\n",
       "      <th>Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15782</th>\n",
       "      <td>oedipus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bourgeois</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12897</th>\n",
       "      <td>hiawatha</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1924</th>\n",
       "      <td>anne</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>with</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>for</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>but</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>was</td>\n",
       "      <td>32.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>11.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>his</td>\n",
       "      <td>39.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>not</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>all</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15783</th>\n",
       "      <td>creon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>they</td>\n",
       "      <td>7.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>15.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10248</th>\n",
       "      <td>bartley</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10249</th>\n",
       "      <td>hilda</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>have</td>\n",
       "      <td>20.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>20.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>this</td>\n",
       "      <td>12.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>21.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15784</th>\n",
       "      <td>antigone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>had</td>\n",
       "      <td>46.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>22.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>from</td>\n",
       "      <td>21.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>22.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>him</td>\n",
       "      <td>49.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>22.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1925</th>\n",
       "      <td>her</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>24.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>you</td>\n",
       "      <td>38.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>25.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>one</td>\n",
       "      <td>24.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>26.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>socialism</td>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14024</th>\n",
       "      <td>martians</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12898</th>\n",
       "      <td>puk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word  The Communist Manifesto  Anne Of The Island  \\\n",
       "0            the                      1.0                 1.0   \n",
       "1            and                      2.0                 2.0   \n",
       "15782    oedipus                      NaN                 NaN   \n",
       "4      bourgeois                      5.0                 NaN   \n",
       "2           that                      3.0                 7.0   \n",
       "12897   hiawatha                      NaN                 NaN   \n",
       "1924        anne                      NaN                 5.0   \n",
       "5           with                      5.0                12.0   \n",
       "8            for                      7.0                13.0   \n",
       "14           but                     13.0                 9.0   \n",
       "42           was                     32.0                 6.0   \n",
       "63           his                     39.0                21.0   \n",
       "21           not                     18.0                16.0   \n",
       "6            all                      6.0                15.0   \n",
       "15783      creon                      NaN                 NaN   \n",
       "7           they                      7.0                17.0   \n",
       "10248    bartley                      NaN                 NaN   \n",
       "10249      hilda                      NaN                 NaN   \n",
       "23          have                     20.0                14.0   \n",
       "13          this                     12.0                32.0   \n",
       "15784   antigone                      NaN                 NaN   \n",
       "112          had                     46.0                10.0   \n",
       "25          from                     21.0                39.0   \n",
       "169          him                     49.0                24.0   \n",
       "1925         her                      NaN                 8.0   \n",
       "60           you                     38.0                 3.0   \n",
       "29           one                     24.0                18.0   \n",
       "33     socialism                     27.0                 NaN   \n",
       "14024   martians                      NaN                 NaN   \n",
       "12898        puk                      NaN                 NaN   \n",
       "\n",
       "       Anne Of Avonlea  Alexander's Bridge and The Barrel Organ  \\\n",
       "0                  1.0                                      1.0   \n",
       "1                  2.0                                      2.0   \n",
       "15782              NaN                                      NaN   \n",
       "4                  NaN                                      NaN   \n",
       "2                  5.0                                      6.0   \n",
       "12897              NaN                                      NaN   \n",
       "1924               8.0                                      NaN   \n",
       "5                 12.0                                     10.0   \n",
       "8                 10.0                                     12.0   \n",
       "14                 9.0                                     14.0   \n",
       "42                 6.0                                      4.0   \n",
       "63                16.0                                      5.0   \n",
       "21                19.0                                     22.0   \n",
       "6                 15.0                                     17.0   \n",
       "15783              NaN                                      NaN   \n",
       "7                 17.0                                     18.0   \n",
       "10248              NaN                                     19.0   \n",
       "10249              NaN                                     20.0   \n",
       "23                14.0                                     23.0   \n",
       "13                40.0                                     26.0   \n",
       "15784              NaN                                      NaN   \n",
       "112               11.0                                      9.0   \n",
       "25                45.0                                     28.0   \n",
       "169               21.0                                     11.0   \n",
       "1925               7.0                                      7.0   \n",
       "60                 4.0                                      3.0   \n",
       "29                44.0                                     23.0   \n",
       "33                 NaN                                      NaN   \n",
       "14024              NaN                                      NaN   \n",
       "12898              NaN                                      NaN   \n",
       "\n",
       "       The Narrative of the Life of Frederick Douglass  \\\n",
       "0                                                  1.0   \n",
       "1                                                  2.0   \n",
       "15782                                              NaN   \n",
       "4                                                  NaN   \n",
       "2                                                  4.0   \n",
       "12897                                              NaN   \n",
       "1924                                               NaN   \n",
       "5                                                  6.0   \n",
       "8                                                  7.0   \n",
       "14                                                11.0   \n",
       "42                                                 3.0   \n",
       "63                                                 5.0   \n",
       "21                                                10.0   \n",
       "6                                                 19.0   \n",
       "15783                                              NaN   \n",
       "7                                                 16.0   \n",
       "10248                                              NaN   \n",
       "10249                                              NaN   \n",
       "23                                                17.0   \n",
       "13                                                 9.0   \n",
       "15784                                              NaN   \n",
       "112                                                8.0   \n",
       "25                                                15.0   \n",
       "169                                               11.0   \n",
       "1925                                              20.0   \n",
       "60                                                40.0   \n",
       "29                                                13.0   \n",
       "33                                                 NaN   \n",
       "14024                                              NaN   \n",
       "12898                                              NaN   \n",
       "\n",
       "       On the Duty of Civil Disobedience  The Song Of Hiawatha  \\\n",
       "0                                    1.0                   1.0   \n",
       "1                                    2.0                   2.0   \n",
       "15782                                NaN                   NaN   \n",
       "4                                    NaN                   NaN   \n",
       "2                                    4.0                  12.0   \n",
       "12897                                NaN                   6.0   \n",
       "1924                                 NaN                   NaN   \n",
       "5                                   10.0                   4.0   \n",
       "8                                    5.0                  14.0   \n",
       "14                                   6.0                  17.0   \n",
       "42                                  16.0                  10.0   \n",
       "63                                  13.0                   3.0   \n",
       "21                                   3.0                  14.0   \n",
       "6                                   22.0                   7.0   \n",
       "15783                                NaN                   NaN   \n",
       "7                                    5.0                  15.0   \n",
       "10248                                NaN                   NaN   \n",
       "10249                                NaN                   NaN   \n",
       "23                                   9.0                  46.0   \n",
       "13                                  11.0                  35.0   \n",
       "15784                                NaN                   NaN   \n",
       "112                                 34.0                  33.0   \n",
       "25                                  23.0                   5.0   \n",
       "169                                 32.0                   8.0   \n",
       "1925                                42.0                  19.0   \n",
       "60                                  33.0                   9.0   \n",
       "29                                  21.0                  45.0   \n",
       "33                                   NaN                   NaN   \n",
       "14024                                NaN                   NaN   \n",
       "12898                                NaN                  28.0   \n",
       "\n",
       "       The War of the Worlds  The Oedipus Trilogy        Avg  \n",
       "0                        1.0                  1.0   1.000000  \n",
       "1                        2.0                  2.0   2.000000  \n",
       "15782                    NaN                  4.0   4.000000  \n",
       "4                        NaN                  NaN   5.000000  \n",
       "2                        4.0                  8.0   5.888889  \n",
       "12897                    NaN                  NaN   6.000000  \n",
       "1924                     NaN                  NaN   6.500000  \n",
       "5                        6.0                 13.0   8.666667  \n",
       "8                        8.0                  6.0   9.111111  \n",
       "14                      11.0                  9.0  11.000000  \n",
       "42                       3.0                 24.0  11.555556  \n",
       "63                      13.0                 12.0  14.111111  \n",
       "21                      21.0                  5.0  14.222222  \n",
       "6                       18.0                 15.0  14.888889  \n",
       "15783                    NaN                 15.0  15.000000  \n",
       "7                        9.0                 32.0  15.111111  \n",
       "10248                    NaN                  NaN  19.000000  \n",
       "10249                    NaN                  NaN  20.000000  \n",
       "23                      16.0                 26.0  20.555556  \n",
       "13                      14.0                 11.0  21.111111  \n",
       "15784                    NaN                 22.0  22.000000  \n",
       "112                      5.0                 43.0  22.111111  \n",
       "25                      10.0                 18.0  22.666667  \n",
       "169                     30.0                 20.0  22.888889  \n",
       "1925                    68.0                 25.0  24.500000  \n",
       "60                      59.0                 40.0  25.444444  \n",
       "29                      19.0                 31.0  26.444444  \n",
       "33                       NaN                  NaN  27.000000  \n",
       "14024                   28.0                  NaN  28.000000  \n",
       "12898                    NaN                  NaN  28.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rank.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>The Communist Manifesto</th>\n",
       "      <th>Anne Of The Island</th>\n",
       "      <th>Anne Of Avonlea</th>\n",
       "      <th>Alexander's Bridge and The Barrel Organ</th>\n",
       "      <th>The Narrative of the Life of Frederick Douglass</th>\n",
       "      <th>On the Duty of Civil Disobedience</th>\n",
       "      <th>The Song Of Hiawatha</th>\n",
       "      <th>The War of the Worlds</th>\n",
       "      <th>The Oedipus Trilogy</th>\n",
       "      <th>Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>0.133833</td>\n",
       "      <td>0.047302</td>\n",
       "      <td>0.050923</td>\n",
       "      <td>0.066149</td>\n",
       "      <td>0.074860</td>\n",
       "      <td>0.073976</td>\n",
       "      <td>0.121865</td>\n",
       "      <td>0.101870</td>\n",
       "      <td>0.048319</td>\n",
       "      <td>0.079900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "      <td>0.040548</td>\n",
       "      <td>0.039079</td>\n",
       "      <td>0.042328</td>\n",
       "      <td>0.044997</td>\n",
       "      <td>0.045687</td>\n",
       "      <td>0.051813</td>\n",
       "      <td>0.049645</td>\n",
       "      <td>0.053220</td>\n",
       "      <td>0.034566</td>\n",
       "      <td>0.044654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1924</th>\n",
       "      <td>anne</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017207</td>\n",
       "      <td>0.014547</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.015877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15782</th>\n",
       "      <td>oedipus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.015669</td>\n",
       "      <td>0.015669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that</td>\n",
       "      <td>0.014649</td>\n",
       "      <td>0.015734</td>\n",
       "      <td>0.016425</td>\n",
       "      <td>0.017222</td>\n",
       "      <td>0.013523</td>\n",
       "      <td>0.019962</td>\n",
       "      <td>0.006015</td>\n",
       "      <td>0.016854</td>\n",
       "      <td>0.010491</td>\n",
       "      <td>0.014542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>was</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.016727</td>\n",
       "      <td>0.015992</td>\n",
       "      <td>0.018767</td>\n",
       "      <td>0.024621</td>\n",
       "      <td>0.007632</td>\n",
       "      <td>0.007501</td>\n",
       "      <td>0.018130</td>\n",
       "      <td>0.004674</td>\n",
       "      <td>0.012997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bourgeois</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>with</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.009364</td>\n",
       "      <td>0.009058</td>\n",
       "      <td>0.009494</td>\n",
       "      <td>0.012593</td>\n",
       "      <td>0.009541</td>\n",
       "      <td>0.016814</td>\n",
       "      <td>0.009543</td>\n",
       "      <td>0.008944</td>\n",
       "      <td>0.010786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12897</th>\n",
       "      <td>hiawatha</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010074</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>for</td>\n",
       "      <td>0.009493</td>\n",
       "      <td>0.008620</td>\n",
       "      <td>0.010401</td>\n",
       "      <td>0.008213</td>\n",
       "      <td>0.011065</td>\n",
       "      <td>0.014091</td>\n",
       "      <td>0.005472</td>\n",
       "      <td>0.007396</td>\n",
       "      <td>0.012071</td>\n",
       "      <td>0.009647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>his</td>\n",
       "      <td>0.002109</td>\n",
       "      <td>0.004682</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.017972</td>\n",
       "      <td>0.012892</td>\n",
       "      <td>0.008073</td>\n",
       "      <td>0.019459</td>\n",
       "      <td>0.005377</td>\n",
       "      <td>0.009280</td>\n",
       "      <td>0.009533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>but</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.010853</td>\n",
       "      <td>0.011600</td>\n",
       "      <td>0.007374</td>\n",
       "      <td>0.006679</td>\n",
       "      <td>0.013944</td>\n",
       "      <td>0.004131</td>\n",
       "      <td>0.006249</td>\n",
       "      <td>0.010289</td>\n",
       "      <td>0.008736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>you</td>\n",
       "      <td>0.002227</td>\n",
       "      <td>0.019341</td>\n",
       "      <td>0.016628</td>\n",
       "      <td>0.019915</td>\n",
       "      <td>0.002492</td>\n",
       "      <td>0.003229</td>\n",
       "      <td>0.007682</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.002623</td>\n",
       "      <td>0.008426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12957</th>\n",
       "      <td>thou</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.016375</td>\n",
       "      <td>0.008351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>not</td>\n",
       "      <td>0.005156</td>\n",
       "      <td>0.006337</td>\n",
       "      <td>0.004970</td>\n",
       "      <td>0.005122</td>\n",
       "      <td>0.007177</td>\n",
       "      <td>0.021283</td>\n",
       "      <td>0.005472</td>\n",
       "      <td>0.004081</td>\n",
       "      <td>0.012172</td>\n",
       "      <td>0.007974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15783</th>\n",
       "      <td>creon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007868</td>\n",
       "      <td>0.007868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1925</th>\n",
       "      <td>her</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014956</td>\n",
       "      <td>0.014952</td>\n",
       "      <td>0.014351</td>\n",
       "      <td>0.004884</td>\n",
       "      <td>0.001908</td>\n",
       "      <td>0.003950</td>\n",
       "      <td>0.001445</td>\n",
       "      <td>0.004438</td>\n",
       "      <td>0.007611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>all</td>\n",
       "      <td>0.011133</td>\n",
       "      <td>0.006419</td>\n",
       "      <td>0.006602</td>\n",
       "      <td>0.005873</td>\n",
       "      <td>0.004918</td>\n",
       "      <td>0.005431</td>\n",
       "      <td>0.009748</td>\n",
       "      <td>0.004463</td>\n",
       "      <td>0.007868</td>\n",
       "      <td>0.006939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>they</td>\n",
       "      <td>0.009493</td>\n",
       "      <td>0.005526</td>\n",
       "      <td>0.005620</td>\n",
       "      <td>0.005564</td>\n",
       "      <td>0.005914</td>\n",
       "      <td>0.014091</td>\n",
       "      <td>0.005436</td>\n",
       "      <td>0.007290</td>\n",
       "      <td>0.003430</td>\n",
       "      <td>0.006929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923</th>\n",
       "      <td>she</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017322</td>\n",
       "      <td>0.016873</td>\n",
       "      <td>0.012982</td>\n",
       "      <td>0.002990</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.001522</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>0.002690</td>\n",
       "      <td>0.006917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word  The Communist Manifesto  Anne Of The Island  \\\n",
       "0            the                 0.133833            0.047302   \n",
       "1            and                 0.040548            0.039079   \n",
       "1924        anne                      NaN            0.017207   \n",
       "15782    oedipus                      NaN                 NaN   \n",
       "2           that                 0.014649            0.015734   \n",
       "42           was                 0.002930            0.016727   \n",
       "4      bourgeois                 0.011719                 NaN   \n",
       "5           with                 0.011719            0.009364   \n",
       "12897   hiawatha                      NaN                 NaN   \n",
       "8            for                 0.009493            0.008620   \n",
       "63           his                 0.002109            0.004682   \n",
       "14           but                 0.007500            0.010853   \n",
       "60           you                 0.002227            0.019341   \n",
       "12957       thou                      NaN                 NaN   \n",
       "21           not                 0.005156            0.006337   \n",
       "15783      creon                      NaN                 NaN   \n",
       "1925         her                      NaN            0.014956   \n",
       "6            all                 0.011133            0.006419   \n",
       "7           they                 0.009493            0.005526   \n",
       "1923         she                      NaN            0.017322   \n",
       "\n",
       "       Anne Of Avonlea  Alexander's Bridge and The Barrel Organ  \\\n",
       "0             0.050923                                 0.066149   \n",
       "1             0.042328                                 0.044997   \n",
       "1924          0.014547                                      NaN   \n",
       "15782              NaN                                      NaN   \n",
       "2             0.016425                                 0.017222   \n",
       "42            0.015992                                 0.018767   \n",
       "4                  NaN                                      NaN   \n",
       "5             0.009058                                 0.009494   \n",
       "12897              NaN                                      NaN   \n",
       "8             0.010401                                 0.008213   \n",
       "63            0.005952                                 0.017972   \n",
       "14            0.011600                                 0.007374   \n",
       "60            0.016628                                 0.019915   \n",
       "12957              NaN                                      NaN   \n",
       "21            0.004970                                 0.005122   \n",
       "15783              NaN                                      NaN   \n",
       "1925          0.014952                                 0.014351   \n",
       "6             0.006602                                 0.005873   \n",
       "7             0.005620                                 0.005564   \n",
       "1923          0.016873                                 0.012982   \n",
       "\n",
       "       The Narrative of the Life of Frederick Douglass  \\\n",
       "0                                             0.074860   \n",
       "1                                             0.045687   \n",
       "1924                                               NaN   \n",
       "15782                                              NaN   \n",
       "2                                             0.013523   \n",
       "42                                            0.024621   \n",
       "4                                                  NaN   \n",
       "5                                             0.012593   \n",
       "12897                                              NaN   \n",
       "8                                             0.011065   \n",
       "63                                            0.012892   \n",
       "14                                            0.006679   \n",
       "60                                            0.002492   \n",
       "12957                                              NaN   \n",
       "21                                            0.007177   \n",
       "15783                                              NaN   \n",
       "1925                                          0.004884   \n",
       "6                                             0.004918   \n",
       "7                                             0.005914   \n",
       "1923                                          0.002990   \n",
       "\n",
       "       On the Duty of Civil Disobedience  The Song Of Hiawatha  \\\n",
       "0                               0.073976              0.121865   \n",
       "1                               0.051813              0.049645   \n",
       "1924                                 NaN                   NaN   \n",
       "15782                                NaN                   NaN   \n",
       "2                               0.019962              0.006015   \n",
       "42                              0.007632              0.007501   \n",
       "4                                    NaN                   NaN   \n",
       "5                               0.009541              0.016814   \n",
       "12897                                NaN              0.010074   \n",
       "8                               0.014091              0.005472   \n",
       "63                              0.008073              0.019459   \n",
       "14                              0.013944              0.004131   \n",
       "60                              0.003229              0.007682   \n",
       "12957                                NaN              0.000326   \n",
       "21                              0.021283              0.005472   \n",
       "15783                                NaN                   NaN   \n",
       "1925                            0.001908              0.003950   \n",
       "6                               0.005431              0.009748   \n",
       "7                               0.014091              0.005436   \n",
       "1923                            0.000147              0.001522   \n",
       "\n",
       "       The War of the Worlds  The Oedipus Trilogy       Avg  \n",
       "0                   0.101870             0.048319  0.079900  \n",
       "1                   0.053220             0.034566  0.044654  \n",
       "1924                     NaN                  NaN  0.015877  \n",
       "15782                    NaN             0.015669  0.015669  \n",
       "2                   0.016854             0.010491  0.014542  \n",
       "42                  0.018130             0.004674  0.012997  \n",
       "4                        NaN                  NaN  0.011719  \n",
       "5                   0.009543             0.008944  0.010786  \n",
       "12897                    NaN                  NaN  0.010074  \n",
       "8                   0.007396             0.012071  0.009647  \n",
       "63                  0.005377             0.009280  0.009533  \n",
       "14                  0.006249             0.010289  0.008736  \n",
       "60                  0.001700             0.002623  0.008426  \n",
       "12957                    NaN             0.016375  0.008351  \n",
       "21                  0.004081             0.012172  0.007974  \n",
       "15783                    NaN             0.007868  0.007868  \n",
       "1925                0.001445             0.004438  0.007611  \n",
       "6                   0.004463             0.007868  0.006939  \n",
       "7                   0.007290             0.003430  0.006929  \n",
       "1923                0.000808             0.002690  0.006917  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_freq['Avg'] = df_freq.drop('Word', axis=1).apply(lambda x: x.mean(), axis=1)\n",
    "df_freq = df_freq.sort_values(ascending = False, by=['Avg'])\n",
    "\n",
    "df_freq.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>The Communist Manifesto</th>\n",
       "      <th>Anne Of The Island</th>\n",
       "      <th>Anne Of Avonlea</th>\n",
       "      <th>Alexander's Bridge and The Barrel Organ</th>\n",
       "      <th>The Narrative of the Life of Frederick Douglass</th>\n",
       "      <th>On the Duty of Civil Disobedience</th>\n",
       "      <th>The Song Of Hiawatha</th>\n",
       "      <th>The War of the Worlds</th>\n",
       "      <th>The Oedipus Trilogy</th>\n",
       "      <th>Sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>1142.0</td>\n",
       "      <td>2859.0</td>\n",
       "      <td>3525.0</td>\n",
       "      <td>1498.0</td>\n",
       "      <td>2253.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>3363.0</td>\n",
       "      <td>4793.0</td>\n",
       "      <td>1437.0</td>\n",
       "      <td>21374.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "      <td>346.0</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>2930.0</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>1375.0</td>\n",
       "      <td>353.0</td>\n",
       "      <td>1370.0</td>\n",
       "      <td>2504.0</td>\n",
       "      <td>1028.0</td>\n",
       "      <td>13287.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>was</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1011.0</td>\n",
       "      <td>1107.0</td>\n",
       "      <td>425.0</td>\n",
       "      <td>741.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>853.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>4560.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that</td>\n",
       "      <td>125.0</td>\n",
       "      <td>951.0</td>\n",
       "      <td>1137.0</td>\n",
       "      <td>390.0</td>\n",
       "      <td>407.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>793.0</td>\n",
       "      <td>312.0</td>\n",
       "      <td>4417.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>you</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1169.0</td>\n",
       "      <td>1151.0</td>\n",
       "      <td>451.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>3257.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>with</td>\n",
       "      <td>100.0</td>\n",
       "      <td>566.0</td>\n",
       "      <td>627.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>379.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>449.0</td>\n",
       "      <td>266.0</td>\n",
       "      <td>3131.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>for</td>\n",
       "      <td>81.0</td>\n",
       "      <td>521.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>333.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>348.0</td>\n",
       "      <td>359.0</td>\n",
       "      <td>2795.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923</th>\n",
       "      <td>she</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1047.0</td>\n",
       "      <td>1168.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2760.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1925</th>\n",
       "      <td>her</td>\n",
       "      <td>NaN</td>\n",
       "      <td>904.0</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>2733.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>but</td>\n",
       "      <td>64.0</td>\n",
       "      <td>656.0</td>\n",
       "      <td>803.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>306.0</td>\n",
       "      <td>2700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>his</td>\n",
       "      <td>18.0</td>\n",
       "      <td>283.0</td>\n",
       "      <td>412.0</td>\n",
       "      <td>407.0</td>\n",
       "      <td>388.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>537.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>2629.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>had</td>\n",
       "      <td>11.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>641.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>2563.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1924</th>\n",
       "      <td>anne</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2047.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>all</td>\n",
       "      <td>95.0</td>\n",
       "      <td>388.0</td>\n",
       "      <td>457.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>1971.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>not</td>\n",
       "      <td>44.0</td>\n",
       "      <td>383.0</td>\n",
       "      <td>344.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>362.0</td>\n",
       "      <td>1953.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>they</td>\n",
       "      <td>81.0</td>\n",
       "      <td>334.0</td>\n",
       "      <td>389.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>1799.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>have</td>\n",
       "      <td>42.0</td>\n",
       "      <td>394.0</td>\n",
       "      <td>507.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>1669.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>him</td>\n",
       "      <td>8.0</td>\n",
       "      <td>262.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>1588.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1287</th>\n",
       "      <td>said</td>\n",
       "      <td>1.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>609.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1572.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>from</td>\n",
       "      <td>41.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>291.0</td>\n",
       "      <td>327.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>1537.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Word  The Communist Manifesto  Anne Of The Island  Anne Of Avonlea  \\\n",
       "0      the                   1142.0              2859.0           3525.0   \n",
       "1      and                    346.0              2362.0           2930.0   \n",
       "42     was                     25.0              1011.0           1107.0   \n",
       "2     that                    125.0               951.0           1137.0   \n",
       "60     you                     19.0              1169.0           1151.0   \n",
       "5     with                    100.0               566.0            627.0   \n",
       "8      for                     81.0               521.0            720.0   \n",
       "1923   she                      NaN              1047.0           1168.0   \n",
       "1925   her                      NaN               904.0           1035.0   \n",
       "14     but                     64.0               656.0            803.0   \n",
       "63     his                     18.0               283.0            412.0   \n",
       "112    had                     11.0               640.0            641.0   \n",
       "1924  anne                      NaN              1040.0           1007.0   \n",
       "6      all                     95.0               388.0            457.0   \n",
       "21     not                     44.0               383.0            344.0   \n",
       "7     they                     81.0               334.0            389.0   \n",
       "23    have                     42.0               394.0            507.0   \n",
       "169    him                      8.0               262.0            315.0   \n",
       "1287  said                      1.0               577.0            609.0   \n",
       "25    from                     41.0               188.0            214.0   \n",
       "\n",
       "      Alexander's Bridge and The Barrel Organ  \\\n",
       "0                                      1498.0   \n",
       "1                                      1019.0   \n",
       "42                                      425.0   \n",
       "2                                       390.0   \n",
       "60                                      451.0   \n",
       "5                                       215.0   \n",
       "8                                       186.0   \n",
       "1923                                    294.0   \n",
       "1925                                    325.0   \n",
       "14                                      167.0   \n",
       "63                                      407.0   \n",
       "112                                     270.0   \n",
       "1924                                      NaN   \n",
       "6                                       133.0   \n",
       "21                                      116.0   \n",
       "7                                       126.0   \n",
       "23                                      115.0   \n",
       "169                                     209.0   \n",
       "1287                                     36.0   \n",
       "25                                       79.0   \n",
       "\n",
       "      The Narrative of the Life of Frederick Douglass  \\\n",
       "0                                              2253.0   \n",
       "1                                              1375.0   \n",
       "42                                              741.0   \n",
       "2                                               407.0   \n",
       "60                                               75.0   \n",
       "5                                               379.0   \n",
       "8                                               333.0   \n",
       "1923                                             90.0   \n",
       "1925                                            147.0   \n",
       "14                                              201.0   \n",
       "63                                              388.0   \n",
       "112                                             255.0   \n",
       "1924                                              NaN   \n",
       "6                                               148.0   \n",
       "21                                              216.0   \n",
       "7                                               178.0   \n",
       "23                                              155.0   \n",
       "169                                             201.0   \n",
       "1287                                             57.0   \n",
       "25                                              179.0   \n",
       "\n",
       "      On the Duty of Civil Disobedience  The Song Of Hiawatha  \\\n",
       "0                                 504.0                3363.0   \n",
       "1                                 353.0                1370.0   \n",
       "42                                 52.0                 207.0   \n",
       "2                                 136.0                 166.0   \n",
       "60                                 22.0                 212.0   \n",
       "5                                  65.0                 464.0   \n",
       "8                                  96.0                 151.0   \n",
       "1923                                1.0                  42.0   \n",
       "1925                               13.0                 109.0   \n",
       "14                                 95.0                 114.0   \n",
       "63                                 55.0                 537.0   \n",
       "112                                21.0                  68.0   \n",
       "1924                                NaN                   NaN   \n",
       "6                                  37.0                 269.0   \n",
       "21                                145.0                 151.0   \n",
       "7                                  96.0                 150.0   \n",
       "23                                 67.0                  45.0   \n",
       "169                                23.0                 256.0   \n",
       "1287                               10.0                 101.0   \n",
       "25                                 35.0                 291.0   \n",
       "\n",
       "      The War of the Worlds  The Oedipus Trilogy      Sum  \n",
       "0                    4793.0               1437.0  21374.0  \n",
       "1                    2504.0               1028.0  13287.0  \n",
       "42                    853.0                139.0   4560.0  \n",
       "2                     793.0                312.0   4417.0  \n",
       "60                     80.0                 78.0   3257.0  \n",
       "5                     449.0                266.0   3131.0  \n",
       "8                     348.0                359.0   2795.0  \n",
       "1923                   38.0                 80.0   2760.0  \n",
       "1925                   68.0                132.0   2733.0  \n",
       "14                    294.0                306.0   2700.0  \n",
       "63                    253.0                276.0   2629.0  \n",
       "112                   582.0                 75.0   2563.0  \n",
       "1924                    NaN                  NaN   2047.0  \n",
       "6                     210.0                234.0   1971.0  \n",
       "21                    192.0                362.0   1953.0  \n",
       "7                     343.0                102.0   1799.0  \n",
       "23                    218.0                126.0   1669.0  \n",
       "169                   157.0                157.0   1588.0  \n",
       "1287                  166.0                 15.0   1572.0  \n",
       "25                    327.0                183.0   1537.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_count.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This definately needs some proper refactoring, but Was curious whether we get anything decent from reading a bunch of random books in\n",
    "\n",
    "Requires an additional folder \"decades\" in the root directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all filenames\n",
    "files = [f for f in listdir(file_path) if isfile(join(file_path, f))]\n",
    "\n",
    "# Do only subset\n",
    "## Is done for 5000 files already, so set down to 20 to increase performance. 5000 books are currently stored in the file\n",
    "files = files[0:20]\n",
    "\n",
    "\n",
    "counter = 0\n",
    "for file in files:\n",
    "    counter = counter + 1\n",
    "    # Read in basic information from file\n",
    "    title, author, date, year, language, encoding, content_lines = read_file(file_path + \"/\" + file)\n",
    "    #line_count = len(content_lines)\n",
    "    decade = math.floor(year / 10) * 10\n",
    "    decade_file = \"decades/\" + str(decade) + \".txt\"\n",
    "    content_all = \" \".join(content_lines)\n",
    "    \n",
    "    if os.path.exists(decade_file):\n",
    "        append_write = 'a' # append if already exists\n",
    "    else:\n",
    "        append_write = 'w' # make a new file if not\n",
    "\n",
    "    fileWriter = open(decade_file,append_write)\n",
    "    fileWriter.write(content_all + '\\n')\n",
    "    fileWriter.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in from the decades files, and see the distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.txt', '1990.txt', '2000.txt']\n",
      "2000.txt\n",
      "1990.txt\n",
      "0.txt\n"
     ]
    }
   ],
   "source": [
    "# Get all filenames\n",
    "files = [f for f in listdir(\"decades\") if isfile(join(\"decades\", f))]\n",
    "print(files)\n",
    "files.sort(reverse=True)\n",
    "\n",
    "\n",
    "col_names = []\n",
    "col_names.append(\"Word\")\n",
    "\n",
    "tables = []\n",
    "\n",
    "for file_name in files:\n",
    "    print(file_name)\n",
    "    \n",
    "    file = open(\"decades/\" + file_name, encoding=\"ISO-8859-1\")\n",
    "    file_content = file.read()\n",
    "    \n",
    "    # Split into words (and do various cleaning)\n",
    "    all_text_lower = file_content.lower()\n",
    "    words = re.findall(r'(\\b[A-Za-z][a-z]{2,9}\\b)', all_text_lower)\n",
    "\n",
    "    # First analysis, but should do something proper\n",
    "    word_frequencies_table, unique_word_count = get_word_frequencies(words)\n",
    "    tables.append(word_frequencies_table)\n",
    "    col_names.append(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary Conclusion\n",
    "We see that even though the books are quite old, no decade prior to 1990s is found.\n",
    "\n",
    "This is when we found out that the \"year\" that's registered in the dataset is the upload-date. \n",
    "\n",
    "Haven gotten this far, we however decided to see if we could find a pattern in this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare ranking between upload-decades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_count= []\n",
    "list_freq = []\n",
    "list_rank = []\n",
    "\n",
    "for df in tables:\n",
    "    #list_count.append(df[['Word', 'count']])\n",
    "    #list_freq.append(df[['Word', 'freq']])\n",
    "    list_rank.append(df[['Word', 'rank']])\n",
    "    \n",
    "#df_count = reduce(lambda left, right: pd.merge(left, right, on=\"Word\", how='outer'), list_count)\n",
    "#df_count.columns = col_names\n",
    "\n",
    "#df_freq = reduce(lambda left, right: pd.merge(left, right, on=\"Word\", how='outer'), list_freq)\n",
    "#df_freq.columns = col_names\n",
    "\n",
    "df_rank = reduce(lambda left, right: pd.merge(left, right, on=\"Word\", how='outer'), list_rank)\n",
    "df_rank.columns = col_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>2000.txt</th>\n",
       "      <th>1990.txt</th>\n",
       "      <th>0.txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>was</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>that</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you</td>\n",
       "      <td>5.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>she</td>\n",
       "      <td>6.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>with</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>her</td>\n",
       "      <td>8.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>for</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>but</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>his</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>anne</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>167.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>had</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>not</td>\n",
       "      <td>14.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>all</td>\n",
       "      <td>14.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>they</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>have</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>him</td>\n",
       "      <td>17.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>said</td>\n",
       "      <td>18.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>131.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>this</td>\n",
       "      <td>19.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>from</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>when</td>\n",
       "      <td>21.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>there</td>\n",
       "      <td>22.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>one</td>\n",
       "      <td>23.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>what</td>\n",
       "      <td>24.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>were</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>them</td>\n",
       "      <td>26.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>are</td>\n",
       "      <td>27.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>would</td>\n",
       "      <td>28.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>like</td>\n",
       "      <td>29.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>out</td>\n",
       "      <td>30.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>who</td>\n",
       "      <td>31.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>their</td>\n",
       "      <td>32.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>never</td>\n",
       "      <td>33.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>can</td>\n",
       "      <td>34.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>very</td>\n",
       "      <td>35.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>will</td>\n",
       "      <td>36.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>been</td>\n",
       "      <td>37.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>old</td>\n",
       "      <td>38.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>97.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>little</td>\n",
       "      <td>39.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>then</td>\n",
       "      <td>40.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>know</td>\n",
       "      <td>41.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>107.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>about</td>\n",
       "      <td>41.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>over</td>\n",
       "      <td>42.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>which</td>\n",
       "      <td>43.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>just</td>\n",
       "      <td>44.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>146.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>could</td>\n",
       "      <td>45.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>your</td>\n",
       "      <td>46.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>more</td>\n",
       "      <td>47.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>into</td>\n",
       "      <td>48.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>man</td>\n",
       "      <td>49.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>has</td>\n",
       "      <td>50.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>mrs</td>\n",
       "      <td>51.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>177.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>well</td>\n",
       "      <td>52.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>time</td>\n",
       "      <td>53.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>see</td>\n",
       "      <td>54.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>down</td>\n",
       "      <td>55.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>139.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>now</td>\n",
       "      <td>56.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>don</td>\n",
       "      <td>57.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>229.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>think</td>\n",
       "      <td>58.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>than</td>\n",
       "      <td>59.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>any</td>\n",
       "      <td>60.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>thou</td>\n",
       "      <td>61.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>come</td>\n",
       "      <td>62.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>147.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>how</td>\n",
       "      <td>63.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>did</td>\n",
       "      <td>64.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>oedipus</td>\n",
       "      <td>65.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>346.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>some</td>\n",
       "      <td>66.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>came</td>\n",
       "      <td>67.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>219.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>too</td>\n",
       "      <td>68.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>only</td>\n",
       "      <td>69.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>good</td>\n",
       "      <td>70.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>such</td>\n",
       "      <td>71.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>must</td>\n",
       "      <td>72.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>before</td>\n",
       "      <td>73.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>davy</td>\n",
       "      <td>74.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>our</td>\n",
       "      <td>75.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>after</td>\n",
       "      <td>76.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>where</td>\n",
       "      <td>77.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>119.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>much</td>\n",
       "      <td>78.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>day</td>\n",
       "      <td>79.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>117.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>its</td>\n",
       "      <td>79.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>marilla</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>miss</td>\n",
       "      <td>81.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>197.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>say</td>\n",
       "      <td>82.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>ever</td>\n",
       "      <td>83.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>through</td>\n",
       "      <td>83.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>here</td>\n",
       "      <td>84.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>going</td>\n",
       "      <td>85.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>303.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>went</td>\n",
       "      <td>85.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>270.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>made</td>\n",
       "      <td>86.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>home</td>\n",
       "      <td>87.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>288.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>back</td>\n",
       "      <td>88.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>205.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>other</td>\n",
       "      <td>89.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>always</td>\n",
       "      <td>90.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>173.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>diana</td>\n",
       "      <td>91.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>344.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>long</td>\n",
       "      <td>92.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>thought</td>\n",
       "      <td>93.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>155.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>upon</td>\n",
       "      <td>94.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>thy</td>\n",
       "      <td>95.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word  2000.txt  1990.txt  0.txt\n",
       "0       the       1.0       1.0    1.0\n",
       "1       and       2.0       2.0    2.0\n",
       "2       was       3.0       3.0    5.0\n",
       "3      that       4.0       4.0    3.0\n",
       "4       you       5.0      59.0   24.0\n",
       "5       she       6.0      96.0   37.0\n",
       "6      with       7.0       6.0    7.0\n",
       "7       her       8.0      68.0   23.0\n",
       "8       for       9.0       8.0    8.0\n",
       "9       but      10.0      11.0   11.0\n",
       "10      his      11.0      13.0    4.0\n",
       "11     anne      12.0       NaN  167.0\n",
       "12      had      13.0       5.0   10.0\n",
       "13      not      14.0      21.0    9.0\n",
       "14      all      14.0      18.0   16.0\n",
       "15     they      15.0       9.0   21.0\n",
       "16     have      16.0      16.0   12.0\n",
       "17      him      17.0      30.0   20.0\n",
       "18     said      18.0      27.0  131.0\n",
       "19     this      19.0      14.0   14.0\n",
       "20     from      20.0      10.0   13.0\n",
       "21     when      21.0      62.0   32.0\n",
       "22    there      22.0      12.0   29.0\n",
       "23      one      23.0      19.0   25.0\n",
       "24     what      24.0      40.0   28.0\n",
       "25     were      25.0       7.0   17.0\n",
       "26     them      26.0      22.0   39.0\n",
       "27      are      27.0      55.0   15.0\n",
       "28    would      28.0      46.0   22.0\n",
       "29     like      29.0      51.0   60.0\n",
       "30      out      30.0      15.0   69.0\n",
       "31      who      31.0      61.0   18.0\n",
       "32    their      32.0      17.0   26.0\n",
       "33    never      33.0     110.0   79.0\n",
       "34      can      34.0      96.0   46.0\n",
       "35     very      35.0      60.0   52.0\n",
       "36     will      36.0      95.0   36.0\n",
       "37     been      37.0      26.0   19.0\n",
       "38      old      38.0     119.0   97.0\n",
       "39   little      39.0      42.0   75.0\n",
       "40     then      40.0      25.0   49.0\n",
       "41     know      41.0      93.0  107.0\n",
       "42    about      41.0      23.0   95.0\n",
       "43     over      42.0      33.0  125.0\n",
       "44    which      43.0      48.0    6.0\n",
       "45     just      44.0      87.0  146.0\n",
       "46    could      45.0      40.0   48.0\n",
       "47     your      46.0     129.0   67.0\n",
       "48     more      47.0      61.0   27.0\n",
       "49     into      48.0      20.0   56.0\n",
       "50      man      49.0      36.0   33.0\n",
       "51      has      50.0      90.0   31.0\n",
       "52      mrs      51.0     129.0  177.0\n",
       "53     well      52.0     107.0   59.0\n",
       "54     time      53.0      38.0   54.0\n",
       "55      see      54.0      66.0   84.0\n",
       "56     down      55.0      34.0  139.0\n",
       "57      now      56.0      45.0   43.0\n",
       "58      don      57.0     121.0  229.0\n",
       "59    think      58.0     105.0  109.0\n",
       "60     than      59.0      79.0   30.0\n",
       "61      any      60.0      76.0   45.0\n",
       "62     thou      61.0       NaN   64.0\n",
       "63     come      62.0      82.0  147.0\n",
       "64      how      63.0      84.0   70.0\n",
       "65      did      64.0      56.0  100.0\n",
       "66  oedipus      65.0       NaN  346.0\n",
       "67     some      66.0      39.0   44.0\n",
       "68     came      67.0      32.0  219.0\n",
       "69      too      68.0      95.0   86.0\n",
       "70     only      69.0      69.0   57.0\n",
       "71     good      70.0     114.0   68.0\n",
       "72     such      71.0      84.0   41.0\n",
       "73     must      72.0      75.0   65.0\n",
       "74   before      73.0      63.0   81.0\n",
       "75     davy      74.0       NaN    NaN\n",
       "76      our      75.0      47.0   42.0\n",
       "77    after      76.0      47.0   87.0\n",
       "78    where      77.0      95.0  119.0\n",
       "79     much      78.0      96.0   66.0\n",
       "80      day      79.0      64.0  117.0\n",
       "81      its      79.0      31.0   50.0\n",
       "82  marilla      80.0       NaN    NaN\n",
       "83     miss      81.0     127.0  197.0\n",
       "84      say      82.0     116.0  105.0\n",
       "85     ever      83.0     116.0  112.0\n",
       "86  through      83.0      37.0  103.0\n",
       "87     here      84.0      74.0   94.0\n",
       "88    going      85.0      86.0  303.0\n",
       "89     went      85.0      49.0  270.0\n",
       "90     made      86.0      63.0   74.0\n",
       "91     home      87.0     120.0  288.0\n",
       "92     back      88.0      63.0  205.0\n",
       "93    other      89.0      77.0   53.0\n",
       "94   always      90.0     127.0  173.0\n",
       "95    diana      91.0       NaN  344.0\n",
       "96     long      92.0      84.0   93.0\n",
       "97  thought      93.0      87.0  155.0\n",
       "98     upon      94.0      24.0  136.0\n",
       "99      thy      95.0       NaN   85.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rank.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to fit models to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_contents = []\n",
    "targets = []\n",
    "\n",
    "files = [f for f in listdir(file_path) if isfile(join(file_path, f))]\n",
    "files = list(filter(lambda file: file[0].isdigit(), files))\n",
    "random.shuffle(files)\n",
    "\n",
    "targets_=['70','80','90','00','10']\n",
    "iter_ = 0\n",
    "\n",
    "for f in files[:120]:\n",
    "    file = open(\"processedData/\" + f, encoding=\"ISO-8859-1\")\n",
    "    file_contents.append(file.read())\n",
    "    iter_ = iter_+1\n",
    "    targets.append(targets_[iter_%5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "n_splits=5 cannot be greater than the number of members in each class.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-2cc486733c0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0mbest_parameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    708\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 710\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    711\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1149\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1151\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    687\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 689\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36msplit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    333\u001b[0m                 .format(self.n_splits, n_samples))\n\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36msplit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter_test_masks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m             \u001b[0mtrain_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[0mtest_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m_iter_test_masks\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_iter_test_masks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 692\u001b[1;33m         \u001b[0mtest_folds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_test_folds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    693\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mtest_folds\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m_make_test_folds\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    661\u001b[0m             raise ValueError(\"n_splits=%d cannot be greater than the\"\n\u001b[0;32m    662\u001b[0m                              \u001b[1;34m\" number of members in each class.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 663\u001b[1;33m                              % (self.n_splits))\n\u001b[0m\u001b[0;32m    664\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_splits\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mmin_groups\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m             warnings.warn((\"The least populated class in y has only %d\"\n",
      "\u001b[1;31mValueError\u001b[0m: n_splits=5 cannot be greater than the number of members in each class."
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('kbest', SelectKBest(chi2, k=100)),\n",
    "    ('nb', MultinomialNB()),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    #'vect__max_df': [1.0),\n",
    "    # 'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    #'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    # 'tfidf__use_idf': (True, False),\n",
    "    # 'tfidf__norm': ('l1', 'l2'),\n",
    "    #'clf__max_iter': (20),\n",
    "    #'clf__alpha': (0.00001),\n",
    "    #'clf__penalty': ('l2'),\n",
    "    # 'clf__max_iter': (10, 50, 80),\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, verbose=1)\n",
    "\n",
    "grid_search.fit(file_contents, targets)\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realisation and conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we came to the conclusion that \"year\" in the Gutenberg dataset shows when the data **was published** to the project, and not the release date of the book.\n",
    "\n",
    "We searched for possible solutions to get the years for book publications, but were unable to find any free API that we could link to our current dataset.\n",
    "\n",
    "We therefore went on a search for other datasets, and to remake our hypothesis entirely.\n",
    "Thus, this part ended in a blind spot. However science is not only about the results, but also about the discoveries along the way, and therefore it is added into this file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Studying language change in Icelandic parliamentary speeches\n",
    "\n",
    "Our task involves research into **language change over the past 100 years**. Additionally we have been tasked with working out factors that influence language change. \n",
    "\n",
    "Another proposed research question could have been focused on figuring out which languages are going extinct. This particular task has been found out to be near impossible to answer given the available data. It is estimated to be very hard to come up with data that capture the amount of speakers for a large enough ranges of combinations of language and year. Furthermore, any data that are available are likely to apply a different definition of \"speaker\" (sometimes including second/third... language speakers, sometimes not) and is also likely to contain politically motivated noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Therefore, we decided to search for English language corpora containing a wide array of text documents collected over the past century for predefined dialects of English and genre of text (movie, articles, books, ...). This surprisingly turned out to be a complex endeavour as all high quality corpora were available only for a big price tag. \n",
    "\n",
    "We also looked into the material provided by the Guttenberg Project [Link](https://www.projekt-gutenberg.org/). This turned out to be promising at first sight as it appears that there is a lot of recently published material. However release date of these documents does not match the year when the documents were actually written and soon enough we figured out that all material is from before 1923. This obviously did not allow us to look much into language change of the 20th and 21st century.\n",
    "\n",
    "_Gerlach, M., & Font-Clos, F. (2020). A standardized Project Gutenberg corpus for statistical analysis of natural language and quantitative linguistics. Entropy, 22(1), 126._\n",
    "\n",
    "Theoretically one could obtain books from after 1923 and include them into the analysis. But one would quickly run into copyright/licensing issues here.\n",
    "\n",
    "Obtaining the content of these books and preprocessing them for the purposes of data analysis turned out to be quite cumbersome as well. Look at Gunnar's notebooks (first draft [here](firstDraft.ipynb), second draft [here](secondDraft.ipynb)) for the details. \n",
    "\n",
    "Finally we turned to looking for non-English corpora and **found an annotated corpus including pre-factured lemmatization of [Icelandic parlimentary speeches](https://clarin.is/en/resources/parliament/) from 1911 until 2018:**\n",
    "\n",
    "_Steingrímsson, Steinþór, Sigrún Helgadóttir, Eiríkur Rögnvaldsson, Starkaður Barkarson and Jón Guðnason. 2018. Risamálheild: A Very Large Icelandic Text Corpus. Proceedings of LREC 2018, pp. 4361-4366. Myazaki, Japan._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## The task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the line with our goal of analyzing the change in language over the past 100 years, we decided to train different models and assess their ability to predict whether an speech held in the Icelandic Parlament belongs to a particular decade. In the end, this is a **document classification task**  in which the input is a large set of parlament speeches and the target/class is the decade in which the speeches were held. \n",
    "\n",
    "A good performance of our proposed classifiers may support the idea that Icelandic has envolved in the years. However, the fact that the models would perform well is not enough to assert that the language has changed. It could be that what has actually changed are the topics or even the way of documenting the speeches. Anyway, for us it was really exiciting to check whether we are able to **fit a  model that predicts reasonably well the decade of an speech by only using the speech itself.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we provide the **setup for a successful** implementation (or replication) of our experiment within this Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Load required libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The following libraries are used during the next sections and therefore need to be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import glob\n",
    "from nltk.probability import FreqDist\n",
    "import random\n",
    "from functools import reduce\n",
    "from nltk import ngrams\n",
    "# Used for building models for classifying:\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#needed afterwards too\n",
    "namespace = \"{http://www.tei-c.org/ns/1.0}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Get the data\n",
    "\n",
    "Data can be downloaded from here: http://www.malfong.is/index.php?dlid=81&lang=en. However, we provided already in our submission file the specifications on how to get the data of our assignment.\n",
    "\n",
    "Then extract zip folder such that a folder labelled `CC_BY` shows up in the parent folder of this notebook. *Test*: `ls ../CC_BY/althingi` should work when run from `.../IcelandicParliamentSpeeches.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Preprocessing helpers\n",
    "\n",
    "The data are available as XML. The text has already been preprocessed to be separated into paragraphs, sentences and words. Furthermore each word tag also includes a `lemma` attribute relating inflected/declensed forms of words to its lemma. This has been done by the authors of the original paper using Machine Learning approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Given a relative path to a file, pull out a list with all the words. This can be achieved by looking for all tags of type `w`, additionally also retrieve the lemma for each word.\n",
    "\n",
    "We will discard all sentences of length 3 or smaller to remove noise and to avoid that our models are able to detect year of speech just based on some short introductory/outro phrases. Furthermore the raw data appear to contain plenty of elements tagged as words that comprise of just a single letter followed by a dot. These will be removed here as well.\n",
    "\n",
    "⚠️ *Pitfall*: The namespace from above must be included when parsing out content from these XML files based on tag names.\n",
    "\n",
    "⚠️ In this kind of preprocessing we lose information about sentence boundaries as all punctuation items from the raw data are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def extract_words(path):\n",
    "    xml_tree = ET.parse(open(path, 'r', encoding=\"utf8\"))\n",
    "    words = []\n",
    "    lemmata = []\n",
    "    \n",
    "    for sentence in xml_tree.getroot().iter('{}s'.format(namespace)):\n",
    "        words_in_sent = sentence.findall('{}w'.format(namespace))\n",
    "        if len(words_in_sent) > 2:\n",
    "            for word in sentence.findall('{}w'.format(namespace)):\n",
    "                if not word.text.endswith('.'):\n",
    "                    words.append(word.text)\n",
    "                    lemmata.append(word.attrib['lemma'])\n",
    "        \n",
    "    return words, lemmata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Extract content of files separated into sentences, note that all stop items are wrapped in a `p` tag in the original documents and are not included here.\n",
    "\n",
    "Also note that some further pre-processing could be done here to exclude items such as numbers, percentages, names, abbreviations, etc. In the original documents these are also assigned to be words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def extract_sentences(path, lemma=False):\n",
    "    xml_tree = ET.parse(open(path, 'r', encoding=\"utf8\"))\n",
    "    sentences = []\n",
    "    \n",
    "    for sentence in xml_tree.getroot().iter('{}s'.format(namespace)):\n",
    "        sentence_cur = []\n",
    "        words_in_sent = sentence.findall('{}w'.format(namespace))\n",
    "        \n",
    "        if len(words_in_sent) > 2:\n",
    "            for word in sentence.findall('{}w'.format(namespace)):\n",
    "                if not word.text.endswith('.'):\n",
    "                    if lemma:\n",
    "                        sentence_cur.append(word.attrib['lemma'])\n",
    "                    else:\n",
    "                        sentence_cur.append(word.text)\n",
    "            \n",
    "            sentences.append(sentence_cur)\n",
    "        \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Retrieve a random selection of `k` file names from the entire corpus. The files must be of type `xml`. This method does not load the entire corpus into memory and allows you to work with smaller selections for test purposes. This method samples only from the `althingi` folder so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_random_sample(k):\n",
    "    files = [filename for filename in glob.iglob('../CC_BY/althingi/**/*.xml', \n",
    "                                                 recursive=True)]\n",
    "    return random.sample(files, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "files = [filename for filename in glob.iglob('../CC_BY/althingi/**/*.xml', recursive=True)]\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Do the same as above but choose `k` files only from a given year (range: 1911-2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_files_for_year(year, k):\n",
    "    files = [filename for filename in glob.iglob('../CC_BY/althingi/{}/'.format(year) + '**/*.xml', \n",
    "                                                 recursive=True)]\n",
    "    return random.sample(files, min(len(files), k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Preliminary Data Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this section, we perform a preliminary data analysis to get a better insight of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Zipf's Law\n",
    "\n",
    "First using frequency distributions of the Natural Language ToolKit (`NLTK`) to look into whether or not we can confirm [Zipf's Law](https://en.wikipedia.org/wiki/Zipf%27s_law) based on the data we have.\n",
    "\n",
    "⚠️ Note that the analysis is done based on 15 randomly selected files from the entire corpus at this point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1df46894488>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "\n",
    "for file in get_random_sample(15):\n",
    "    words.extend(extract_words(file)[1])\n",
    "    \n",
    "fq = FreqDist(word.lower() for word in words)\n",
    "fq.plot(25, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Visualizing the same data but with using the logarithm of the occurrences, this should ideally obtain a straight line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1df4644e108>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXyU1d338c8v+wZJSEJYQgjIqggIQRYBEeiiWFda6C2l1hZKbd1an7u1T2/1bq197G0XrbdorFJpK1QQrUurVnABZDHsIlHZCWsSEiAkISE5zx8ZbYwBEpjJlZn5vl+veWXmus7M/CbCl+OZc65jzjlERCT4RXhdgIiI+IcCXUQkRCjQRURChAJdRCREKNBFREJElFdvnJ6e7nJycrx6exGRoLRmzZpi51xGU+c8C/ScnBzy8/O9ensRkaBkZrtOdU5DLiIiIUKBLiISIhToIiIhwrMxdBEJLjU1NRQWFlJVVeV1KWEhLi6OrKwsoqOjm/0cBbqINEthYSHt2rUjJycHM/O6nJDmnKOkpITCwkJ69OjR7OdpyEVEmqWqqoq0tDSFeSswM9LS0lr8f0MKdBFpNoV56zmb33XQBXrRsRP890ubqT5Z53UpIiJtStAF+uodh5mzfCd3LdqEruUuIvJvQRfokwZ25vaJvXlubSGPLNnqdTkiEsTeeustrrzySq/L8JugnOVy24Te7Cqp4Df/+ojstASuHtzV65JEJAjU1tYSGRnpdRmf45zDOUdExLn1sYMy0M2M/3f9hewtq+T/LNhIl5R4huV08LoskbDx3y9t5oN9R/36mud3ac89X7nglOd//etfExcXx6233sodd9zBhg0bWLJkCYsXL2bOnDlMmjSJ+++/H+cckyZN4oEHHgAgKSmJH/7wh7z22mv85je/oby8nNtvv5309HSGDBly2poOHz7MTTfdxPbt20lISCAvL4+BAwdSXl7OLbfcQn5+PmbGPffcw/XXX8+rr77KT3/6U2pra0lPT2fx4sXce++9JCUlceeddwIwYMAAXn75ZQAuv/xyLrvsMlasWMELL7xA9+7dz+l3GHRDLp+IjYok7xtDyUqNZ+bcfHYUH/e6JBEJoLFjx7J06VIA8vPzKS8vp6amhmXLltG7d29+/OMfs2TJEtavX897773HCy+8AMDx48cZMGAAq1atIjc3lxkzZvDSSy+xdOlSDhw4cNr3vOeee7jooovYuHEj999/P9OnTwfgF7/4BcnJyWzatImNGzcyfvx4ioqKmDFjBs899xwbNmxgwYIFZ/xMH374IdOnT2fdunXnHOYQpD30T6QkxPDUjcO49tHl3PSn91j0vVGkJsZ4XZZIyDtdTzpQhg4dypo1azh27BixsbEMGTKE/Px8li5dyle+8hXGjRtHRkb9VWVvuOEG3nnnHa655hoiIyO5/vrrASgoKKBHjx707t0bgGnTppGXl3fK91y2bBnPPfccAOPHj6ekpIQjR47wxhtvMH/+/E/bpaam8tJLLzF27NhPFwJ16HDmUYPu3bszYsSIs/uFNCFoe+ifyElP5InpuewtreS7f17DiZO1XpckIgEQHR1NTk4Oc+bMYdSoUYwZM4Y333yTbdu2kZ2dfcrnxcXFfWbcvCXzu5uaSWdmOOc+9zpNHQOIioqiru7f06wbLhZKTExsdi3NEfSBDpCb04H/+epAVu88zI8XbtR0RpEQNXbsWB588EHGjh3LmDFjeOyxxxg8eDAjRozg7bffpri4mNraWubNm8ell176uef369ePHTt2sG3bNgDmzZt3xvf761//CtTPiElPT6d9+/Z88Ytf5JFHHvm0XWlpKSNHjuTtt99mx44dQP34O9Tv/bB27VoA1q5d++n5QAiJQAe4enBX7vxiH15Yv4/fv/Gx1+WISACMGTOG/fv3M3LkSDIzM4mLi2PMmDF07tyZX/3qV1x22WUMGjSIIUOGcPXVV3/u+XFxceTl5TFp0iRGjx59xnHre++9l/z8fAYOHMhPfvITnn76aQB+9rOfUVpayoABAxg0aBBvvvkmGRkZ5OXlcd111zFo0CCmTJkCwPXXX8/hw4cZPHgws2fPpk+fPv7/xfhYc3qzZnYH8B3AAZuAbznnqhqcvxH4H2Cv79Ajzrk/nu41c3Nznb93LHLOceeCjTy3tpDfTRnEtRdl+fX1RcLZli1b6N+/v9dlhJWmfudmtsY5l9tU+zP20M2sK3ArkOucGwBEAlObaPo359xg3+20YR4oZsavrruQET078J8LN7Jqe4kXZYiIeKK5Qy5RQLyZRQEJwL7AlXRuYqIieHxaLt06JDDzz2vYVlTudUki0sbNmTOHwYMHf+b2/e9/3+uyWqy5Qy63Ab8EKoHXnXM3NDp/I/AroAj4CLjDObenideZCcwEyM7OHrpr1yn3Oj1nu0squPbR5STFRfH8zZfQQdMZRc7Jli1b6Nevn6642EqccxQUFPh9yCUVuBroAXQBEs1sWqNmLwE5zrmBwBvA06coMM85l+ucy/1kvmigZKclkDc9l/1Hqpg5N5+qGk1nFDkXcXFxlJSUaBZZK/hkg4u4uLgWPa85C4smAjucc0UAZrYIGAX8pcGbNxysfgJ4oEVVBMjQ7qn89muD+MEz67j77+/z68mDvC5JJGhlZWVRWFhIUVGR16WEhU+2oGuJ5gT6bmCEmSVQP+QyAfjM9BQz6+yc2+97eBWwpUVVBNCVA7uwesdhnlm1m/+68nzaxTV/fz4R+bfo6OgWbYcmre+MQy7OuVXAQmAt9VMWI4A8M/u5mV3la3armW02sw3Uz4i5MUD1npVJF3bmZJ3jnY+KvS5FRCRgmnUtF+fcPcA9jQ7f3eD8XcBdfqzLr4Z2TyU5PprFBQeZNLCz1+WIiAREyKwUPZ2oyAgu65vBWx8WUVunL3REJDSFRaADjO+fyeHj1azfU+p1KSIiARE2gX5pnwwiI4w3thzyuhQRkYAIm0BPjo9mWE4qi7cc9LoUEZGACJtAB5jYP5OPDpaz53CF16WIiPhdWAX6+H4dAdRLF5GQFFaB3jMjiZ7piSwu0Di6iISesAp0gAn9O7Jq+2HKT5z0uhQREb8Ku0Af3y+T6to6ln2s61GISGgJu0DPzUmlfVyUpi+KSMgJu0CPjozg0r4debPgkFaNikhICbtAB5jYvyMlx6vZUFjmdSkiIn4TloH+yapRTV8UkVASloGekhDD0O6pLNY4uoiEkLAMdKgfdik4cIzCUq0aFZHQELaBPr5fJgBvapGRiISIsA308zISyUlL0PRFEQkZYRvoZsb4fpms2FbCca0aFZEQELaBDvXj6NW1dSzbqr1GRST4NSvQzewO3ybQ75vZPDOLa3Q+1sz+ZmZbzWyVmeUEolh/G9ajA+1iozR9UURCwhkD3cy6ArcCuc65AUAkMLVRs28Dpc65XsDvgAf8XWggREdGMLZvBksKiqjTqlERCXLNHXKJAuLNLApIAPY1On818LTv/kJggpmZf0oMrIn9O1JcfoKNe494XYqIyDk5Y6A75/YCDwK7gf3AEefc642adQX2+NqfBI4AaY1fy8xmmlm+meUXFbWNqx2O69ORCIMlGnYRkSDXnCGXVOp74D2ALkCimU1r3KyJp35uDMM5l+ecy3XO5WZkZJxNvX6Xmli/alTTF0Uk2DVnyGUisMM5V+ScqwEWAaMatSkEugH4hmWSgcP+LDSQJvTP5IP9R9l/pNLrUkREzlpzAn03MMLMEnzj4hOALY3avAh803d/MrDEORc03zJO+HSvUfXSRSR4NWcMfRX1X3SuBTb5npNnZj83s6t8zZ4E0sxsK/BD4CcBqjcgenVMIrtDgqYvikhQi2pOI+fcPcA9jQ7f3eB8FfBVP9bVqupXjXbkmdW7qag+SUJMs34tIiJtSlivFG1oYv9Mqk/WsXxrideliIicFQW6z8U9OpAUG8WSAg27iEhwUqD7xERFMLZPOou3HNKqUREJSgr0Bib0y+TQsRNs3nfU61JERFpMgd7AuL4ZmMEbmu0iIkFIgd5AWlIsQ7JTWaxxdBEJQgr0Rsb368j7e49y4EiV16WIiLSIAr2Rif3r9xpdor1GRSTIKNAb6ZOZRNeUeE1fFJGgo0BvxMyY2L8jy7YWU1VT63U5IiLNpkBvwoT+mVTV1LFce42KSBBRoDdheM8OpCZE8+PnNvHuNoW6iAQHBXoTYqMimT9zJO3jo5j2x1U8suRjrR4VkTZPgX4KfTu148UfjObKgV148PWPuPFP73H4eLXXZYmInJIC/TSSYqN4aOpgfnntAFZuK+GKh5aSvzNoNmISkTCjQD8DM+OG4d1ZdPMoYqIimJK3krx3thFEGzKJSJhQoDfTgK7JvHzraL7QP5P7/1HAjLlrOFJR43VZIiKfUqC3QPu4aGZPG8LdV57PWx8eYtIflrKxsMzrskREAAV6i5kZN43uwbOzRlJX55g8ewVzV+zUEIyIeO6MgW5mfc1sfYPbUTO7vVGbcWZ2pEGbu0/1eqFiSHYqr9w6hkt6pXH33zfzg3nrOFalIRgR8c4Zd0N2zn0IDAYws0hgL/B8E02XOueu9G95bVtqYgxPfnMYj7+znQdf/5AP9h3lT98aRve0RK9LE5Ew1NIhlwnANufcrkAUE4wiIozvjTuPZ74znLKKaqY8vpIdxce9LktEwlBLA30qMO8U50aa2QYz+6eZXdBUAzObaWb5ZpZfVFTUwrdu24b3TOOZGSOorq1jyuMr2Hqo3OuSRCTMNDvQzSwGuApY0MTptUB359wg4A/AC029hnMuzzmX65zLzcjIOJt627T+ndszf+YI6hxMzVvJRwePeV2SiISRlvTQLwfWOuc+d6Fw59xR51y57/4/gGgzS/dTjUGlT2Y75s8cQYTVh/qW/dpwWkRaR0sC/eucYrjFzDqZmfnuX+x73ZJzLy849eqYxN++O5KYyAi+/sRK3t97xOuSRCQMNCvQzSwB+AKwqMGxWWY2y/dwMvC+mW0AHgamujCfmN0jPZG/fXcEiTFR/McTK7UASUQCzrzK3dzcXJefn+/Je7emPYcr+PoTKzlSWcPcmy7mouxUr0sSkSBmZmucc7lNndNK0QDr1iGBZ787kg6JMXzjydW6WqOIBIwCvRV0SYnnbzNH0rFdLNOfWs2q7WH79YKIBJACvZV0So5j/swRdEmJ58Y57/Gu9isVET9ToLeiju3jmDdjBNkdEvjWn97jnY9Ca3GViHhLgd7KMtrF8syM4fRIT+Q7c/NZuKaQ7UXlVNXUel2aiAS5M16cS/wvLSmWeTNG8I2nVnHngg2fHu/YLpauqfFkpSbQNSWerNR/37qmJBAfE+lh1SLS1inQPZKaGMPCWaNYv6eMvaWVFJZWsresgsLSSjYWlvHq+/upqf3slNK0xBiyUuP53rjz+PKAzh5VLiJtlQLdQ3HRkYzomdbkudo6x6FjVQ3CvpLC0gre21nKrfPXs+h7CQzomtzKFYtIW6aFRUGmpPwEV/5hGZERxsu3jCYlIcbrkkSkFWlhUQhJS4pl9rShHDp6gtvmr6euLqyvsCAiDSjQg9Dgbinc/ZXzefujIh5a/LHX5YhIG6FAD1I3DM/m+iFZPLT4Y94sOOR1OSLSBijQg5SZ8ctrB3B+5/bcNn8du0sqvC5JRDymQA9icdGRPDZtKACz/rJGi5NEwpwCPchlpyXw+6mD+WD/UX72wvuE+WXoRcKaAj0EjO+Xya0TerNwTSHzVu/xuhwR8YgCPUTcNqE3l/bJ4N4XN7N+j3ZHEglHCvQQERlh/H7KYDLaxXLzX9ZQUn7C65JEpJUp0ENIamIMj00bSvHxam6bv55aLToSCStnDHQz62tm6xvcjprZ7Y3amJk9bGZbzWyjmQ0JXMlyOhdmJXPf1QNYtrWY3/7rQ6/LEZFWdMaLcznnPgQGA5hZJLAXeL5Rs8uB3r7bcGC276d44GvDurF2dyn/++Y2BmWl8MULOnldkoi0gpYOuUwAtjnndjU6fjUw19VbCaSYma7v6qF7r7qAC7sm86NnN7Cj+LjX5YhIK2hpoE8F5jVxvCvQcL5coe+YeCQuOpLZ04YQGWl87y9r2KlQFwl5zQ50M4sBrgIWNHW6iWOf+0bOzGaaWb6Z5RcVaT/NQMtKTeDhqRexveg44x58i28+tZolBQf1ZalIiGpJD/1yYK1z7mAT5wqBbg0eZwH7GjdyzuU553Kdc7kZGRktq1TOytg+GSz98WXcNqE3W/Yf5aY/5TPuwTd5/O1tlB6v9ro8EfGjZm9wYWbzgdecc3OaODcJ+AFwBfVfhj7snLv4dK+nDS5aX01tHa9tPsDcFbtYveMwsVERfGVQF6aP7M7ArBSvyxORZjjdBhfNCnQzS6B+jLync+6I79gsAOfcY2ZmwCPAl4EK4FvOudOmtQLdWwUHjvLnFbt4ft1eKqprGdwthekju3PFhZ2Ji9Zm1CJt1TkHeiAo0NuGo1U1LFpTyNyVu9hedJwOiTFMGdaNG4Znk5Wa4HV5ItKIAl3OyDnH8q0lzF2xkze2HMTM+PKATswY05PB3TQcI9JWnC7Qz7iwSMKDmTG6dzqje6ezt6ySuSt28syq3byycT/DclL59uiefOH8TCIjmprQJCJtgXrockrlJ07y7Ht7eGr5DgpLK8lJS+Cm0T2YPDSLhBj1BUS8oCEXOScna+t4/YODPLF0O+t2l5EcH80Nw7P55qgcMtvHeV2eSFhRoIvfrNl1mD8u3cFrmw8QGWFcNagr3xnTg/6d23tdmkhY0Bi6+M3Q7h0Y2r0Du0sqeGr5Dp7N38NzawsZ3SudKcO6MbZPBsnx0V6XKRKW1EOXc3KkooZnVu/m6Xd3cuBoFZERxrCcVCb0y2R8/470TE+kfpmCiPiDhlwk4GrrHOv3lLGk4CCLtxyi4MAxAHLSEhjfL5MJ/TsyLKcDMVHaU0XkXCjQpdXtLatkScEhlmw5yPJtJVSfrCMpNoqxfdIZ3y+TcX0zSE+K9bpMkaCjQBdPVVSf5N2tJSwuOMSSgoMcPHoCMxiYlULvjkl0TYknKzWerqnxZKUk0DkljuhI9eRFmqIvRcVTCTFRTDw/k4nnZ+LcADbvO8qSgkMs+7iYZR8Xc/BYFQ37FREGme3jPhP0XVMSyEqNJz0plogWZn2P9ERio3R9Ggl96qGL56pP1rH/SCWFpZXsLa2ksKySwtIK9pZWsreskv1Hqs7pGu69Oybx528Pp1Oy5sxL8FMPXdq0mKgIuqcl0j0tscnzJ2vrOHjsBIWHKzjcwmu4H6ms4b5XtnD97Hf563eGk5Pe9HuIhAIFurR5UZERdE2Jp2tK/Fk9/4IuyXxzzmomP7aCuTddzPldtAhKQpO+eZKQd2FWMs9+dyQxkcaUvBXk7zzsdUkiAaFAl7DQq2MSC743ioykWKY9uYq3PjzkdUkifqdAl7DRNSWeZ2eN5LyMJGbMzeelDZ/b9lYkqCnQJaykJ8Uyb+YILuqWyq3z1/HMqt1elyTiNwp0CTvt46J5+qaLGdcng58+v4nZb23zuiQRv1CgS1iKj4kkb3ouVw3qwgOvFvCrf27BqzUZIv7SrEA3sxQzW2hmBWa2xcxGNjo/zsyOmNl63+3uwJQr4j/RkRH8fspgvjGiO4+/vZ27Fm06pwVMIl5r7jz0h4BXnXOTzSwGaGo7+KXOuSv9V5pI4EVEGD+/+gJSEqL5w5KtHKs6yW+nDNKlAiQonTHQzaw9MBa4EcA5Vw20bLmeSBtmZvzoi31Jjo/mvle2cLSqhhuGdyclIZrUhBhSEqJJSYhWyEub15week+gCJhjZoOANcBtzrnjjdqNNLMNwD7gTufc5sYvZGYzgZkA2dnZ51S4iL99Z0xP2sdHc9eiTSz9uPhz5+OjI0lNiCY5IYZUX8gnx9ff79upHVcP7upB1SL/dsaLc5lZLrASuMQ5t8rMHgKOOuf+q0Gb9kCdc67czK4AHnLO9T7d6+riXNJWlZSf4MDRKsoqaiirqKG0opojlTWUVVRT6jtWVlFNme9YWUUNJ+scr98xlj6Z7bwuX0LcuV6cqxAodM6t8j1eCPykYQPn3NEG9/9hZo+aWbpz7vPdHJE2Li0plrQWbL5RXH6CEfcvZuGaQn56Rf8AViZyemec5eKcOwDsMbO+vkMTgA8atjGzTubbONLMLva9bomfaxVpk9KTYrmsX0cWrd1LTW2d1+VIGGvuPPRbgL+a2UZgMHC/mc0ys1m+85OB931j6A8DU50m9UoY+erQLIrLT/D2h0VelyJhrFnTFp1z64HGYzaPNTj/CPCIH+sSCSqX9etIelIMC9cUMvH8TK/LkTCllaIifhAdGcE1g7uyuOBgizfhEPEXBbqIn0zOzaKm1vHCur1elyJhSoEu4if9OrXnwq7JLFhT6HUpEqYU6CJ+9NXcLLbsP8r7e494XYqEIQW6iB9dNagLMZERLFQvXTygQBfxo5SEGL5wfiZ/X7+X6pOaky6tS4Eu4meTc7Morahh8ZaDXpciYUaBLuJnY3tnkNk+Vl+OSqtToIv4WWSEcd2QLN7+qIhDR6u8LkfCiAJdJAAmD82its7xvOakSytSoIsEwHkZSQzJTmHBmkLtVSqtRoEuEiBfze3G1kPlrN9T5nUpEiYU6CIBMmlgZ+KiNSddWo8CXSRA2sdF8+ULOvHihn1U1dR6XY6EAQW6SAB9Nbcbx6pO8trmA16XImFAgS4SQCN7ptE1JV7DLtIqFOgiARQRYVw/NItlW4vZW1bpdTkS4hToIgE2eUgWzsHza9VLl8BSoIsEWHZaAsN7dGCh5qRLgDUr0M0sxcwWmlmBmW0xs5GNzpuZPWxmW81so5kNCUy5IsHpq7nd2FlSwXs7S70uRUJYc3voDwGvOuf6AYOALY3OXw709t1mArP9VqFICLjiwk4kxkSyIH+P16VICDtjoJtZe2As8CSAc67aOdd46dvVwFxXbyWQYmad/V6tSJBKiIli0sDOvLJpP8dPnPS6HAlRzemh9wSKgDlmts7M/mhmiY3adAUadj0Kfcc+w8xmmlm+meUXFRWdddEiwWjy0G5UVNfyz/c1J10CozmBHgUMAWY75y4CjgM/adTGmnje5779cc7lOedynXO5GRkZLS5WJJgNy0klJy1Bwy4SMM0J9EKg0Dm3yvd4IfUB37hNtwaPs4B9516eSOgwMyYPzWLVjsPsLqnwuhwJQWcMdOfcAWCPmfX1HZoAfNCo2YvAdN9slxHAEefcfv+WKhL8rhuShRks1Jx0CYDmznK5BfirmW0EBgP3m9ksM5vlO/8PYDuwFXgCuNnvlYqEgC4p8Yzulc5zawqpq9OcdPGvqOY0cs6tB3IbHX6swXkHfN+PdYmErMlDs7ht/npWbC/hkl7pXpcjIaRZgS4i/vOlCzrRLi6K/1y4kUv7ZnBRtxSGdE+lZ3oiZk3NLxBpHgW6SCuLi47kd18bzNMrdvLShn08s2o3ACkJ0fXhnp3KkO6pDMxKpl1ctLfFSlBRoIt4YOL5mUw8P5O6OsfWonLW7S5l7a4y1u4u5c0P69domEHfzHZclJ3KkOwUhnZPpWdGkseVS1tmXl0sKDc31+Xn53vy3iJt2ZHKGtbvKWPtrlLW7i5l/Z4yjlXVry6dktuNX1wzgJgoXVcvXJnZGudc4+80AfXQRdqc5PhoLu2TwaV96hff1dU5thWVs3BNIY+/s51dh4/z2LShpCTEeFyptDX6Z16kjYuIMHpntuOuK/rz+ymDWburjGsffZftReVelyZtjAJdJIhcc1FXnpkxnKOVNVzzv8t5d2ux1yVJG6JAFwkyuTkdeOH7l5DZPo7pT61m3urdXpckbYQCXSQIdeuQwHM3j+KSXunctWgT9738AbVaeRr2FOgiQap9XDRPfjOXG0fl8MdlO5g5N59yXWs9rCnQRYJYVGQE9151Ab+4+gLe+qiIybPfZW9ZpddliUcU6CIh4Bsjc5hz4zD2llZy9SPLWbdbe5eGIwW6SIgY2yeDRTePIj4mgql5K3lpg7YkCDcKdJEQ0juzHS/cfAkDs5K5Zd46fv1qAXsOazONcKGl/yIh6MTJWn666H2e822kkd0hgUt6pTHqvHRGnZdGWlKsxxXK2Trd0n8FukgI+/jgMZZvLWbZ1hJWbS/hmG8WTP/O7bnkvDQu6ZXOxT06kBirq4AECwW6iHCyto5Ne4/w7rYSlm8tJn9XKdUn64iKMC7KTmHUeelc0iudXh2Tmtz1/VQiIozkeF3mt7Uo0EXkc6pqasnfWcrybcW8u7WYTXuPcLZrk76Wm8V911yoq0C2Al1tUUQ+Jy46ktG90xndu34bvCMVNazcUcL+Fs5j31F8nKdX7GLP4UoemzaU5AT11r3SrEA3s53AMaAWONn4XwczGwf8HdjhO7TIOfdz/5UpIoGWnBDNly7odFbPHdQthR8/t5HrZi9nzo0Xk52W4OfqpDla8v9HlznnBp+qqw8s9Z0frDAXCS/XDcniz98eTnF5Ndc+upy1WtjkCQ14iYhfjOiZxqKbR5EYG8XX81byysb9XpcUdpob6A543czWmNnMU7QZaWYbzOyfZnZBUw3MbKaZ5ZtZflFR0VkVLCJt13kZSTx/8ygGdE3m+8+sZfZb2/Bq4kU4atYsFzPr4pzbZ2YdgX8Btzjn3mlwvj1Q55wrN7MrgIecc71P95qa5SISuqpqarlzwQZe3rifqcPq90GNjtSAgD+cbpZLs37Dzrl9vp+HgOeBixudP+qcK/fd/wcQbWbp51S1iAStuOhIHp56ET+4rBfz39vDt+a8x9GqGq/LCnlnDHQzSzSzdp/cB74IvN+oTSczM9/9i32vW+L/ckUkWEREGHd+qS+/njyQldtLuP7Rd3VdmQBrTg89E1hmZhuA1cArzrlXzWyWmc3ytZkMvO9r8zAw1WngTESAr+V2Y+5NF3PgaBXXPvou6/eUeV1SyNJKURFpFVsPHePGOe9RXH6C+6+9kP6d27fo+e3jo+ncPo6IiJZcmCD0aOm/iLQJxeUnmDE3n3W7z66XHhcdQc/0JM7rmETP9MRPf/bMSCQhJjwWvmvpv4i0CelJsQfGBysAAAWISURBVMybMYLlW4upqa1r0XMPH69he1E524rK2bCnjJc37qNhf7RrSjw9MxI5LyOJ83w/c3M6hNX1ZRToItKq4qIjmdA/85xfp6qmll0lFWwrKmfboXK2Fx9nW1E5C/L3cLy6FoA+mUn8z+RBDOqWcs7vFwwU6CISlOKiI+nbqR19O7X7zHHnHAePniB/12F++coWrn10OTPG9uSOiX2Ii470qNrWET7/LyIiYcHM6JQcx5UDu/DaHWOZMqwbj7+9nSseXsqaXYe9Li+gFOgiErLax0Xzq+sG8udvX8yJmjomP7aCX7z8AZW+IZlQo0AXkZA3pncGr90xlhuGZ/Pksh1c/tA7rNoeemsfFegiEhaSYqO475oLeWbGcGqdY0reSu59cTMV1Se9Ls1vFOgiElZGnZfOa7eP5cZROfzp3Z186ffv8O62Yq/L8gsFuoiEnYSYKO696gKe/e5IIs34jydW8X+f30T5ieDurWulqIiEtcrqWn7z+oc8uXwHKfHRpCfFBvw9pwzrxnfG9Dyr52qlqIjIKcTHRPKzK8/n8gs7M3fFzhavYD0bgfpHQ4EuIgIM7Z7K0O6pXpdxTjSGLiISIhToIiIhQoEuIhIiFOgiIiFCgS4iEiIU6CIiIUKBLiISIhToIiIhwrOl/2ZWBOw6y6enA6FxNZ2zE86fP5w/O4T359dnr9fdOZfRVCPPAv1cmFn+qa5lEA7C+fOH82eH8P78+uxn/uwachERCREKdBGREBGsgZ7ndQEeC+fPH86fHcL78+uzn0FQjqGLiMjnBWsPXUREGlGgByEzizaz280s0utaRKTtCNpAN7MMM1tnZv/pu4VTuP0COOCcq/W6EPEfM7vVzLaY2V+b2f4qM/tJoOvympl1MbOFDR5fZ2bvmNmPzOxKL2tra4J2DN3MvgXUArOAbzjntnlcUqsws3jgOudcs/7SS/AwswLgcufcjma0jXLONbmj8enOBTsz6whMB04ClwBfc8EaYgEQFD10M3vBzNaY2WYzm+k7fBS4C+gALDCzsNhOzzlXCTzudR2BZGY5ZvZ+g8d3mtm9ZvaWmf3O1zvbYmbDzGyRmX1sZvd5WfO5MrPHgJ7Ai2ZWY2YZvuMRZrbVzNLN7E9m9lszexN4wMxuNLNHfO0+c867T3JuzOwBM7u5weN7fT3xT/48lAF9qe/IjQF+5EGZbVZQBDpwk3NuKJAL3GpmmcBvgQnOuX7AeuA6LwuUVlPtnBsLPAb8Hfg+MAC40czSPK3sHDjnZgH7gMuA+4AbfKcmAhucc58s++4DTHTONRVkpzsXLOYDUxo8/hrwXoPHM4F4oD/QG7jFzGJar7y2LVgC/VYz2wCsBLoB3wXecc7t851fDoz2qjhpVS/6fm4CNjvn9jvnTgDbqf+zEQqeon5YAeAmYE6DcwtO893J6c4FBefcOqCjb9x8EFAK7G7QZDwwx9U75js3wINS26Q2P0xhZuOo76WMdM5VmNlb1PfIezVo5qgfT5fQcJLPdjbiGtw/4ftZ1+D+J4/b/J/n5nDO7TGzg2Y2HhjOv3vrAMdP89TTnQsmC4HJQCfqe+yNuUb39XffJxh66MlAqS/M+wEjgFhgtJl19I2d/wfwloc1in8dpL6XlmZmsUA4zmT4I/AX4Nlg73WfhfnAVOpDfWGjc/8CvgFgZn2BbKCgVatrw4Ih0F8FosxsI/XT9VYCRcAd1P/HXQ+sds793bsSxZ+cczXAz4FVwMuE51/YF4EkPjvcEhacc5uBdsBe59z+Rqf/CFT58uAZYLpvyE0I4mmLIqHMzHKB3znnxnhdiwSPkBhzFAklvsVC3+OzY+ciZ6QeuohIiAiGMXQREWkGBbqISIhQoIuIhAgFuohIiFCgi4iEiP8PMk6q0hGaBa4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "freq_df = pd.DataFrame.from_dict(fq, orient='index', columns=['word_occur'])\n",
    "freq_df.sort_values(by='word_occur', inplace=True, ascending=False)\n",
    "freq_df.word_occur = np.log2(freq_df['word_occur'])\n",
    "freq_df.head(25).plot(kind='line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Disappearing words / new words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here is a description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "words_1914 = []\n",
    "words_2014 = []\n",
    "\n",
    "for file in get_files_for_year(1914, 25):\n",
    "    words_1914.extend(extract_words(file)[1])\n",
    "    \n",
    "for file in get_files_for_year(2014, 25):\n",
    "    words_2014.extend(extract_words(file)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Development of average sentence length\n",
    "\n",
    "This is just one possible metric for the development/analysis of language complexity. There is so much more you could come up with here.\n",
    "\n",
    "Obviously our choice to discard very short sentences in the preprocessing step has an impact on the values here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1df46540548>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def avg_sentence_length_year(year, k):\n",
    "    sentence_len = []\n",
    "    for file in get_files_for_year(year, k):\n",
    "        sentences = extract_sentences(file)\n",
    "        sentence_len.extend([len(s) for s in sentences])\n",
    "    \n",
    "    return reduce(lambda a, b: a + b, sentence_len) / len(sentence_len)\n",
    "        \n",
    "sentence_len_years = []\n",
    "\n",
    "for year in range(1924, 2018):\n",
    "    sentence_len_years.append(avg_sentence_length_year(year, 20))\n",
    "    \n",
    "avg_df = pd.DataFrame(sentence_len_years, index=range(1924, 2018), columns=['avg_sent_len'])\n",
    "avg_df.plot(kind='line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here is a description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def most_common_ngrams(n, top_k, sample):\n",
    "    file_contents = []\n",
    "\n",
    "    for file in get_random_sample(sample):\n",
    "        file_contents.extend(extract_words(file)[1])\n",
    "    \n",
    "    fq_ngr = FreqDist(ngrams(file_contents, n))\n",
    "    fq_ngr.plot(top_k, cumulative=False)\n",
    "    \n",
    "most_common_ngrams(n=3, top_k=25, sample=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building model for classifying speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is the main core of the modelling task of this assignment. It is organized as follows:\n",
    "\n",
    "* Firstly, the data is splitted into **training and test sets** according to the criteria that in our opinion fits better to the nature of data and task.\n",
    "* Secondly, we describe the **methods** that are going to be used for **feature extraction** from our documents.\n",
    "* After that, we describe the **classifiers chosen** to be trained and why they were selected.\n",
    "* Then, we **train 7 models** combining the feature extraction techniques described and the classifiers selected. This is done through a cross-validated grid search in which many hyperparameters are combined. The goal of this search is to find the **best hyperparameter combination of each of the 7 models.**\n",
    "* Finally, we **compare the results from the training within and between the models.**\n",
    "* Evaluation on test data will be performed in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our whole dataset contains **380285 speeches** hold in the Icelandic parliament from 1911 to 2017. In order to perform our train-test split, we took into account the following considerations:\n",
    "\n",
    "* Documents are **classified in directories by year and month instead of decade.**\n",
    "* Decades (classes) are highly **unbalanced.** There are much more documents from laterdecades as from the earlier ones. As an example, 1912 has only 14 documents while 2011 has 13957. This may introduce bias in the training of the models if not dealt. \n",
    "\n",
    "To solve the first problem, we use the help function get_files_for_year() created above, which takes n documents from an specified year. After that, for each of the documents, the year is substituted by the decade as shown in the next two sections. This can be done iteratively through a list of years. In this way, **we obtain a dataset with a bunch of corpora labelled by decade.**\n",
    "\n",
    "To solve the problem of unbalance within classes, we **limit the number of documents to be extracted from each year to 200 for the training set.** This way, we ensure that there will not be too big differences within the number of documents sampled within the years (maximum of 200 vs minimum of 14) and neither within the decades. We choose 200 since we consider it to be a good balance for **undersampling the majority classes but not loosing as much information as we would keep it to minimum of 14**.\n",
    "\n",
    "Note that, in order to make the runtimes of our notebook shorter (feasible) we **skip intermediate decades from our classification task.** This would simulate that there were not speeches held in some decades. We like to imagine it as weird regime which combines a decade of democracy followed by a decade of dictatorship. In summary:\n",
    "\n",
    "- 1910s, 1930s, 1950s, 1970s, 1990s, and 2010s are considered.\n",
    "- Whereas 1920s, 1940s, 1960s, 1980s, and 2000s are discarded.\n",
    "\n",
    "We will perform a train/test split of the approximate proportion 80/20. We will see why it will be approximate in the next two sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8 years out of the 10 years** that form a decade are chosen for each of the 6 decades considered for the train set. **The other 2 are left for the test set**. The selection of the years was completly random. For each of the decades a maximum of 1600 documents are chosen. However, this will not be equal for all the decandes, since, as explained above, not all the years have at least 200 documents. \n",
    "\n",
    "Note that for the last decade, we just have documents until 2017. The split will be 6 years (train) vs 1 (test) in this case. Same applies for first decade (in this case, 7 vs. 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set seed for reproducibility\n",
    "random.seed(123)\n",
    "\n",
    "file_contents = []\n",
    "targets = []\n",
    "\n",
    "\n",
    "for year in [1911, 1912, 1914, 1915, 1916, 1918, 1919,\n",
    "             1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, \n",
    "             1951, 1952, 1953, 1955, 1956, 1957, 1958, 1959,\n",
    "             1970, 1971, 1972, 1973, 1974, 1975, 1978, 1979,\n",
    "             1990, 1991, 1992, 1993, 1995, 1996, 1997, 1999,\n",
    "             2010, 2011, 2012, 2013, 2014, 2016, 2017]:\n",
    "    for file in get_files_for_year(year, 200):\n",
    "        file_contents.append(extract_words(file)[1])\n",
    "        targets.append(year - year%10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's randomly choose a fixed number of documents (here currently: 200) from various different decades. Then passing (document, decade) pairs to the model below. The decade is computed by subtracting `mod(<year>, 10)` from `<year>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the other **2 years that were not selected** within the decades in the train set. In this case, **we do not have to limit the number of documents for year.** It doesn't make sense to undersample the test set since it represents \"unseen\" data. And, unseen data should be as close to reality as possible. That means, that it is normal that there are much more documents from later decades than from earlier. \n",
    "\n",
    "So, instead of 200, we will put there a very large number to be sure that all the documents from every year are selected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed for reproducibility\n",
    "random.seed(123)\n",
    "\n",
    "file_contents_test = []\n",
    "targets_test = []\n",
    "\n",
    "for year in [1913, 1917,1930, 1939, 1950, 1954, 1976, 1977, 1994, 2013,2015]:\n",
    "    for file in get_files_for_year(year, 200000):\n",
    "        file_contents_test.append(extract_words(file)[1])\n",
    "        targets_test.append(year - year%10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See classes distribution within train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([1910, 1930, 1950, 1970, 1990, 2010])\n",
      "dict_values([539, 574, 1600, 1600, 1600, 1400])\n",
      "dict_keys([1910, 1930, 1950, 1970, 1990, 2010])\n",
      "dict_values([2037, 1662, 4563, 4660, 8187, 28292])\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(Counter(targets).keys()) \n",
    "print(Counter(targets).values()) \n",
    "\n",
    "print(Counter(targets_test).keys()) \n",
    "print(Counter(targets_test).values()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that although there are some differences within the classes for the train split, it is acceptable to perform the classification task. Maximum within the classes for training is 1600.\n",
    "\n",
    "Test set is expected to have much more class imbalance. However, our model should dealt with it thanks to the undersampling that was performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Text feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We have considered 3 different methods for text feature extraction: Tf-idf, word2vec and doc2vec. All of them will be implemented through the corresponding functions from *sklearn* library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Helper function to transform the data so that it is in the right format for the tfidfVectorizer() function that will be used later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class JoinElement(object):\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        #joins the elements of a list (which represents a document) into a single string \n",
    "        #with a blank space separation between each word\n",
    "        return [' '.join(X[i]) for i in range(len(X))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "More information about it: [sklearn documentation.](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<ins>Original paper</ins>:\n",
    "Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed representations of words and phrases and their compositionality. Advances in neural information processing systems, 26, 3111-3119.\n",
    "\n",
    "With this model every word is assigned a unique vector of configurable cardinality such that the dot product of two randomly chosen vectors should be proportional to the semantic similarity for the associated words. This happens during the training step using logistic regression and sliding windows. Personally I found that this video delivers a solid explanation of the concepts: https://www.youtube.com/watch?v=QyrUentbkvw\n",
    "\n",
    "However, since we are working with entire documents as training items we have to somehow aggregate the vectors for every word in a given document. This can be done e.g. by taking the mean and/or summing up the vectors (see `MeanEmbeddingVectorizer`), optionally weighted by TF-IDF (see `MeanEmbeddingVectorizerTfidf`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(BaseEstimator):\n",
    "    def fit(self, X, y):\n",
    "        self.word2vec = Word2Vec(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec.wv[w] for w in words if w in self.word2vec.wv.vocab]\n",
    "                    or [np.zeros(self.word2vec.vector_size)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizerTfidf(BaseEstimator):\n",
    "    def fit(self, X, y):\n",
    "        self.word2vec = Word2Vec(X)\n",
    "        self.X_joined = [' '.join(X[i]) for i in range(len(X))]\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.transformed = self.vectorizer.fit_transform(self.X_joined)\n",
    "        self.transformed = pd.DataFrame.sparse.from_spmatrix(self.transformed)\n",
    "        return self\n",
    "    \n",
    "    def tfidf(self, w, docid):\n",
    "        if w in self.vectorizer.vocabulary_:\n",
    "            return self.transformed[self.vectorizer.vocabulary_[w]][docid]\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec.wv[w] * self.tfidf(w, i) for w in words if w in self.word2vec.wv.vocab]\n",
    "                    or [np.zeros(self.word2vec.vector_size)], axis=0)\n",
    "            for i, words in enumerate(X)\n",
    "        ])\n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        self = self.fit(X, y)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Finally we are attempting to build a model using _Doc2Vec_. After training this model with our training corpus we receive a vector of configurable cardinality for each document.\n",
    "\n",
    "<ins>Original paper</ins>: Le, Quoc, and Tomas Mikolov. \"Distributed representations of sentences and documents.\" International conference on machine learning. 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Doc2Vectorizer(BaseEstimator):\n",
    "    def __init__(self, window=2, vector_size=100):\n",
    "        self.window = window\n",
    "        self.vector_size = vector_size\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        docs = [TaggedDocument(X[i], [y[i]]) for i in range(len(X))]\n",
    "        self.doc_vec = Doc2Vec(docs, vector_size=self.vector_size, window=self.window, min_count=1, workers=4)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return [self.doc_vec.infer_vector(X[i]) for i in range(len(X))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**BERT** (*Bidirectional Encoder Representations from Transformers*) is also interesting to look at, but we'll skip this here because we predict training a model from scratch would use up too many resources. Given more time however you could search for pretrained networks that roughly serve the purpose of classification of documents according to publication year.\n",
    "\n",
    "<ins>Paper</ins>: Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "3 different classifiers are going to be trained: Multinomial Naive Bayes, Support Vector Machines and Random Forest Classifier. All of them will be implemented using sklearn library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Multinominal Naive Bayes (MNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "MNB is a common method for document classification due to its good balance between computational efficiency and predictive performance [(Eibe, 2006)](https://www.cs.waikato.ac.nz/~eibe/pubs/FrankAndBouckaertPKDD06new.pdf). Therefore, we decided to choose it as one of our classifiers. \n",
    "\n",
    "Details on the algorithm implementation can be found in the [sklearn documentation.](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html).\n",
    "\n",
    "The set of hyperparameters chosen to perform the grid search cross-validation during the training are based on the recommendations[ from this article. ](https://medium.com/@kocur4d/hyper-parameter-tuning-with-pipelines-5310aff069d6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Support vector machines are widely used for classification purposes. What is more, it improves Multinominal Naive Bayes in terms of performance in most of the classification taks. Thus, it was also chosen as one of our classifiers to be trained.\n",
    "\n",
    "Details on the algorithm implementation can be found in [sklearn documentation.](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n",
    "\n",
    "The set of hyperparameters chosen to perform the grid search cross-validation during the training are based on the recommendations [ from this article. ](https://www.vebuso.com/2020/03/svm-hyperparameter-tuning-using-gridsearchcv/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Random Forest Classifier is one of the best methods according to the literature for classification tasks. However, the runtime may be extremly large (specially when increasing the size of the forest within grid search CV setups). \n",
    "\n",
    "Details on the algorithm implementation can be found in [sklearn documentation.](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html).\n",
    "\n",
    "The set of hyperparameters chosen to perform the grid search cross-validation during the training are based on the recommendations [ from this article. ](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are 3 methods for feature extraction and 3 classifiers, we should train 9 kind of models with their different combinations of hyperparameters. However, multinomial naive bayes does not take negative values produced by Word2Vec and Doc2Vec. Therefore, we have 7.\n",
    "\n",
    "For each model, **a grid search is performed with different combinations of hyperparameters** for the classifiers and the text extraction methods. Afterwards, the most relevant results of each of the models are stored in a pandas data frame.\n",
    "\n",
    "The goal of this grid search is to find the best combination of hyperparameters for each of our 7 combinations.\n",
    "\n",
    "Note that **ideally we should perform a random search prior to the grid search to limit the scope of the best hyperparameters** to be used and then perform a more accurate search. However, this would lead to a tedious notebook and extremly large runtimes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Model 1: TFIDF vectorizer, select K best and Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>param_MNB__alpha</th>\n",
       "      <th>param_MNB__fit_prior</th>\n",
       "      <th>param_k_best__k</th>\n",
       "      <th>param_k_best__score_func</th>\n",
       "      <th>param_tfidf__analyzer</th>\n",
       "      <th>param_tfidf__smooth_idf</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>8.014502</td>\n",
       "      <td>1.741826</td>\n",
       "      <td>0.5</td>\n",
       "      <td>False</td>\n",
       "      <td>500</td>\n",
       "      <td>&lt;function chi2 at 0x000001DF45FE41F8&gt;</td>\n",
       "      <td>word</td>\n",
       "      <td>True</td>\n",
       "      <td>0.79</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>7.048457</td>\n",
       "      <td>1.596769</td>\n",
       "      <td>0.5</td>\n",
       "      <td>False</td>\n",
       "      <td>500</td>\n",
       "      <td>&lt;function chi2 at 0x000001DF45FE41F8&gt;</td>\n",
       "      <td>word</td>\n",
       "      <td>False</td>\n",
       "      <td>0.79</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>6.818281</td>\n",
       "      <td>1.615518</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>False</td>\n",
       "      <td>500</td>\n",
       "      <td>&lt;function chi2 at 0x000001DF45FE41F8&gt;</td>\n",
       "      <td>word</td>\n",
       "      <td>False</td>\n",
       "      <td>0.78</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>6.993267</td>\n",
       "      <td>1.632762</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>False</td>\n",
       "      <td>500</td>\n",
       "      <td>&lt;function chi2 at 0x000001DF45FE41F8&gt;</td>\n",
       "      <td>word</td>\n",
       "      <td>True</td>\n",
       "      <td>0.78</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>7.149327</td>\n",
       "      <td>1.662384</td>\n",
       "      <td>1.16667</td>\n",
       "      <td>False</td>\n",
       "      <td>500</td>\n",
       "      <td>&lt;function chi2 at 0x000001DF45FE41F8&gt;</td>\n",
       "      <td>word</td>\n",
       "      <td>True</td>\n",
       "      <td>0.77</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model  mean_fit_time  mean_score_time param_MNB__alpha  \\\n",
       "6      1       8.014502         1.741826              0.5   \n",
       "7      1       7.048457         1.596769              0.5   \n",
       "15     1       6.818281         1.615518         0.833333   \n",
       "14     1       6.993267         1.632762         0.833333   \n",
       "22     1       7.149327         1.662384          1.16667   \n",
       "\n",
       "   param_MNB__fit_prior param_k_best__k  \\\n",
       "6                 False             500   \n",
       "7                 False             500   \n",
       "15                False             500   \n",
       "14                False             500   \n",
       "22                False             500   \n",
       "\n",
       "                 param_k_best__score_func param_tfidf__analyzer  \\\n",
       "6   <function chi2 at 0x000001DF45FE41F8>                  word   \n",
       "7   <function chi2 at 0x000001DF45FE41F8>                  word   \n",
       "15  <function chi2 at 0x000001DF45FE41F8>                  word   \n",
       "14  <function chi2 at 0x000001DF45FE41F8>                  word   \n",
       "22  <function chi2 at 0x000001DF45FE41F8>                  word   \n",
       "\n",
       "   param_tfidf__smooth_idf  mean_test_score  rank_test_score  \n",
       "6                     True             0.79                1  \n",
       "7                    False             0.79                2  \n",
       "15                   False             0.78                4  \n",
       "14                    True             0.78                3  \n",
       "22                    True             0.77                5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#choose parameters for the different steps in the pipeline\n",
    "parameters_model_1 = {\n",
    "    \n",
    "    #select KBest\n",
    "    #k number of top features to select (default 10)\n",
    "    \"k_best__k\": [10, 500],\n",
    "    #score function to be used (default f_classif)\n",
    "    \"k_best__score_func\": [chi2],\n",
    "    \n",
    "    #MultinomialNaiveBayes\n",
    "    #alpha is a parameter for smoothing (default value is 1)\n",
    "    \"MNB__alpha\": np.linspace(0.5, 1.5, 4), \n",
    "    #whether to learn class prior probabilities or not (dafult value is True)\n",
    "    \"MNB__fit_prior\": [True,False],\n",
    "    \n",
    "    #TFIDF Vectorizer\n",
    "    #Whether the feature should be made of word or character n-grams (default word)\n",
    "    \"tfidf__analyzer\": [\"word\"],\n",
    "    #Smooth idf weights by adding one to document frequencies, \n",
    "    #as if an extra document was seen containing every term in the collection exactly once.\n",
    "    #Prevents zero divisions (default True)\n",
    "    \"tfidf__smooth_idf\": [True, False]\n",
    "    \n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "#build a pipeline\n",
    "model_1_pipeline = Pipeline([\n",
    "        #joins list into a single string\n",
    "        ('join', JoinElement()),\n",
    "        #tfidf vectorizer\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        #select 1000 best from word vectors\n",
    "        ('k_best', SelectKBest()),\n",
    "        #apply naive bayes\n",
    "        ('MNB', MultinomialNB())\n",
    "    ])\n",
    "\n",
    "#design grid search\n",
    "grid_search_model_1 = GridSearchCV(\n",
    "    #pipeline to be followed\n",
    "    model_1_pipeline,\n",
    "    #parameters\n",
    "    param_grid=parameters_model_1,\n",
    "    #number of folds for CV\n",
    "    cv=5,\n",
    "    #scoring to be considered for the cv\n",
    "    scoring = \"accuracy\",\n",
    "    #parallelize if possible\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#fit the grid search for training data\n",
    "grid_search_model_1.fit(file_contents, targets)\n",
    "\n",
    "#save results of cross validation\n",
    "cv_results_model_1 = pd.DataFrame(grid_search_model_1.cv_results_)\n",
    "\n",
    "#filter columns to be kept in the dataframe\n",
    "filter_col = [col for col in cv_results_model_1 if (col.startswith(\"param_\") or col.startswith(\"mean_\") or col.startswith(\"rank\"))]\n",
    "\n",
    "#save results with only filtered columns\n",
    "cv_results_model_1 = cv_results_model_1[filter_col]\n",
    "\n",
    "#save name of the model for later comparison\n",
    "cv_results_model_1.insert(loc=0, column=\"Model\", value= \"1\")\n",
    "\n",
    "#round mean_test_score\n",
    "cv_results_model_1[\"mean_test_score\"] = cv_results_model_1[\"mean_test_score\"].round(2)\n",
    "\n",
    "#show best 5 sorted by mean_test_score\n",
    "display(cv_results_model_1.sort_values(by=\"mean_test_score\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2: TFIDF vectorizer, select K best and SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>param_SVC__C</th>\n",
       "      <th>param_SVC__gamma</th>\n",
       "      <th>param_SVC__kernel</th>\n",
       "      <th>param_k_best__k</th>\n",
       "      <th>param_k_best__score_func</th>\n",
       "      <th>param_tfidf__analyzer</th>\n",
       "      <th>param_tfidf__smooth_idf</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2</td>\n",
       "      <td>12.700479</td>\n",
       "      <td>2.974798</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>linear</td>\n",
       "      <td>500</td>\n",
       "      <td>&lt;function chi2 at 0x000001DF45FE41F8&gt;</td>\n",
       "      <td>word</td>\n",
       "      <td>True</td>\n",
       "      <td>0.81</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>2</td>\n",
       "      <td>12.566272</td>\n",
       "      <td>3.009181</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>linear</td>\n",
       "      <td>500</td>\n",
       "      <td>&lt;function chi2 at 0x000001DF45FE41F8&gt;</td>\n",
       "      <td>word</td>\n",
       "      <td>False</td>\n",
       "      <td>0.81</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>2</td>\n",
       "      <td>12.502276</td>\n",
       "      <td>3.018540</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>linear</td>\n",
       "      <td>500</td>\n",
       "      <td>&lt;function chi2 at 0x000001DF45FE41F8&gt;</td>\n",
       "      <td>word</td>\n",
       "      <td>False</td>\n",
       "      <td>0.81</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>2</td>\n",
       "      <td>12.433534</td>\n",
       "      <td>3.040412</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>linear</td>\n",
       "      <td>500</td>\n",
       "      <td>&lt;function chi2 at 0x000001DF45FE41F8&gt;</td>\n",
       "      <td>word</td>\n",
       "      <td>False</td>\n",
       "      <td>0.81</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>2</td>\n",
       "      <td>12.739758</td>\n",
       "      <td>3.056037</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>linear</td>\n",
       "      <td>500</td>\n",
       "      <td>&lt;function chi2 at 0x000001DF45FE41F8&gt;</td>\n",
       "      <td>word</td>\n",
       "      <td>True</td>\n",
       "      <td>0.81</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model  mean_fit_time  mean_score_time param_SVC__C param_SVC__gamma  \\\n",
       "122     2      12.700479         2.974798          100              0.1   \n",
       "123     2      12.566272         3.009181          100              0.1   \n",
       "111     2      12.502276         3.018540          100                1   \n",
       "135     2      12.433534         3.040412          100            0.001   \n",
       "134     2      12.739758         3.056037          100            0.001   \n",
       "\n",
       "    param_SVC__kernel param_k_best__k               param_k_best__score_func  \\\n",
       "122            linear             500  <function chi2 at 0x000001DF45FE41F8>   \n",
       "123            linear             500  <function chi2 at 0x000001DF45FE41F8>   \n",
       "111            linear             500  <function chi2 at 0x000001DF45FE41F8>   \n",
       "135            linear             500  <function chi2 at 0x000001DF45FE41F8>   \n",
       "134            linear             500  <function chi2 at 0x000001DF45FE41F8>   \n",
       "\n",
       "    param_tfidf__analyzer param_tfidf__smooth_idf  mean_test_score  \\\n",
       "122                  word                    True             0.81   \n",
       "123                  word                   False             0.81   \n",
       "111                  word                   False             0.81   \n",
       "135                  word                   False             0.81   \n",
       "134                  word                    True             0.81   \n",
       "\n",
       "     rank_test_score  \n",
       "122                1  \n",
       "123                4  \n",
       "111                4  \n",
       "135                4  \n",
       "134                1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#choose parameters for the different steps in the pipeline\n",
    "parameters_model_2 = {\n",
    "\n",
    "    #select KBest\n",
    "    #k number of top features to select (default 10)\n",
    "    \"k_best__k\": [10, 500],\n",
    "    #score function to be used (default f_classif)\n",
    "    \"k_best__score_func\": [chi2],\n",
    "    \n",
    "    #SVC\n",
    "    #Specifies the kernel type to be used in the algorithm\n",
    "    \"SVC__kernel\" : [\"linear\", \"poly\", \"sigmoid\"],\n",
    "    #Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid'\n",
    "    \"SVC__gamma\": [1,0.1,0.001],\n",
    "    #Regularization parameter. The strength of the regularization is inversely proportional to C. \n",
    "    #Must be strictly positive. The penalty is a squared l2 penalty.\n",
    "    \"SVC__C\": [0.1,1, 10, 100],\n",
    "    \n",
    "    #TFIDF Vectorizer\n",
    "    #Whether the feature should be made of word or character n-grams (default word)\n",
    "    \"tfidf__analyzer\": [\"word\"],\n",
    "    #Smooth idf weights by adding one to document frequencies, \n",
    "    #as if an extra document was seen containing every term in the collection exactly once.\n",
    "    #Prevents zero divisions (default True)\n",
    "    \"tfidf__smooth_idf\": [True, False]\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "#build a pipeline\n",
    "model_2_pipeline = Pipeline([\n",
    "        #joins list into a single string\n",
    "        ('join', JoinElement()),\n",
    "        #tfidf vectorizer\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        #select 1000 best from word vectors\n",
    "        ('k_best', SelectKBest()),\n",
    "        #apply naive bayes\n",
    "        ('SVC', SVC())\n",
    "    ])\n",
    "\n",
    "#design grid search\n",
    "grid_search_model_2 = GridSearchCV(\n",
    "    #pipeline to be followed\n",
    "    model_2_pipeline,\n",
    "    #parameters\n",
    "    param_grid=parameters_model_2,\n",
    "    #number of folds for CV\n",
    "    cv=5,\n",
    "    #scoring to be considered for the cv\n",
    "    scoring = \"accuracy\",\n",
    "    #parallelize if possible\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#fit the grid search for training data\n",
    "grid_search_model_2.fit(file_contents, targets)\n",
    "\n",
    "#save results of cross validation\n",
    "cv_results_model_2 = pd.DataFrame(grid_search_model_2.cv_results_)\n",
    "\n",
    "#filter columns to be kept in the dataframe\n",
    "filter_col = [col for col in cv_results_model_2 if (col.startswith(\"param_\") or col.startswith(\"mean_\") or col.startswith(\"rank\"))]\n",
    "\n",
    "#save results with only filtered columns\n",
    "cv_results_model_2 = cv_results_model_2[filter_col]\n",
    "\n",
    "#save name of the model for later comparison\n",
    "cv_results_model_2.insert(loc=0, column=\"Model\", value= \"2\")\n",
    "\n",
    "#round mean_test_score\n",
    "cv_results_model_2[\"mean_test_score\"] = cv_results_model_2[\"mean_test_score\"].round(2)\n",
    "\n",
    "#show best 5 sorted by mean_test_score\n",
    "display(cv_results_model_2.sort_values(by=\"mean_test_score\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3: TFIDF vectorizer, select K best and Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guill\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-033bf00753a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;31m#fit the grid search for training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m \u001b[0mgrid_search_model_3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;31m#save results of cross validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    708\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 710\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    711\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1149\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1151\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    687\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 689\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1017\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1018\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1019\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    907\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    908\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 909\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    910\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    560\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    561\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 562\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    563\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    428\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#choose parameters for the different steps in the pipeline\n",
    "parameters_model_3 = {\n",
    "    \n",
    "    #select KBest\n",
    "    #k number of top features to select (default 10)\n",
    "    \"k_best__k\": [10, 500],\n",
    "    #score function to be used (default f_classif)\n",
    "    \"k_best__score_func\": [chi2],\n",
    "    \n",
    "    #RF classifier\n",
    "    #nThe number of trees in the forest (default is 100)\n",
    "    \"clf__n_estimators\" : [10,100,200],\n",
    "    #The minimum number of samples required to split an internal node (default is 2 but it is a large dataset)\n",
    "    \"clf__min_samples_split\": [10, 40, 80],\n",
    "    #The number of features to consider when looking for the best split (default \"auto\" but sparse dataset)\n",
    "    'clf__max_features': [\"auto\", 10],\n",
    "    #maximum depth of the tree (default None)\n",
    "    'clf__max_depth': [10, None],\n",
    "\n",
    "    \n",
    "    \n",
    "    #TFIDF Vectorizer\n",
    "    #Whether the feature should be made of word or character n-grams (default word)\n",
    "    \"tfidf__analyzer\": [\"word\"],\n",
    "    #Smooth idf weights by adding one to document frequencies, \n",
    "    #as if an extra document was seen containing every term in the collection exactly once.\n",
    "    #Prevents zero divisions (default True)\n",
    "    \"tfidf__smooth_idf\": [True, False]\n",
    "\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "#build a pipeline\n",
    "model_3_pipeline = Pipeline([\n",
    "        #joins list into a single string\n",
    "        ('join', JoinElement()),\n",
    "        #tfidf vectorizer\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        #select 1000 best from word vectors\n",
    "        ('k_best', SelectKBest()),\n",
    "        #apply naive bayes\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "#design grid search\n",
    "grid_search_model_3 = GridSearchCV(\n",
    "    #pipeline to be followed\n",
    "    model_3_pipeline,\n",
    "    #parameters\n",
    "    param_grid=parameters_model_3,\n",
    "    #number of folds for CV\n",
    "    cv=5,\n",
    "    #scoring to be considered for the cv\n",
    "    scoring = \"accuracy\",\n",
    "    #parallelize if possible\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#fit the grid search for training data\n",
    "grid_search_model_3.fit(file_contents, targets)\n",
    "\n",
    "#save results of cross validation\n",
    "cv_results_model_3 = pd.DataFrame(grid_search_model_3.cv_results_)\n",
    "\n",
    "#filter columns to be kept in the dataframe\n",
    "filter_col = [col for col in cv_results_model_3 if (col.startswith(\"param_\") or col.startswith(\"mean_\") or col.startswith(\"rank\"))]\n",
    "\n",
    "#save results with only filtered columns\n",
    "cv_results_model_3 = cv_results_model_3[filter_col]\n",
    "\n",
    "#save name of the model for later comparison\n",
    "cv_results_model_3.insert(loc=0, column=\"Model\", value= \"3\")\n",
    "\n",
    "#round mean_test_score\n",
    "cv_results_model_3[\"mean_test_score\"] = cv_results_model_3[\"mean_test_score\"].round(2)\n",
    "\n",
    "#show best 5 sorted by mean_test_score\n",
    "display(cv_results_model_3.sort_values(by=\"mean_test_score\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Model 4: Word2Vec and SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#choose parameters for the different steps in the pipeline\n",
    "parameters_model_4 = {\n",
    "    \n",
    "    #SVC\n",
    "    #Specifies the kernel type to be used in the algorithm\n",
    "    \"SVC__kernel\" : [\"linear\", \"poly\", \"sigmoid\"],\n",
    "    #Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid'\n",
    "    \"SVC__gamma\": [1,0.1,0.001],\n",
    "    #Regularization parameter. The strength of the regularization is inversely proportional to C. \n",
    "    #Must be strictly positive. The penalty is a squared l2 penalty.\n",
    "    \"SVC__C\": [0.1,1, 10, 100]\n",
    "    \n",
    "    #defaults for Word2Vec\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "#build a pipeline\n",
    "model_4_pipeline = Pipeline([\n",
    "        #tfidf vectorizer\n",
    "        ('word2vec', MeanEmbeddingVectorizer()),\n",
    "        #apply naive bayes\n",
    "        ('SVC', SVC())\n",
    "    ])\n",
    "\n",
    "#design grid search\n",
    "grid_search_model_4 = GridSearchCV(\n",
    "    #pipeline to be followed\n",
    "    model_4_pipeline,\n",
    "    #parameters\n",
    "    param_grid=parameters_model_4,\n",
    "    #number of folds for CV\n",
    "    cv=5,\n",
    "    #scoring to be considered for the cv\n",
    "    scoring = \"accuracy\",\n",
    "    #parallelize if possible\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#fit the grid search for training data\n",
    "grid_search_model_4.fit(file_contents, targets)\n",
    "\n",
    "#save results of cross validation\n",
    "cv_results_model_4 = pd.DataFrame(grid_search_model_4.cv_results_)\n",
    "\n",
    "#filter columns to be kept in the dataframe\n",
    "filter_col = [col for col in cv_results_model_4 if (col.startswith(\"param_\") or col.startswith(\"mean_\") or col.startswith(\"rank\"))]\n",
    "\n",
    "#save results with only filtered columns\n",
    "cv_results_model_4 = cv_results_model_4[filter_col]\n",
    "\n",
    "#save name of the model for later comparison\n",
    "cv_results_model_4.insert(loc=0, column=\"Model\", value= \"4\")\n",
    "\n",
    "#round mean_test_score\n",
    "cv_results_model_4[\"mean_test_score\"] = cv_results_model_4[\"mean_test_score\"].round(2)\n",
    "\n",
    "#show best 5 sorted by mean_test_score\n",
    "display(cv_results_model_4.sort_values(by=\"mean_test_score\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Model 5: Word2Vec and Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#choose parameters for the different steps in the pipeline\n",
    "parameters_model_5 = {\n",
    "    \n",
    "    #RF classifier\n",
    "    #nThe number of trees in the forest (default is 100)\n",
    "    \"clf__n_estimators\" : [10,100,200],\n",
    "    #The minimum number of samples required to split an internal node (default is 2 but it is a large dataset)\n",
    "    \"clf__min_samples_split\": [10, 40, 80],\n",
    "    #The number of features to consider when looking for the best split (default \"auto\" but sparse dataset)\n",
    "    'clf__max_features': [\"auto\", 10],\n",
    "    #maximum depth of the tree (default None)\n",
    "    'clf__max_depth': [10, None]\n",
    "    \n",
    "    #defaults word2vec\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "#build a pipeline\n",
    "model_5_pipeline = Pipeline([\n",
    "        #tfidf vectorizer\n",
    "        ('word2vec', MeanEmbeddingVectorizer()),\n",
    "        #apply naive bayes\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "#design grid search\n",
    "grid_search_model_5 = GridSearchCV(\n",
    "    #pipeline to be followed\n",
    "    model_5_pipeline,\n",
    "    #parameters\n",
    "    param_grid=parameters_model_5,\n",
    "    #number of folds for CV\n",
    "    cv=5,\n",
    "    #scoring to be considered for the cv\n",
    "    scoring = \"accuracy\",\n",
    "    #parallelize if possible\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#fit the grid search for training data\n",
    "grid_search_model_5.fit(file_contents, targets)\n",
    "\n",
    "#save results of cross validation\n",
    "cv_results_model_5 = pd.DataFrame(grid_search_model_5.cv_results_)\n",
    "\n",
    "#filter columns to be kept in the dataframe\n",
    "filter_col = [col for col in cv_results_model_5 if (col.startswith(\"param_\") or col.startswith(\"mean_\") or col.startswith(\"rank\"))]\n",
    "\n",
    "#save results with only filtered columns\n",
    "cv_results_model_5 = cv_results_model_5[filter_col]\n",
    "\n",
    "#save name of the model for later comparison\n",
    "cv_results_model_5.insert(loc=0, column=\"Model\", value= \"5\")\n",
    "\n",
    "#round mean_test_score\n",
    "cv_results_model_5[\"mean_test_score\"] = cv_results_model_5[\"mean_test_score\"].round(2)\n",
    "\n",
    "#show best 5 sorted by mean_test_score\n",
    "display(cv_results_model_5.sort_values(by=\"mean_test_score\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Model 6: Doc2Vec and Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#choose parameters for the different steps in the pipeline\n",
    "parameters_model_6 = {\n",
    "    \n",
    "    #SVC\n",
    "    #Specifies the kernel type to be used in the algorithm\n",
    "    \"SVC__kernel\" : [\"linear\", \"poly\", \"sigmoid\"],\n",
    "    #Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid'\n",
    "    \"SVC__gamma\": [1,0.1,0.001],\n",
    "    #Regularization parameter. The strength of the regularization is inversely proportional to C. \n",
    "    #Must be strictly positive. The penalty is a squared l2 penalty.\n",
    "    \"SVC__C\": [0.1,1, 10, 100]\n",
    "    \n",
    "    #defaults doc2vec\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "#build a pipeline\n",
    "model_6_pipeline = Pipeline([\n",
    "        #tfidf vectorizer\n",
    "        ('doc2vec', Doc2Vectorizer()),\n",
    "        #apply naive bayes\n",
    "        ('SVC', SVC())\n",
    "    ])\n",
    "\n",
    "#design grid search\n",
    "grid_search_model_6 = GridSearchCV(\n",
    "    #pipeline to be followed\n",
    "    model_6_pipeline,\n",
    "    #parameters\n",
    "    param_grid=parameters_model_6,\n",
    "    #number of folds for CV\n",
    "    cv=5,\n",
    "    #scoring to be considered for the cv\n",
    "    scoring = \"accuracy\",\n",
    "    #parallelize if possible\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#fit the grid search for training data\n",
    "grid_search_model_6.fit(file_contents, targets)\n",
    "\n",
    "#save results of cross validation\n",
    "cv_results_model_6 = pd.DataFrame(grid_search_model_6.cv_results_)\n",
    "\n",
    "#filter columns to be kept in the dataframe\n",
    "filter_col = [col for col in cv_results_model_6 if (col.startswith(\"param_\") or col.startswith(\"mean_\") or col.startswith(\"rank\"))]\n",
    "\n",
    "#save results with only filtered columns\n",
    "cv_results_model_6 = cv_results_model_6[filter_col]\n",
    "\n",
    "#save name of the model for later comparison\n",
    "cv_results_model_6.insert(loc=0, column=\"Model\", value= \"6\")\n",
    "\n",
    "#round mean_test_score\n",
    "cv_results_model_6[\"mean_test_score\"] = cv_results_model_6[\"mean_test_score\"].round(2)\n",
    "\n",
    "#show best 5 sorted by mean_test_score\n",
    "display(cv_results_model_6.sort_values(by=\"mean_test_score\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Model 7: Doc2Vec and Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#choose parameters for the different steps in the pipeline\n",
    "parameters_model_7 = {\n",
    "    \n",
    "    \n",
    "    #RF classifier\n",
    "    #nThe number of trees in the forest (default is 100)\n",
    "    \"clf__n_estimators\" : [10,100,200],\n",
    "    #The minimum number of samples required to split an internal node (default is 2 but it is a large dataset)\n",
    "    \"clf__min_samples_split\": [10, 40, 80],\n",
    "    #The number of features to consider when looking for the best split (default \"auto\" but sparse dataset)\n",
    "    'clf__max_features': [\"auto\", 10],\n",
    "    #maximum depth of the tree (default None)\n",
    "    'clf__max_depth': [10, None]\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "#build a pipeline\n",
    "model_7_pipeline = Pipeline([\n",
    "        #tfidf vectorizer\n",
    "        ('doc2vec', Doc2Vectorizer()),\n",
    "        #apply naive bayes\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "#design grid search\n",
    "grid_search_model_7 = GridSearchCV(\n",
    "    #pipeline to be followed\n",
    "    model_7_pipeline,\n",
    "    #parameters\n",
    "    param_grid=parameters_model_7,\n",
    "    #number of folds for CV\n",
    "    cv=5,\n",
    "    #scoring to be considered for the cv\n",
    "    scoring = \"accuracy\",\n",
    "    #parallelize if possible\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#fit the grid search for training data\n",
    "grid_search_model_7.fit(file_contents, targets)\n",
    "\n",
    "#save results of cross validation\n",
    "cv_results_model_7 = pd.DataFrame(grid_search_model_7.cv_results_)\n",
    "\n",
    "#filter columns to be kept in the dataframe\n",
    "filter_col = [col for col in cv_results_model_7 if (col.startswith(\"param_\") or col.startswith(\"mean_\") or col.startswith(\"rank\"))]\n",
    "\n",
    "#save results with only filtered columns\n",
    "cv_results_model_7 = cv_results_model_7[filter_col]\n",
    "\n",
    "#save name of the model for later comparison\n",
    "cv_results_model_7.insert(loc=0, column=\"Model\", value= \"7\")\n",
    "\n",
    "#round mean_test_score\n",
    "cv_results_model_7[\"mean_test_score\"] = cv_results_model_7[\"mean_test_score\"].round(2)\n",
    "\n",
    "#show best 5 sorted by mean_test_score\n",
    "display(cv_results_model_7.sort_values(by=\"mean_test_score\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare CV results from trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, the results from CV are compared within the trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ Export results from grid serach. This allows us to experiment with visualiazations and results from CV without having to rerun the whole script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results.to_csv(\"cv_results.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataframe showing the best models according to the **mean accuracy within the test folds** used for cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge cv results into 1 that keeps the relevant information\n",
    "\n",
    "#empty dataframe that will keep all the results\n",
    "cv_results = pd.DataFrame()\n",
    "\n",
    "#loop over cv results\n",
    "for i in [cv_results_model_1, cv_results_model_2, cv_results_model_3, cv_results_model_4,\n",
    "          cv_results_model_5, cv_results_model_6, cv_results_model_7]:\n",
    "    \n",
    "    #select relevant columns\n",
    "    selected = i[[\"Model\",\"mean_fit_time\",\"mean_score_time\",\"mean_test_score\"]]\n",
    "    \n",
    "    #append to cv results\n",
    "    cv_results = cv_results.append(selected)\n",
    "    \n",
    "#show models with best scores\n",
    "display(cv_results.sort_values(by=\"mean_test_score\", ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ *This table could be improved by also indicating the parameters used in each model but I thought it would be a bit overwhelming*\n",
    "\n",
    "⚠️⚠️ Models x y seem to achieve a better accuracy since they appear more often within the first positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tradeoff score vs mean fit time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plot to check if there is some kind of tradeoff between accuracy and runtime of the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by model\n",
    "groups = cv_results.groupby(\"Model\")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "for name, group in groups:\n",
    "    ax.plot(group.mean_fit_time, group.mean_test_score, marker='o', linestyle='', ms=12, label=\"Model %s\" %name)\n",
    "ax.legend(loc = 1)\n",
    "plt.xlabel(\"Mean fit time\")\n",
    "plt.ylabel(\"Mean test Score (accuracy)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️⚠️ Clear relation between runtime of the models and accuracy within test folds?\n",
    "\n",
    "⚠️⚠️ Some pre-processing or classifier takes more time to be run?\n",
    "\n",
    "⚠️⚠️ Further analysis on what it increases runtime of the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best estimator from each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by model and take best mean_test_score for each type of model\n",
    "best_models = cv_results.groupby(\"Model\")[[\"mean_test_score\"]].max()\n",
    "\n",
    "#plot\n",
    "plt.bar(best_models.index, best_models[\"mean_test_score\"])\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Mean_test_score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️⚠️ As expected best models are x and y\n",
    "\n",
    "⚠️⚠️ Worse models are x and y\n",
    "\n",
    "⚠️⚠️ Are they stable to changes in hyperparameters? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean of mean scores within the folds for each model\n",
    "stability = cv_results.groupby(\"Model\")[[\"mean_test_score\"]].mean()\n",
    "#standard deviation of the same\n",
    "stability[\"standard_deviation\"] = cv_results.groupby(\"Model\")[[\"mean_test_score\"]].std()\n",
    "\n",
    "#show\n",
    "stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️⚠️ Model x is not very stable to the change of hyperparameters. What is influencing it so much?\n",
    "\n",
    "⚠️⚠️ Show an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cv_results_model_1[\"param_MNB__alpha\"], cv_results_model_1[\"mean_test_score\"], marker=\"o\")\n",
    "plt.xlabel(\"Parameter alpha\")\n",
    "plt.ylabel(\"Mean_test_score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This grid searches helped us to see which combinations of models and hyperparameters are expected to be the best, how stable they are and other insights. However, **we cannot draw strong conclusions on this since we are still dealing with train data.** This step is only helping us to understand the models better and choose the ones with which we want to test (or validate). To proceed further we decided to select **the best combination of hyperparameters for each of the 7 models, predict on test data**, evaluate and draw conclusions. That is done in the next section of this notebook. We know that this are not strictly the best 7 models (see raw results), but we wanted to include more diversity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict on test data** using the best combinations of hyperparameters used in the models obtained in the training phase and evaluate using different metrics.\n",
    "\n",
    "⚠️ *This simulates predictions on unseen data. However, since it is done for many models and then we will choose the best model out of them, it behaves more like a validation set that would help us choose which model we would apply to actually unseen data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add models to be evaluated\n",
    "models = [\n",
    "    grid_search_model_1,\n",
    "    grid_search_model_2,\n",
    "    grid_search_model_3,\n",
    "    grid_search_model_4,\n",
    "    grid_search_model_5,\n",
    "    grid_search_model_6,\n",
    "    grid_search_model_7\n",
    "]\n",
    "\n",
    "evaluation = pd.DataFrame(columns=[\"model\"\n",
    "                                   , \"mean_fit_time\", \"accuracy\"\n",
    "                                   , \"recall_macro\", \"recall_micro\"\n",
    "                                   , \"precision_macro\", \"precision_micro\"\n",
    "                                   , \"f1_macro\", \"f1_micro\"\n",
    "                                   , \"model_definition\"\n",
    "                                  ])\n",
    "\n",
    "i = -1 # Ensure that first item is index 0 in the loop\n",
    "for model_ in models:\n",
    "    # Yucky method of finding mean fit times:\n",
    "    i = i +1\n",
    "    mean_fit_time = cv_results.groupby(\"Model\")[\"mean_fit_time\"].mean()[i]\n",
    "    \n",
    "    # Predict\n",
    "    preds = model_.best_estimator_.predict(file_contents_test)\n",
    "    model = cv_results.iloc[model_.best_index_,0]\n",
    "\n",
    "    # Calculate metrix\n",
    "    to_append = [\n",
    "            \"Model \" + str(i+1),\n",
    "            mean_fit_time, \n",
    "            accuracy_score(y_true=targets_test,y_pred=preds),\n",
    "            #choose micro or macro according to criteria\n",
    "            recall_score(y_true=targets_test,y_pred=preds, average=\"macro\"),\n",
    "            recall_score(y_true=targets_test,y_pred=preds, average=\"micro\"),\n",
    "            precision_score(y_true=targets_test,y_pred=preds, average=\"macro\"),\n",
    "            precision_score(y_true=targets_test,y_pred=preds, average=\"micro\"),\n",
    "            f1_score(y_true=targets_test,y_pred=preds, average=\"macro\"),\n",
    "            f1_score(y_true=targets_test,y_pred=preds, average=\"micro\"),\n",
    "            model_\n",
    "            ]\n",
    "    \n",
    "    #Append Metrics\n",
    "    evaluation_length = len(evaluation)\n",
    "    evaluation.loc[evaluation_length] = to_append\n",
    "    \n",
    "    # Print results and Confusion Matrix for each model\n",
    "    print(\"####################################################################\")\n",
    "    print(\"####################################################################\")\n",
    "    print(\"                        Model \"+ str(i+1) + \":\")\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    \n",
    "    print(evaluation.loc[i, 'mean_fit_time':'f1_macro'])\n",
    "    print(\"\\n\")\n",
    "    print(\"Pipeline: \")\n",
    "    print(model_.best_estimator_)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"Confusion Matrix: \")\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    plot_confusion_matrix(estimator=model_.best_estimator_\n",
    "                          , X=file_contents_test\n",
    "                          , y_true=targets_test\n",
    "                          , ax=ax\n",
    "                         )\n",
    "    plt.show()\n",
    "    \n",
    "# Print table of the models compared and sorted:\n",
    "evaluation.sort_values(by=\"accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **best estimator we found after everything is:** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main **conclusion**: for unseen data, we would choose to use the estimator from above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Draw conclusions from above. but add more evaulations and stuff....**\n",
    "**Should probably be done after proper train/test is defined, and full model is tried**\n",
    "\n",
    "Some observations:\n",
    "* Model 1,2,3 (TFIDF) seems to work best overall, no matter training size\n",
    "* Model 5,6,7 (Doc2Vec) takes by far the most time, but their accuracy greatly increased when increasing training set\n",
    "\n",
    "\n",
    "As often is the case in the fields of science, not all research leads to useable results. We ended up having to remodel our plans several times during this project, including a complete pivot of the datasets.\n",
    "\n",
    "This did however give us some insight into how larger projects are managed. This also lead us to an interesting path of looking at a relatively obscure language.\n",
    "\n",
    "Although further works is possible, we reached the conclusion that there is a change in the Icelandic spoken language throughout time, and it is therefore possible to train models that estimates which decade a given speech is from. However, take into account what explained in section 3.2: it may be also due to other factors (for instance, topic used).\n",
    "\n",
    "Overall we did work with Data-Oriented Programming best practices. We were able to develop a scientific workflow. From the given data, we managed to train a model for prediction on test data with **decent results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Works\n",
    "As we drilled down this dataset, we kept getting new ideas that we would like to experiment with, and try to gain better insight.\n",
    "Specifically, our next steps would be:\n",
    "\n",
    "## Predict Different Sources\n",
    "As it currently stands, we are trying to estimate a decade of speeches from \"Althingi\". However, the dataset has several other sources of Icelandic; both written and spoken (TV scripts, cinema and others).\n",
    "\n",
    "We would like to see if it was possible to extend our model to be able to classify the source.\n",
    "\n",
    "## Treating years as Contious Variables\n",
    "We are currently treating decades as a class. By discretizing results from a regression algorithm, we think it should be possible to keep some nominal knowledge of the ordering of the years, and thus improving our predictions. It would be also interesting to see if we can achieve also decent prediction by narrowing a bit the intervals for the years (instead of decades, lustrums). And of course, it would be interesting to rerun the model in a more powerful machine using all the decades instead of discarding the intermediate ones.\n",
    "\n",
    "## Gaining insight into Explanatory Variables\n",
    "From our results, it is clear that it is somewhat possible to predict the decades. However, we are still treating the algorithms as \"Black Boxes\". \n",
    "We would like to dive deeper into the decision treas/boundaries, to see if we can locate what it is that makes the predictions possible. It might be new words introduced, semantic changes, or something entirely different.\n",
    "\n",
    "\n",
    "## Additional Feature Extraction and Classifiers\n",
    "We would like to extend the list to include more classifiers, as well as trying to develop some additional feature extractions.\n",
    "E.g. \"Glove Embedding\"\n",
    "E.g. \"Neural networks\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "105px",
    "width": "242px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "421.771px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "desirable-transmission",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction-to-Assignment\" data-toc-modified-id=\"Introduction-to-Assignment-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction to Assignment</a></span><ul class=\"toc-item\"><li><span><a href=\"#First-draft\" data-toc-modified-id=\"First-draft-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>First draft</a></span><ul class=\"toc-item\"><li><span><a href=\"#Topic-and-Questions-to-answer\" data-toc-modified-id=\"Topic-and-Questions-to-answer-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Topic and Questions to answer</a></span></li><li><span><a href=\"#Justification-For-Limit-Of-Scope\" data-toc-modified-id=\"Justification-For-Limit-Of-Scope-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Justification For Limit Of Scope</a></span></li><li><span><a href=\"#Workflow-plan-&amp;-Project-management\" data-toc-modified-id=\"Workflow-plan-&amp;-Project-management-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Workflow plan &amp; Project management</a></span></li><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-1.1.4\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;</span>Data</a></span></li></ul></li><li><span><a href=\"#Second-Draft\" data-toc-modified-id=\"Second-Draft-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Second Draft</a></span></li><li><span><a href=\"#Pivoting-Point\" data-toc-modified-id=\"Pivoting-Point-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Pivoting Point</a></span></li><li><span><a href=\"#Language-change-in-Icelandic-Parliamentary-Speeches\" data-toc-modified-id=\"Language-change-in-Icelandic-Parliamentary-Speeches-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Language change in Icelandic Parliamentary Speeches</a></span></li></ul></li><li><span><a href=\"#Estimating-publication-year-from-Project-Gutenberg\" data-toc-modified-id=\"Estimating-publication-year-from-Project-Gutenberg-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Estimating publication year from Project Gutenberg</a></span><ul class=\"toc-item\"><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-packages\" data-toc-modified-id=\"Import-packages-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Import packages</a></span></li><li><span><a href=\"#Define-Constants\" data-toc-modified-id=\"Define-Constants-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Define Constants</a></span></li></ul></li><li><span><a href=\"#Importing-the-data\" data-toc-modified-id=\"Importing-the-data-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Importing the data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Getting-the-content\" data-toc-modified-id=\"Getting-the-content-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Getting the content</a></span></li><li><span><a href=\"#Data-Cleansing\" data-toc-modified-id=\"Data-Cleansing-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Data Cleansing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Read-a-single-file\" data-toc-modified-id=\"Read-a-single-file-2.2.2.1\"><span class=\"toc-item-num\">2.2.2.1&nbsp;&nbsp;</span>Read a single file</a></span></li><li><span><a href=\"#Return-list-of-all-words\" data-toc-modified-id=\"Return-list-of-all-words-2.2.2.2\"><span class=\"toc-item-num\">2.2.2.2&nbsp;&nbsp;</span>Return list of all words</a></span></li></ul></li></ul></li><li><span><a href=\"#Statistics\" data-toc-modified-id=\"Statistics-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Statistics</a></span><ul class=\"toc-item\"><li><span><a href=\"#First-attempt\" data-toc-modified-id=\"First-attempt-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>First attempt</a></span></li><li><span><a href=\"#Read-all-files,-and-do-preprocessing\" data-toc-modified-id=\"Read-all-files,-and-do-preprocessing-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Read all files, and do preprocessing</a></span></li><li><span><a href=\"#Compare-Word-ranking-between-titles\" data-toc-modified-id=\"Compare-Word-ranking-between-titles-2.3.3\"><span class=\"toc-item-num\">2.3.3&nbsp;&nbsp;</span>Compare Word ranking between titles</a></span></li></ul></li><li><span><a href=\"#Second-testing\" data-toc-modified-id=\"Second-testing-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Second testing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Read-in-from-the-decades-files,-and-see-the-distributions\" data-toc-modified-id=\"Read-in-from-the-decades-files,-and-see-the-distributions-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Read in from the decades files, and see the distributions</a></span></li><li><span><a href=\"#Preliminary-Conclusion\" data-toc-modified-id=\"Preliminary-Conclusion-2.4.2\"><span class=\"toc-item-num\">2.4.2&nbsp;&nbsp;</span>Preliminary Conclusion</a></span></li><li><span><a href=\"#Compare-ranking-between-upload-decades\" data-toc-modified-id=\"Compare-ranking-between-upload-decades-2.4.3\"><span class=\"toc-item-num\">2.4.3&nbsp;&nbsp;</span>Compare ranking between upload-decades</a></span></li></ul></li><li><span><a href=\"#Trying-to-fit-models-to-predict\" data-toc-modified-id=\"Trying-to-fit-models-to-predict-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Trying to fit models to predict</a></span><ul class=\"toc-item\"><li><span><a href=\"#Read-in-files\" data-toc-modified-id=\"Read-in-files-2.5.1\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;</span>Read in files</a></span></li><li><span><a href=\"#Train-models\" data-toc-modified-id=\"Train-models-2.5.2\"><span class=\"toc-item-num\">2.5.2&nbsp;&nbsp;</span>Train models</a></span></li></ul></li><li><span><a href=\"#Realisation-and-conclusion\" data-toc-modified-id=\"Realisation-and-conclusion-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Realisation and conclusion</a></span></li></ul></li><li><span><a href=\"#Studying-language-change-in-Icelandic-parliamentary-speeches\" data-toc-modified-id=\"Studying-language-change-in-Icelandic-parliamentary-speeches-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Studying language change in Icelandic parliamentary speeches</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-required-libraries\" data-toc-modified-id=\"Load-required-libraries-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Load required libraries</a></span></li><li><span><a href=\"#Get-the-data\" data-toc-modified-id=\"Get-the-data-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Get the data</a></span></li><li><span><a href=\"#Preprocessing-helpers\" data-toc-modified-id=\"Preprocessing-helpers-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>Preprocessing helpers</a></span></li></ul></li><li><span><a href=\"#Preliminary-Data-Analysis\" data-toc-modified-id=\"Preliminary-Data-Analysis-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Preliminary Data Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Zipf's-Law\" data-toc-modified-id=\"Zipf's-Law-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Zipf's Law</a></span></li><li><span><a href=\"#Disappearing-words-/-new-words\" data-toc-modified-id=\"Disappearing-words-/-new-words-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Disappearing words / new words</a></span></li><li><span><a href=\"#Development-of-average-sentence-length\" data-toc-modified-id=\"Development-of-average-sentence-length-3.3.3\"><span class=\"toc-item-num\">3.3.3&nbsp;&nbsp;</span>Development of average sentence length</a></span></li><li><span><a href=\"#n-grams\" data-toc-modified-id=\"n-grams-3.3.4\"><span class=\"toc-item-num\">3.3.4&nbsp;&nbsp;</span>n-grams</a></span></li></ul></li><li><span><a href=\"#Building-model-for-classifying-speeches\" data-toc-modified-id=\"Building-model-for-classifying-speeches-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Building model for classifying speeches</a></span><ul class=\"toc-item\"><li><span><a href=\"#Constructing-training-and-test-data\" data-toc-modified-id=\"Constructing-training-and-test-data-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>Constructing training and test data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Train-data\" data-toc-modified-id=\"Train-data-3.4.1.1\"><span class=\"toc-item-num\">3.4.1.1&nbsp;&nbsp;</span>Train data</a></span></li><li><span><a href=\"#Test-data\" data-toc-modified-id=\"Test-data-3.4.1.2\"><span class=\"toc-item-num\">3.4.1.2&nbsp;&nbsp;</span>Test data</a></span></li></ul></li><li><span><a href=\"#Text-feature-extraction\" data-toc-modified-id=\"Text-feature-extraction-3.4.2\"><span class=\"toc-item-num\">3.4.2&nbsp;&nbsp;</span>Text feature extraction</a></span><ul class=\"toc-item\"><li><span><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-3.4.2.1\"><span class=\"toc-item-num\">3.4.2.1&nbsp;&nbsp;</span>TF-IDF</a></span></li><li><span><a href=\"#Word2Vec\" data-toc-modified-id=\"Word2Vec-3.4.2.2\"><span class=\"toc-item-num\">3.4.2.2&nbsp;&nbsp;</span>Word2Vec</a></span></li><li><span><a href=\"#Doc2Vec\" data-toc-modified-id=\"Doc2Vec-3.4.2.3\"><span class=\"toc-item-num\">3.4.2.3&nbsp;&nbsp;</span>Doc2Vec</a></span></li></ul></li><li><span><a href=\"#Classifiers\" data-toc-modified-id=\"Classifiers-3.4.3\"><span class=\"toc-item-num\">3.4.3&nbsp;&nbsp;</span>Classifiers</a></span><ul class=\"toc-item\"><li><span><a href=\"#Multinominal-Naive-Bayes\" data-toc-modified-id=\"Multinominal-Naive-Bayes-3.4.3.1\"><span class=\"toc-item-num\">3.4.3.1&nbsp;&nbsp;</span>Multinominal Naive Bayes</a></span></li><li><span><a href=\"#Support-Vector-Machines\" data-toc-modified-id=\"Support-Vector-Machines-3.4.3.2\"><span class=\"toc-item-num\">3.4.3.2&nbsp;&nbsp;</span>Support Vector Machines</a></span></li><li><span><a href=\"#Random-Forest-Classifier\" data-toc-modified-id=\"Random-Forest-Classifier-3.4.3.3\"><span class=\"toc-item-num\">3.4.3.3&nbsp;&nbsp;</span>Random Forest Classifier</a></span></li></ul></li><li><span><a href=\"#Train-models\" data-toc-modified-id=\"Train-models-3.4.4\"><span class=\"toc-item-num\">3.4.4&nbsp;&nbsp;</span>Train models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model-1:-TFIDF-vectorizer,-select-K-best-and-Multinomial-Naive-Bayes\" data-toc-modified-id=\"Model-1:-TFIDF-vectorizer,-select-K-best-and-Multinomial-Naive-Bayes-3.4.4.1\"><span class=\"toc-item-num\">3.4.4.1&nbsp;&nbsp;</span>Model 1: TFIDF vectorizer, select K best and Multinomial Naive Bayes</a></span></li><li><span><a href=\"#Model-2:-TFIDF-vectorizer,-select-K-best-and-SVC\" data-toc-modified-id=\"Model-2:-TFIDF-vectorizer,-select-K-best-and-SVC-3.4.4.2\"><span class=\"toc-item-num\">3.4.4.2&nbsp;&nbsp;</span>Model 2: TFIDF vectorizer, select K best and SVC</a></span></li><li><span><a href=\"#Model-3:-TFIDF-vectorizer,-select-K-best-and-Random-Forest-Classifier\" data-toc-modified-id=\"Model-3:-TFIDF-vectorizer,-select-K-best-and-Random-Forest-Classifier-3.4.4.3\"><span class=\"toc-item-num\">3.4.4.3&nbsp;&nbsp;</span>Model 3: TFIDF vectorizer, select K best and Random Forest Classifier</a></span></li><li><span><a href=\"#Model-4:-Word2Vec-and-SVC\" data-toc-modified-id=\"Model-4:-Word2Vec-and-SVC-3.4.4.4\"><span class=\"toc-item-num\">3.4.4.4&nbsp;&nbsp;</span>Model 4: Word2Vec and SVC</a></span></li><li><span><a href=\"#Model-5:-Word2Vec-and-Random-Forest-Classifier\" data-toc-modified-id=\"Model-5:-Word2Vec-and-Random-Forest-Classifier-3.4.4.5\"><span class=\"toc-item-num\">3.4.4.5&nbsp;&nbsp;</span>Model 5: Word2Vec and Random Forest Classifier</a></span></li><li><span><a href=\"#Model-6:-Doc2Vec-and-Support-Vector-Machines\" data-toc-modified-id=\"Model-6:-Doc2Vec-and-Support-Vector-Machines-3.4.4.6\"><span class=\"toc-item-num\">3.4.4.6&nbsp;&nbsp;</span>Model 6: Doc2Vec and Support Vector Machines</a></span></li><li><span><a href=\"#Model-7:-Doc2Vec-and-Random-Forest-Classifier\" data-toc-modified-id=\"Model-7:-Doc2Vec-and-Random-Forest-Classifier-3.4.4.7\"><span class=\"toc-item-num\">3.4.4.7&nbsp;&nbsp;</span>Model 7: Doc2Vec and Random Forest Classifier</a></span></li></ul></li><li><span><a href=\"#Compare-CV-results-from-trained-models\" data-toc-modified-id=\"Compare-CV-results-from-trained-models-3.4.5\"><span class=\"toc-item-num\">3.4.5&nbsp;&nbsp;</span>Compare CV results from trained models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Raw-results\" data-toc-modified-id=\"Raw-results-3.4.5.1\"><span class=\"toc-item-num\">3.4.5.1&nbsp;&nbsp;</span>Raw results</a></span></li><li><span><a href=\"#Tradeoff-score-vs-mean-fit-time\" data-toc-modified-id=\"Tradeoff-score-vs-mean-fit-time-3.4.5.2\"><span class=\"toc-item-num\">3.4.5.2&nbsp;&nbsp;</span>Tradeoff score vs mean fit time</a></span></li><li><span><a href=\"#Best-estimator-from-each-model\" data-toc-modified-id=\"Best-estimator-from-each-model-3.4.5.3\"><span class=\"toc-item-num\">3.4.5.3&nbsp;&nbsp;</span>Best estimator from each model</a></span></li><li><span><a href=\"#Best-5-models\" data-toc-modified-id=\"Best-5-models-3.4.5.4\"><span class=\"toc-item-num\">3.4.5.4&nbsp;&nbsp;</span>Best 5 models</a></span></li></ul></li></ul></li><li><span><a href=\"#Evaluation-and-model-selection\" data-toc-modified-id=\"Evaluation-and-model-selection-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Evaluation and model selection</a></span><ul class=\"toc-item\"><li><span><a href=\"#Michaels-Testing\" data-toc-modified-id=\"Michaels-Testing-3.5.1\"><span class=\"toc-item-num\">3.5.1&nbsp;&nbsp;</span>Michaels Testing</a></span></li></ul></li></ul></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Conclusion</a></span></li><li><span><a href=\"#Further-Works\" data-toc-modified-id=\"Further-Works-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Further Works</a></span><ul class=\"toc-item\"><li><span><a href=\"#Predict-Different-Sources\" data-toc-modified-id=\"Predict-Different-Sources-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Predict Different Sources</a></span></li><li><span><a href=\"#Treating-years-as-Contious-Variables\" data-toc-modified-id=\"Treating-years-as-Contious-Variables-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Treating years as Contious Variables</a></span></li><li><span><a href=\"#Gaining-insight-into-Explanatory-Variables\" data-toc-modified-id=\"Gaining-insight-into-Explanatory-Variables-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Gaining insight into Explanatory Variables</a></span></li><li><span><a href=\"#Additional-Feature-Extraction-and-Classifiers\" data-toc-modified-id=\"Additional-Feature-Extraction-and-Classifiers-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Additional Feature Extraction and Classifiers</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-raleigh",
   "metadata": {},
   "source": [
    "# Introduction to Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-former",
   "metadata": {},
   "source": [
    "This is the third Exercise of **188.995 Data-Oriented Programming Paradigms**\n",
    "\n",
    "We are group 18, and consist of:\n",
    "* Guillermo Alamán Requena, Matr. Nr: 11937906\n",
    "* Michael Ferdinand Moser, Matr. Nr: 01123077 \n",
    "* Paul Joe Maliakel, Matr. Nr: 12012422\n",
    "* Gunnar Sjúrðarson Knudsen, Matr. Nr: 12028205\n",
    "\n",
    "In this task we were asked to choose one vaguely worded question, and then narrow the scope, figuring out how to get the data, before finally solving the question at hand.\n",
    "We chose **Question 21**, which contains:\n",
    "* How does the use of various communication languages in countries change over time?\n",
    "* Which languages grow and which disappear,  and what are their characteristics?\n",
    "* Are there other factors that correlate with the appearance or disappearance of languages?\n",
    "\n",
    "\n",
    "We soon realized that the question as stated is far too broad, and we therefore had to limit it.\n",
    "\n",
    "After having discussed amoung our groups, we came to the following plan:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-parts",
   "metadata": {},
   "source": [
    "## First draft\n",
    "### Topic and Questions to answer\n",
    "We've selected question 21, which is regarding how communication languages in countries change over time.\n",
    "\n",
    "After having discussed the data available, and planned a workflow, we've decided to try to answer the questions:\n",
    "* How has the English language changed in the past 100 years based on word frequencies, sentence length, ...?\n",
    "* Can we find parallel developments between different genres of text?\n",
    "* Can the publication year of a movie/article/whatever be predicted based on the text and its characteristics?\n",
    "\n",
    "\n",
    "### Justification For Limit Of Scope\n",
    "The sample questions stated in the task description are too broad, to be answered in a single 160 hour project.\n",
    "* Lot's of issues, such as: \n",
    "* Lack of census data; \n",
    "* other changes such as phonetic, semantic and syntactic meanings; \n",
    "* High correlation with e.g:\n",
    "    * country population\n",
    "    * age of speakers\n",
    "    * ...\n",
    "* What counts as a language? \n",
    "    * dialect? \n",
    "    * Mutually Intelligible?\n",
    "* Political dimensions\n",
    "* Multilingual people\n",
    "* How do we check accuracy of the available data?\n",
    "* ...\n",
    "\n",
    "\n",
    "Historical data for language use is likely not available for most languages, as it's topics for great research to estimate merely historical populations - especially before 1850 or so. \n",
    "The evolution of languages are much less documented. \n",
    "Lack of census data overall, but other changes are even harder to gauge, such as phonetic, semantic, and syntactic meanings. Highly correlated with population of countries, but also with \"hidden\" correlations, such as age of speakers, ... \n",
    "Even dead languages can be revived. \n",
    "\n",
    "What constitutes a language? Dialect? Mutually Intelligible? Also do not forget the political dimension, e.g. Croatian/Serbian really are just dialects of the same language but they want to keep separate. On the other end of this scheme the variant of Chinese spoken in Beijing may be drastically different from the Chinese spoken in other regions of the country, but still falls under the same \"Chinese\" umbrella to communicate unity. \n",
    "\n",
    "How much is spoken? Should we consider people who studied a language as their second, third... language? If so, how well should be the command over the language for the person to count? A1/B1/C2 level?\n",
    "%How do we check accuracy of the available data?\n",
    "\n",
    "### Workflow plan & Project management\n",
    "* Outline the plan\n",
    "    * Get, understand and clean data: articles/movie scripts/video transcripts over the years (see next section)\n",
    "    * Train-test split: keeping proportion of publication years within the splits.\n",
    "    * Preprocessing: text feature extraction, feature selection, scaling, etc. (Come back here if necessary)\n",
    "    * Visualization: evolution of words over the years, word-clouds and other relevant characteristics.\n",
    "    * Define evaluation metrics, train different models/parameters using CV and select best one for predictions.\n",
    "    * Predict, conclude, report and publish notebook in Kaggle Kernel.\n",
    "* How the work will be divided up between group members\n",
    "    * Acquisition, cleaning and prepossessing of the data will be done commonly.\n",
    "    * Each member of the group will train a model and report results using same evaluation metrics. \n",
    "    * Jointly choose the best model and conclude.\n",
    "    * Presentation, report and publishing will be also split. \n",
    "* Timeline: To be defined after review meeting    \n",
    "\n",
    "\n",
    "### Data\n",
    "\n",
    "Our goal is to get a dataset similar to:\n",
    "\n",
    "\n",
    "\n",
    "| **Corpus** | **Year Published** | **Type**               | ... |\n",
    "|------------|--------------------|------------------------|-----|\n",
    "| Text1      | 1976               | News                   | ... |\n",
    "| Text2      | 1976               | Movie Script           | ... |\n",
    "| ...        | ...                | ...                    | ... |\n",
    "| TextN      | 2009               | Scientific Article     | ... |\n",
    "\n",
    "Feature extraction from texts will be performed to obtain appropriate features for modeling. To build a dataset like this one, we will rely on the following kind sources: \n",
    "\n",
    "* \\url{https://www.kaggle.com/asad1m9a9h6mood/news-articles} - News articles from 2015 until date.\n",
    "* \\url{https://www.kaggle.com/snapcrack/all-the-news} - 143000 articles from 15 American Publications. \n",
    "* NLTK\n",
    "* ...\n",
    "\n",
    "\n",
    "## Second Draft\n",
    "After having a preliminary meeting with Univ.Prof. Dr. Hanbury and Dipl.-Ing. Dr. Piroi, who gave great input, we decided to further limit out goal to only use Project Gutenberg as a datasource, and setting our hypothesis to see wether it was possible to generate a model that predicted the publication year/decade for a set of books.\n",
    "\n",
    "## Pivoting Point\n",
    "After having done a decent portion of work, we reached to the conclusion that our dataset was not suitable to solve the question we had original set out, and we were forced to pivot.\n",
    "\n",
    "We discussed whether we wanted to change the goal from classifying, but as we were all quite interrested in a classification algorithm, and wanted to do proper NLP, we instead searched for another dataset.\n",
    "\n",
    "## Language change in Icelandic Parliamentary Speeches\n",
    "We found the dataset with all icelandic parliamentary speeches going back a century. This is further described in section 3. \n",
    "With this great dataset, our goal was to develop a model that could try to predict which decade a speech is from"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-domain",
   "metadata": {},
   "source": [
    "# Estimating publication year from Project Gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-architect",
   "metadata": {},
   "source": [
    "This was the attempt at our first hypothesis. \n",
    "We import a large corpus of books from Project Gutenberg, and cleanse the data, so it's ready for machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-variation",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We start by setting up all packages needed for the project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-library",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "behind-pursuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from builtins import str\n",
    "import os\n",
    "from six import u\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from operator import itemgetter    \n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "import random\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import math\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-removal",
   "metadata": {},
   "source": [
    "### Define Constants\n",
    "Constant that are used in this part is also set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sweet-toddler",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"processedData\"\n",
    "\n",
    "TEXT_START_MARKERS = frozenset((u(_) for _ in (\n",
    "    \"*END*THE SMALL PRINT\",\n",
    "    \"*** START OF THE PROJECT GUTENBERG\",\n",
    "    \"*** START OF THIS PROJECT GUTENBERG\",\n",
    "    \"This etext was prepared by\",\n",
    "    \"E-text prepared by\",\n",
    "    \"Produced by\",\n",
    "    \"Distributed Proofreading Team\",\n",
    "    \"Proofreading Team at http://www.pgdp.net\",\n",
    "    \"http://gallica.bnf.fr)\",\n",
    "    \"      http://archive.org/details/\",\n",
    "    \"http://www.pgdp.net\",\n",
    "    \"by The Internet Archive)\",\n",
    "    \"by The Internet Archive/Canadian Libraries\",\n",
    "    \"by The Internet Archive/American Libraries\",\n",
    "    \"public domain material from the Internet Archive\",\n",
    "    \"Internet Archive)\",\n",
    "    \"Internet Archive/Canadian Libraries\",\n",
    "    \"Internet Archive/American Libraries\",\n",
    "    \"material from the Google Print project\",\n",
    "    \"*END THE SMALL PRINT\",\n",
    "    \"***START OF THE PROJECT GUTENBERG\",\n",
    "    \"This etext was produced by\",\n",
    "    \"*** START OF THE COPYRIGHTED\",\n",
    "    \"The Project Gutenberg\",\n",
    "    \"http://gutenberg.spiegel.de/ erreichbar.\",\n",
    "    \"Project Runeberg publishes\",\n",
    "    \"Beginning of this Project Gutenberg\",\n",
    "    \"Project Gutenberg Online Distributed\",\n",
    "    \"Gutenberg Online Distributed\",\n",
    "    \"the Project Gutenberg Online Distributed\",\n",
    "    \"Project Gutenberg TEI\",\n",
    "    \"This eBook was prepared by\",\n",
    "    \"http://gutenberg2000.de erreichbar.\",\n",
    "    \"This Etext was prepared by\",\n",
    "    \"This Project Gutenberg Etext was prepared by\",\n",
    "    \"Gutenberg Distributed Proofreaders\",\n",
    "    \"Project Gutenberg Distributed Proofreaders\",\n",
    "    \"the Project Gutenberg Online Distributed Proofreading Team\",\n",
    "    \"**The Project Gutenberg\",\n",
    "    \"*SMALL PRINT!\",\n",
    "    \"More information about this book is at the top of this file.\",\n",
    "    \"tells you about restrictions in how the file may be used.\",\n",
    "    \"l'authorization à les utilizer pour preparer ce texte.\",\n",
    "    \"of the etext through OCR.\",\n",
    "    \"*****These eBooks Were Prepared By Thousands of Volunteers!*****\",\n",
    "    \"We need your donations more than ever!\",\n",
    "    \" *** START OF THIS PROJECT GUTENBERG\",\n",
    "    \"****     SMALL PRINT!\",\n",
    "    '[\"Small Print\" V.',\n",
    "    '      (http://www.ibiblio.org/gutenberg/',\n",
    "    'and the Project Gutenberg Online Distributed Proofreading Team',\n",
    "    'Mary Meehan, and the Project Gutenberg Online Distributed Proofreading',\n",
    "    '                this Project Gutenberg edition.',\n",
    ")))\n",
    "\n",
    "\n",
    "TEXT_END_MARKERS = frozenset((u(_) for _ in (\n",
    "    \"*** END OF THE PROJECT GUTENBERG\",\n",
    "    \"*** END OF THIS PROJECT GUTENBERG\",\n",
    "    \"***END OF THE PROJECT GUTENBERG\",\n",
    "    \"End of the Project Gutenberg\",\n",
    "    \"End of The Project Gutenberg\",\n",
    "    \"Ende dieses Project Gutenberg\",\n",
    "    \"by Project Gutenberg\",\n",
    "    \"End of Project Gutenberg\",\n",
    "    \"End of this Project Gutenberg\",\n",
    "    \"Ende dieses Projekt Gutenberg\",\n",
    "    \"        ***END OF THE PROJECT GUTENBERG\",\n",
    "    \"*** END OF THE COPYRIGHTED\",\n",
    "    \"End of this is COPYRIGHTED\",\n",
    "    \"Ende dieses Etextes \",\n",
    "    \"Ende dieses Project Gutenber\",\n",
    "    \"Ende diese Project Gutenberg\",\n",
    "    \"**This is a COPYRIGHTED Project Gutenberg Etext, Details Above**\",\n",
    "    \"Fin de Project Gutenberg\",\n",
    "    \"The Project Gutenberg Etext of \",\n",
    "    \"Ce document fut presente en lecture\",\n",
    "    \"Ce document fut présenté en lecture\",\n",
    "    \"More information about this book is at the top of this file.\",\n",
    "    \"We need your donations more than ever!\",\n",
    "    \"END OF PROJECT GUTENBERG\",\n",
    "    \" End of the Project Gutenberg\",\n",
    "    \" *** END OF THIS PROJECT GUTENBERG\",\n",
    ")))\n",
    "\n",
    "\n",
    "LEGALESE_START_MARKERS = frozenset((u(_) for _ in (\n",
    "    \"<<THIS ELECTRONIC VERSION OF\",\n",
    ")))\n",
    "\n",
    "\n",
    "LEGALESE_END_MARKERS = frozenset((u(_) for _ in (\n",
    "    \"SERVICE THAT CHARGES FOR DOWNLOAD\",\n",
    ")))\n",
    "\n",
    "TITLE_MARKERS = frozenset((u(_) for _ in (\n",
    "    \"Title:\",\n",
    ")))\n",
    "\n",
    "AUTHOR_MARKERS = frozenset((u(_) for _ in (\n",
    "    \"Author:\",\n",
    ")))\n",
    "DATE_MARKERS = frozenset((u(_) for _ in (\n",
    "    \"Release Date:\",\"Release Date:\"\n",
    ")))\n",
    "LANGUAGE_MARKERS = frozenset((u(_) for _ in (\n",
    "    \"Language:\",\n",
    ")))\n",
    "ENCODING_MARKERS = frozenset((u(_) for _ in (\n",
    "    \"Character set encoding:\",\n",
    ")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-humanity",
   "metadata": {},
   "source": [
    "## Importing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-stanford",
   "metadata": {},
   "source": [
    "This is a very rough first draft at importing and cleansing the data. \n",
    "Solution is heavily inspired by https://gist.github.com/mbforbes/cee3fd5bb3a797b059524fe8c8ccdc2b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-carpet",
   "metadata": {},
   "source": [
    "### Getting the content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-wonder",
   "metadata": {},
   "source": [
    "Start by downloading the repository of (english) books. This is done in bash. Only tested on Ubuntu, but mac should work the same\n",
    "\n",
    "```\n",
    "wget -m -H -nd \"http://www.gutenberg.org/robot/harvest?filetypes[]=txt&langs[]=en\"\n",
    "\n",
    "                http://www.gutenberg.org/robot/harvest?offset=40532&filetypes[]=txt&langs[]=en\n",
    "```\n",
    "Takes a few hours to run, and is stored in a folder called rawContent. \n",
    "This is then copied to another folder, and we can start to clean up the mess\n",
    "\n",
    "First we delete some dublications of the same books:\n",
    "```\n",
    "ls | grep \"\\-8.zip\" | xargs rm\n",
    "ls | grep \"\\-0.zip\" | xargs rm\n",
    "```\n",
    "We can then unzip the files, and remove the zip files\n",
    "```\n",
    "unzip \"*zip\"\n",
    "rm *.zip\n",
    "```\n",
    "\n",
    "Next we take care of some nested foldering\n",
    "```\n",
    "mv */*.txt ./\n",
    "```\n",
    "And finally, we remove all rubbish that isn't a real book:\n",
    "\n",
    "```\n",
    "ls | grep -v \"\\.txt\" | xargs rm -rf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-candle",
   "metadata": {},
   "source": [
    "### Data Cleansing\n",
    "As the data is not given in a computer-friendly format, a lot of string operations are needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-bacteria",
   "metadata": {},
   "source": [
    "#### Read a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "according-auditor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "    file = open(file_name, encoding=\"ISO-8859-1\")\n",
    "    file_content = file.read()\n",
    "\n",
    "    lines = file_content.splitlines()\n",
    "    sep = str(os.linesep)\n",
    "\n",
    "    # Initialize results for single book\n",
    "    content_lines = []\n",
    "    i = 0\n",
    "    footer_found = False\n",
    "    ignore_section = False\n",
    "\n",
    "    title = \"\"\n",
    "    author = \"\"\n",
    "    date = \"\"\n",
    "    language = \"\"\n",
    "    encoding = \"\"\n",
    "    year = 0\n",
    "\n",
    "    # Reset flags for each book\n",
    "    title_found = False\n",
    "    author_found = False\n",
    "    date_found = False\n",
    "    language_found = False\n",
    "    encoding_found = False\n",
    "\n",
    "    for line in lines:\n",
    "            reset = False\n",
    "\n",
    "            #print(line)\n",
    "            if i <= 600:\n",
    "                # Shamelessly stolen\n",
    "                if any(line.startswith(token) for token in TEXT_START_MARKERS):\n",
    "                    reset = True\n",
    "\n",
    "                # Extract Metadata\n",
    "                if title_found == False:\n",
    "                    if any(line.startswith(token) for token in TITLE_MARKERS):\n",
    "                        title_found = True\n",
    "                        title = line\n",
    "                if author_found == False:\n",
    "                    if any(line.startswith(token) for token in AUTHOR_MARKERS):\n",
    "                        author_found = True\n",
    "                        author = line\n",
    "                if date_found == False:\n",
    "                    if any(line.startswith(token) for token in DATE_MARKERS):\n",
    "                        date_found = True\n",
    "                        date = line\n",
    "                        year = int(re.findall(r'\\d{4}', date)[0])\n",
    "                if language_found == False:\n",
    "                    if any(line.startswith(token) for token in LANGUAGE_MARKERS):\n",
    "                        language_found = True\n",
    "                        language = line\n",
    "                if encoding_found == False:\n",
    "                    if any(line.startswith(token) for token in ENCODING_MARKERS):\n",
    "                        encoding_found = True\n",
    "                        encoding = line\n",
    "\n",
    "                # More theft from above\n",
    "                if reset:\n",
    "                    content_lines = []\n",
    "                    continue\n",
    "\n",
    "            # I feel like a criminal by now. Guess what? Also stolen\n",
    "            if i >= 100:\n",
    "                if any(line.startswith(token) for token in TEXT_END_MARKERS):\n",
    "                    footer_found = True\n",
    "\n",
    "                if footer_found:\n",
    "                    break\n",
    "\n",
    "            if any(line.startswith(token) for token in LEGALESE_START_MARKERS):\n",
    "                ignore_section = True\n",
    "                continue\n",
    "            elif any(line.startswith(token) for token in LEGALESE_END_MARKERS):\n",
    "                ignore_section = False\n",
    "                continue\n",
    "\n",
    "            if not ignore_section:\n",
    "                if line != \"\": # Screw the blank lines\n",
    "                    content_lines.append(line.rstrip(sep))\n",
    "                i += 1\n",
    "\n",
    "            sep.join(content_lines)\n",
    "\n",
    "    # Do more cleaning\n",
    "    for token in TITLE_MARKERS:\n",
    "        title = title.replace(token, '').lstrip().rstrip()\n",
    "    for token in AUTHOR_MARKERS:\n",
    "        author = author.replace(token, '').lstrip().rstrip()\n",
    "    for token in LANGUAGE_MARKERS:\n",
    "        language = language.replace(token, '').lstrip().rstrip()\n",
    "    for token in DATE_MARKERS:\n",
    "        date = date.replace(token, '').lstrip().rstrip()\n",
    "    for token in ENCODING_MARKERS:\n",
    "        encoding = encoding.replace(token, '').lstrip().rstrip()\n",
    "    return title, author, date, year, language, encoding, content_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-sensitivity",
   "metadata": {},
   "source": [
    "#### Return list of all words\n",
    "Currently quite an empty function. However, I assume that some cleaning of cases etc. will be done here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "subtle-socket",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(content_lines):\n",
    "    all_text_lower = \" \".join(content_lines).lower()\n",
    "    words = re.findall(r'(\\b[A-Za-z][a-z]{2,9}\\b)', all_text_lower)\n",
    "\n",
    "    # Do more cleansing. E.g. cases and stuff\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-pursuit",
   "metadata": {},
   "source": [
    "## Statistics\n",
    "We start by doing some exploratory data analysis, to see how well our scraping works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-complaint",
   "metadata": {},
   "source": [
    "### First attempt\n",
    "Trying a simple word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "approximate-fitness",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_frequencies(words):\n",
    "    frequency = {}\n",
    "    for word in words:\n",
    "        count = frequency.get(word,0)\n",
    "        frequency[word] = count + 1\n",
    "\n",
    "    word_count = len(words)\n",
    "    unique_word_count = 0\n",
    "    word_list = []\n",
    "    word_list_count = []\n",
    "    for key, value in reversed(sorted(frequency.items(), key = itemgetter(1))):\n",
    "        word_list.append(key)\n",
    "        word_list_count.append(value)\n",
    "        unique_word_count = unique_word_count + 1\n",
    "    \n",
    "    word_list_freq = [freq / word_count for freq in word_list_count]\n",
    "    \n",
    "    word_freq = pd.DataFrame(list(zip(word_list, word_list_count, word_list_freq))\n",
    "                             , columns = ['Word', 'count', 'freq'])\n",
    "    \n",
    "    word_freq['rank'] = word_freq['count'].rank(ascending = False, method=\"dense\")\n",
    "\n",
    "    return(word_freq, unique_word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-aquatic",
   "metadata": {},
   "source": [
    "### Read all files, and do preprocessing\n",
    "Well... Only ten files currently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "flexible-nirvana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all filenames\n",
    "files = [f for f in listdir(file_path) if isfile(join(file_path, f))]\n",
    "files = list(filter(lambda file: file[0].isdigit(), files))\n",
    "random.shuffle(files)\n",
    "\n",
    "\n",
    "# Do only subset\n",
    "files = files[0:10]\n",
    "\n",
    "list_of_file = []\n",
    "list_of_title = []\n",
    "list_of_author = []\n",
    "list_of_date = []\n",
    "list_of_year = []\n",
    "list_of_language = []\n",
    "list_of_encoding = []\n",
    "list_of_word_count = []\n",
    "list_of_unique_word_count = []\n",
    "list_of_word_frequencies = []\n",
    "iter_ = 0\n",
    "\n",
    "for file in files:\n",
    "    # Read in basic information from file\n",
    "    title, author, date, year, language, encoding, content_lines = read_file(file_path + \"/\" + file)\n",
    "    line_count = len(content_lines)\n",
    "\n",
    "    # Not sure if we want this for later:\n",
    "    #content_all = \" \".join(content_lines)\n",
    "    \n",
    "    # Split into words (and do various cleaning)\n",
    "    words = get_words(content_lines)\n",
    "    word_count = len(words)\n",
    "\n",
    "    # First analysis, but should do something proper\n",
    "    word_frequencies_table, unique_word_count = get_word_frequencies(words)\n",
    "    \n",
    "    # Append to results\n",
    "    list_of_file.append(file)\n",
    "    list_of_title.append(title)\n",
    "    list_of_author.append(author)\n",
    "    list_of_date.append(date)\n",
    "    list_of_year.append(year)\n",
    "    list_of_language.append(language)\n",
    "    list_of_encoding.append(encoding)\n",
    "    list_of_word_count.append(word_count)\n",
    "    list_of_unique_word_count.append(unique_word_count)\n",
    "    list_of_word_frequencies.append(word_frequencies_table)\n",
    "    \n",
    "    \n",
    "    # Show basic information\n",
    "    #print(iter_)\n",
    "    iter_ = iter_ + 1\n",
    "    #print(\"################################\")\n",
    "    #print(\"################################\")\n",
    "    #print(\"Filename: \" + str(file))\n",
    "    #print(\"Title: \" + str(title))\n",
    "    #print(\"Author(s): \" + str(author))\n",
    "    #print(\"Date: \" + str(date))\n",
    "    #print(\"Year: \" + str(year))\n",
    "    #print(\"Language: \" + str(language))\n",
    "    #print(\"Encoding: \" + str(encoding))\n",
    "    #print(\"################################\")\n",
    "    #print(\"Words in book: \" + str(word_count))\n",
    "    #print(\"Unique words in book: \" + str(unique_word_count))\n",
    "    #print(\"################################\")\n",
    "    #print(word_frequencies_table)\n",
    "\n",
    "# Feel free to change to dict? list? separate files?\n",
    "## nested dataframes works, but looks super ungly when printing\n",
    "### Fuck it - This is tooo useless killing it again\n",
    "#all_res = pd.DataFrame(list(zip(list_of_file\n",
    "#                                , list_of_title\n",
    "#                                , list_of_author\n",
    "#                                , list_of_date\n",
    "#                                , list_of_language\n",
    "#                                , list_of_encoding\n",
    "#                                , list_of_word_count\n",
    "#                                , list_of_unique_word_count\n",
    "#                                , list_of_word_frequencies\n",
    "#                                ))\n",
    "#                             , columns = ['file'\n",
    "#                                          , 'title'\n",
    "#                                          , 'author'\n",
    "#                                          , 'date'\n",
    "#                                          , 'language'\n",
    "#                                          , 'encoding'\n",
    "#                                          , 'word_count'\n",
    "#                                          , 'unique_word_count'\n",
    "#                                          , 'word_frequencies'\n",
    "#                                         ]\n",
    "#                      )\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-quantum",
   "metadata": {},
   "source": [
    "### Compare Word ranking between titles\n",
    "This is our first attemt at seeing how the ranking of words change between titles. Idea is to see that the zipf-distribution changes as time passes buy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "naval-magazine",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_count= []\n",
    "list_freq = []\n",
    "list_rank = []\n",
    "\n",
    "\n",
    "col_names = list_of_title.copy()\n",
    "col_names.insert(0,'Word')\n",
    "\n",
    "for df in list_of_word_frequencies:\n",
    "    list_count.append(df[['Word', 'count']])\n",
    "    list_freq.append(df[['Word', 'freq']])\n",
    "    list_rank.append(df[['Word', 'rank']])\n",
    "    \n",
    "df_count = reduce(lambda left, right: pd.merge(left, right, on=\"Word\", how='outer'), list_count)\n",
    "df_count.columns = col_names\n",
    "df_count['Sum'] = df_count.drop('Word', axis=1).apply(lambda x: x.sum(), axis=1)\n",
    "df_count = df_count.sort_values(ascending = False, by=['Sum'])\n",
    "\n",
    "df_freq = reduce(lambda left, right: pd.merge(left, right, on=\"Word\", how='outer'), list_freq)\n",
    "df_freq.columns = col_names\n",
    "df_freq['Avg'] = df_freq.drop('Word', axis=1).apply(lambda x: x.mean(), axis=1)\n",
    "df_freq = df_freq.sort_values(ascending = False, by=['Avg'])\n",
    "\n",
    "df_rank = reduce(lambda left, right: pd.merge(left, right, on=\"Word\", how='outer'), list_rank)\n",
    "df_rank.columns = col_names\n",
    "df_rank['Avg'] = df_rank.drop('Word', axis=1).apply(lambda x: x.mean(), axis=1)\n",
    "df_rank = df_rank.sort_values(by=['Avg'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ruled-treasury",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Early Bardic Literature, Ireland</th>\n",
       "      <th>Life's Progress Through The Passions</th>\n",
       "      <th>Worldwide Effects of Nuclear War: Some Perspectives</th>\n",
       "      <th>Birds in London</th>\n",
       "      <th>The Last Straw</th>\n",
       "      <th>Paper-bag Cookery</th>\n",
       "      <th>Man's Redemption of Man</th>\n",
       "      <th>Prey of the Space Falcon</th>\n",
       "      <th>Jacques Bonneval</th>\n",
       "      <th>The Cup of Comus</th>\n",
       "      <th>Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6702</th>\n",
       "      <td>nuclear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>with</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6703</th>\n",
       "      <td>radiation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10165</th>\n",
       "      <td>kessler</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>for</td>\n",
       "      <td>16.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3251</th>\n",
       "      <td>natura</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>but</td>\n",
       "      <td>14.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>15.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>was</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>this</td>\n",
       "      <td>9.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>16.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>from</td>\n",
       "      <td>10.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>17.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>not</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>17.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10533</th>\n",
       "      <td>butter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>his</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>18.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>all</td>\n",
       "      <td>19.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>have</td>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>19.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6705</th>\n",
       "      <td>fallout</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10535</th>\n",
       "      <td>greased</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>one</td>\n",
       "      <td>18.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>22.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>are</td>\n",
       "      <td>27.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>they</td>\n",
       "      <td>22.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>which</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>23.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11355</th>\n",
       "      <td>redemption</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>literature</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6708</th>\n",
       "      <td>global</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11356</th>\n",
       "      <td>shalt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6709</th>\n",
       "      <td>fission</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6710</th>\n",
       "      <td>fusion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Word  Early Bardic Literature, Ireland  \\\n",
       "0             the                               1.0   \n",
       "1             and                               2.0   \n",
       "6702      nuclear                               NaN   \n",
       "2            that                               3.0   \n",
       "5            with                               6.0   \n",
       "6703    radiation                               NaN   \n",
       "10165     kessler                               NaN   \n",
       "15            for                              16.0   \n",
       "3251       natura                               NaN   \n",
       "13            but                              14.0   \n",
       "4             was                               5.0   \n",
       "8            this                               9.0   \n",
       "9            from                              10.0   \n",
       "7             not                               8.0   \n",
       "10533      butter                               NaN   \n",
       "6             his                               7.0   \n",
       "19            all                              19.0   \n",
       "12           have                              13.0   \n",
       "6705      fallout                               NaN   \n",
       "10535     greased                               NaN   \n",
       "17            one                              18.0   \n",
       "28            are                              27.0   \n",
       "22           they                              22.0   \n",
       "3           which                               4.0   \n",
       "11355  redemption                               NaN   \n",
       "18     literature                              18.0   \n",
       "6708       global                               NaN   \n",
       "11356       shalt                               NaN   \n",
       "6709      fission                               NaN   \n",
       "6710       fusion                               NaN   \n",
       "\n",
       "       Life's Progress Through The Passions  \\\n",
       "0                                       1.0   \n",
       "1                                       2.0   \n",
       "6702                                    NaN   \n",
       "2                                       5.0   \n",
       "5                                       9.0   \n",
       "6703                                    NaN   \n",
       "10165                                   NaN   \n",
       "15                                     11.0   \n",
       "3251                                   15.0   \n",
       "13                                     13.0   \n",
       "4                                       4.0   \n",
       "8                                      14.0   \n",
       "9                                      17.0   \n",
       "7                                      10.0   \n",
       "10533                                   NaN   \n",
       "6                                       3.0   \n",
       "19                                     14.0   \n",
       "12                                     16.0   \n",
       "6705                                    NaN   \n",
       "10535                                   NaN   \n",
       "17                                     35.0   \n",
       "28                                     39.0   \n",
       "22                                     19.0   \n",
       "3                                      12.0   \n",
       "11355                                   NaN   \n",
       "18                                      NaN   \n",
       "6708                                    NaN   \n",
       "11356                                   NaN   \n",
       "6709                                    NaN   \n",
       "6710                                    NaN   \n",
       "\n",
       "       Worldwide Effects of Nuclear War: Some Perspectives  Birds in London  \\\n",
       "0                                                    1.0                1.0   \n",
       "1                                                    2.0                2.0   \n",
       "6702                                                 3.0                NaN   \n",
       "2                                                    4.0                3.0   \n",
       "5                                                    9.0                6.0   \n",
       "6703                                                 8.0                NaN   \n",
       "10165                                                NaN                NaN   \n",
       "15                                                   5.0                9.0   \n",
       "3251                                                 NaN                NaN   \n",
       "13                                                  17.0               15.0   \n",
       "4                                                   21.0               16.0   \n",
       "8                                                    9.0               12.0   \n",
       "9                                                    7.0               18.0   \n",
       "7                                                   24.0               12.0   \n",
       "10533                                                NaN                NaN   \n",
       "6                                                   40.0               13.0   \n",
       "19                                                  20.0               21.0   \n",
       "12                                                  17.0               10.0   \n",
       "6705                                                20.0                NaN   \n",
       "10535                                                NaN                NaN   \n",
       "17                                                  23.0               19.0   \n",
       "28                                                  12.0                5.0   \n",
       "22                                                  27.0               11.0   \n",
       "3                                                    6.0               20.0   \n",
       "11355                                                NaN                NaN   \n",
       "18                                                   NaN                NaN   \n",
       "6708                                                25.0                NaN   \n",
       "11356                                                NaN                NaN   \n",
       "6709                                                27.0                NaN   \n",
       "6710                                                27.0                NaN   \n",
       "\n",
       "       The Last Straw  Paper-bag Cookery  Man's Redemption of Man  \\\n",
       "0                 1.0                1.0                      1.0   \n",
       "1                 2.0                2.0                      2.0   \n",
       "6702              NaN                NaN                      NaN   \n",
       "2                 5.0               27.0                      3.0   \n",
       "5                10.0                4.0                      6.0   \n",
       "6703              NaN                NaN                      NaN   \n",
       "10165             8.0                NaN                      NaN   \n",
       "15               10.0                6.0                      5.0   \n",
       "3251              NaN                NaN                      NaN   \n",
       "13                6.0               33.0                     13.0   \n",
       "4                 4.0               84.0                     10.0   \n",
       "8                11.0               16.0                      7.0   \n",
       "9                26.0               25.0                     15.0   \n",
       "7                19.0               24.0                     11.0   \n",
       "10533             NaN               18.0                      NaN   \n",
       "6                 7.0               87.0                      8.0   \n",
       "19               24.0               36.0                     12.0   \n",
       "12               11.0               42.0                      9.0   \n",
       "6705              NaN                NaN                      NaN   \n",
       "10535             NaN               22.0                      NaN   \n",
       "17               14.0               15.0                     12.0   \n",
       "28               26.0               12.0                     18.0   \n",
       "22               21.0               43.0                     28.0   \n",
       "3                46.0               29.0                      4.0   \n",
       "11355             NaN                NaN                     24.0   \n",
       "18                NaN                NaN                     30.0   \n",
       "6708              NaN                NaN                      NaN   \n",
       "11356             NaN                NaN                     26.0   \n",
       "6709              NaN                NaN                      NaN   \n",
       "6710              NaN                NaN                      NaN   \n",
       "\n",
       "       Prey of the Space Falcon  Jacques Bonneval  The Cup of Comus   Avg  \n",
       "0                           1.0               1.0               1.0   1.0  \n",
       "1                           3.0               2.0               2.0   2.1  \n",
       "6702                        NaN               NaN               NaN   3.0  \n",
       "2                           5.0               6.0               4.0   6.5  \n",
       "5                           8.0               8.0               3.0   6.9  \n",
       "6703                        NaN               NaN               NaN   8.0  \n",
       "10165                       NaN               NaN               NaN   8.0  \n",
       "15                         10.0               7.0               8.0   8.7  \n",
       "3251                        NaN               NaN               NaN  15.0  \n",
       "13                         15.0              10.0              17.0  15.3  \n",
       "4                           4.0               3.0              12.0  16.3  \n",
       "8                          45.0              21.0              23.0  16.7  \n",
       "9                          11.0              28.0              15.0  17.2  \n",
       "7                          31.0               9.0              30.0  17.8  \n",
       "10533                       NaN               NaN               NaN  18.0  \n",
       "6                           2.0              12.0               7.0  18.6  \n",
       "19                         22.0              19.0               9.0  19.6  \n",
       "12                         37.0              11.0              33.0  19.9  \n",
       "6705                        NaN               NaN               NaN  20.0  \n",
       "10535                       NaN               NaN               NaN  22.0  \n",
       "17                         35.0              27.0              24.0  22.2  \n",
       "28                         36.0              16.0              33.0  22.4  \n",
       "22                         20.0              13.0              22.0  22.6  \n",
       "3                          54.0              28.0              29.0  23.2  \n",
       "11355                       NaN               NaN               NaN  24.0  \n",
       "18                          NaN               NaN               NaN  24.0  \n",
       "6708                        NaN               NaN               NaN  25.0  \n",
       "11356                       NaN               NaN               NaN  26.0  \n",
       "6709                        NaN               NaN               NaN  27.0  \n",
       "6710                        NaN               NaN               NaN  27.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rank.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "continued-doctor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Early Bardic Literature, Ireland</th>\n",
       "      <th>Life's Progress Through The Passions</th>\n",
       "      <th>Worldwide Effects of Nuclear War: Some Perspectives</th>\n",
       "      <th>Birds in London</th>\n",
       "      <th>The Last Straw</th>\n",
       "      <th>Paper-bag Cookery</th>\n",
       "      <th>Man's Redemption of Man</th>\n",
       "      <th>Prey of the Space Falcon</th>\n",
       "      <th>Jacques Bonneval</th>\n",
       "      <th>The Cup of Comus</th>\n",
       "      <th>Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>0.121750</td>\n",
       "      <td>0.068558</td>\n",
       "      <td>0.102675</td>\n",
       "      <td>0.086259</td>\n",
       "      <td>0.041425</td>\n",
       "      <td>0.064763</td>\n",
       "      <td>0.113229</td>\n",
       "      <td>0.098364</td>\n",
       "      <td>0.053267</td>\n",
       "      <td>0.076532</td>\n",
       "      <td>0.082682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "      <td>0.050349</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>0.031668</td>\n",
       "      <td>0.049755</td>\n",
       "      <td>0.024964</td>\n",
       "      <td>0.053218</td>\n",
       "      <td>0.048649</td>\n",
       "      <td>0.028665</td>\n",
       "      <td>0.047003</td>\n",
       "      <td>0.068666</td>\n",
       "      <td>0.043844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6702</th>\n",
       "      <td>nuclear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.019866</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.019866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that</td>\n",
       "      <td>0.015409</td>\n",
       "      <td>0.018447</td>\n",
       "      <td>0.012392</td>\n",
       "      <td>0.015043</td>\n",
       "      <td>0.018452</td>\n",
       "      <td>0.004449</td>\n",
       "      <td>0.013371</td>\n",
       "      <td>0.012497</td>\n",
       "      <td>0.013351</td>\n",
       "      <td>0.017490</td>\n",
       "      <td>0.014090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10165</th>\n",
       "      <td>kessler</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011758</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>with</td>\n",
       "      <td>0.009892</td>\n",
       "      <td>0.012537</td>\n",
       "      <td>0.007278</td>\n",
       "      <td>0.009268</td>\n",
       "      <td>0.009588</td>\n",
       "      <td>0.016951</td>\n",
       "      <td>0.009104</td>\n",
       "      <td>0.010564</td>\n",
       "      <td>0.011888</td>\n",
       "      <td>0.017675</td>\n",
       "      <td>0.011474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>his</td>\n",
       "      <td>0.009639</td>\n",
       "      <td>0.023447</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.006833</td>\n",
       "      <td>0.012120</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.008250</td>\n",
       "      <td>0.030276</td>\n",
       "      <td>0.009327</td>\n",
       "      <td>0.007588</td>\n",
       "      <td>0.010801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>was</td>\n",
       "      <td>0.009956</td>\n",
       "      <td>0.019726</td>\n",
       "      <td>0.003934</td>\n",
       "      <td>0.006153</td>\n",
       "      <td>0.020803</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>0.006828</td>\n",
       "      <td>0.014429</td>\n",
       "      <td>0.015317</td>\n",
       "      <td>0.005367</td>\n",
       "      <td>0.010302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>for</td>\n",
       "      <td>0.005263</td>\n",
       "      <td>0.011917</td>\n",
       "      <td>0.009245</td>\n",
       "      <td>0.007928</td>\n",
       "      <td>0.009588</td>\n",
       "      <td>0.011995</td>\n",
       "      <td>0.009673</td>\n",
       "      <td>0.010435</td>\n",
       "      <td>0.012391</td>\n",
       "      <td>0.007126</td>\n",
       "      <td>0.009556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567</th>\n",
       "      <td>you</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.004166</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>0.022974</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002276</td>\n",
       "      <td>0.011273</td>\n",
       "      <td>0.014814</td>\n",
       "      <td>0.005182</td>\n",
       "      <td>0.007695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6703</th>\n",
       "      <td>radiation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007671</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>but</td>\n",
       "      <td>0.006658</td>\n",
       "      <td>0.010677</td>\n",
       "      <td>0.004721</td>\n",
       "      <td>0.006361</td>\n",
       "      <td>0.013929</td>\n",
       "      <td>0.003886</td>\n",
       "      <td>0.005690</td>\n",
       "      <td>0.005540</td>\n",
       "      <td>0.010791</td>\n",
       "      <td>0.004257</td>\n",
       "      <td>0.007251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3251</th>\n",
       "      <td>natura</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006898</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>not</td>\n",
       "      <td>0.008497</td>\n",
       "      <td>0.012382</td>\n",
       "      <td>0.003344</td>\n",
       "      <td>0.007399</td>\n",
       "      <td>0.005970</td>\n",
       "      <td>0.004618</td>\n",
       "      <td>0.006543</td>\n",
       "      <td>0.003092</td>\n",
       "      <td>0.011568</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>0.006628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>which</td>\n",
       "      <td>0.011858</td>\n",
       "      <td>0.011801</td>\n",
       "      <td>0.009048</td>\n",
       "      <td>0.005210</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.004336</td>\n",
       "      <td>0.013087</td>\n",
       "      <td>0.001288</td>\n",
       "      <td>0.004527</td>\n",
       "      <td>0.002961</td>\n",
       "      <td>0.006466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>this</td>\n",
       "      <td>0.007736</td>\n",
       "      <td>0.007402</td>\n",
       "      <td>0.007278</td>\n",
       "      <td>0.007399</td>\n",
       "      <td>0.008683</td>\n",
       "      <td>0.006364</td>\n",
       "      <td>0.008819</td>\n",
       "      <td>0.001868</td>\n",
       "      <td>0.005487</td>\n",
       "      <td>0.003517</td>\n",
       "      <td>0.006455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>have</td>\n",
       "      <td>0.006722</td>\n",
       "      <td>0.006317</td>\n",
       "      <td>0.004721</td>\n",
       "      <td>0.007890</td>\n",
       "      <td>0.008683</td>\n",
       "      <td>0.003266</td>\n",
       "      <td>0.007681</td>\n",
       "      <td>0.002383</td>\n",
       "      <td>0.009647</td>\n",
       "      <td>0.002499</td>\n",
       "      <td>0.005981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>from</td>\n",
       "      <td>0.007356</td>\n",
       "      <td>0.006162</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>0.005663</td>\n",
       "      <td>0.004161</td>\n",
       "      <td>0.004562</td>\n",
       "      <td>0.005121</td>\n",
       "      <td>0.008181</td>\n",
       "      <td>0.004527</td>\n",
       "      <td>0.004812</td>\n",
       "      <td>0.005861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10533</th>\n",
       "      <td>butter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005801</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>had</td>\n",
       "      <td>0.001966</td>\n",
       "      <td>0.018331</td>\n",
       "      <td>0.001377</td>\n",
       "      <td>0.003756</td>\n",
       "      <td>0.007779</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.003698</td>\n",
       "      <td>0.010758</td>\n",
       "      <td>0.007773</td>\n",
       "      <td>0.001851</td>\n",
       "      <td>0.005768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word  Early Bardic Literature, Ireland  \\\n",
       "0            the                          0.121750   \n",
       "1            and                          0.050349   \n",
       "6702     nuclear                               NaN   \n",
       "2           that                          0.015409   \n",
       "10165    kessler                               NaN   \n",
       "5           with                          0.009892   \n",
       "6            his                          0.009639   \n",
       "4            was                          0.009956   \n",
       "15           for                          0.005263   \n",
       "1567         you                          0.000063   \n",
       "6703   radiation                               NaN   \n",
       "13           but                          0.006658   \n",
       "3251      natura                               NaN   \n",
       "7            not                          0.008497   \n",
       "3          which                          0.011858   \n",
       "8           this                          0.007736   \n",
       "12          have                          0.006722   \n",
       "9           from                          0.007356   \n",
       "10533     butter                               NaN   \n",
       "52           had                          0.001966   \n",
       "\n",
       "       Life's Progress Through The Passions  \\\n",
       "0                                  0.068558   \n",
       "1                                  0.035500   \n",
       "6702                                    NaN   \n",
       "2                                  0.018447   \n",
       "10165                                   NaN   \n",
       "5                                  0.012537   \n",
       "6                                  0.023447   \n",
       "4                                  0.019726   \n",
       "15                                 0.011917   \n",
       "1567                               0.004166   \n",
       "6703                                    NaN   \n",
       "13                                 0.010677   \n",
       "3251                               0.006898   \n",
       "7                                  0.012382   \n",
       "3                                  0.011801   \n",
       "8                                  0.007402   \n",
       "12                                 0.006317   \n",
       "9                                  0.006162   \n",
       "10533                                   NaN   \n",
       "52                                 0.018331   \n",
       "\n",
       "       Worldwide Effects of Nuclear War: Some Perspectives  Birds in London  \\\n",
       "0                                               0.102675           0.086259   \n",
       "1                                               0.031668           0.049755   \n",
       "6702                                            0.019866                NaN   \n",
       "2                                               0.012392           0.015043   \n",
       "10165                                                NaN                NaN   \n",
       "5                                               0.007278           0.009268   \n",
       "6                                               0.000197           0.006833   \n",
       "4                                               0.003934           0.006153   \n",
       "15                                              0.009245           0.007928   \n",
       "1567                                                 NaN           0.000812   \n",
       "6703                                            0.007671                NaN   \n",
       "13                                              0.004721           0.006361   \n",
       "3251                                                 NaN                NaN   \n",
       "7                                               0.003344           0.007399   \n",
       "3                                               0.009048           0.005210   \n",
       "8                                               0.007278           0.007399   \n",
       "12                                              0.004721           0.007890   \n",
       "9                                               0.008065           0.005663   \n",
       "10533                                                NaN                NaN   \n",
       "52                                              0.001377           0.003756   \n",
       "\n",
       "       The Last Straw  Paper-bag Cookery  Man's Redemption of Man  \\\n",
       "0            0.041425           0.064763                 0.113229   \n",
       "1            0.024964           0.053218                 0.048649   \n",
       "6702              NaN                NaN                      NaN   \n",
       "2            0.018452           0.004449                 0.013371   \n",
       "10165        0.011758                NaN                      NaN   \n",
       "5            0.009588           0.016951                 0.009104   \n",
       "6            0.012120           0.000338                 0.008250   \n",
       "4            0.020803           0.000507                 0.006828   \n",
       "15           0.009588           0.011995                 0.009673   \n",
       "1567         0.022974                NaN                 0.002276   \n",
       "6703              NaN                NaN                      NaN   \n",
       "13           0.013929           0.003886                 0.005690   \n",
       "3251              NaN                NaN                      NaN   \n",
       "7            0.005970           0.004618                 0.006543   \n",
       "3            0.000543           0.004336                 0.013087   \n",
       "8            0.008683           0.006364                 0.008819   \n",
       "12           0.008683           0.003266                 0.007681   \n",
       "9            0.004161           0.004562                 0.005121   \n",
       "10533             NaN           0.005801                      NaN   \n",
       "52           0.007779           0.000394                 0.003698   \n",
       "\n",
       "       Prey of the Space Falcon  Jacques Bonneval  The Cup of Comus       Avg  \n",
       "0                      0.098364          0.053267          0.076532  0.082682  \n",
       "1                      0.028665          0.047003          0.068666  0.043844  \n",
       "6702                        NaN               NaN               NaN  0.019866  \n",
       "2                      0.012497          0.013351          0.017490  0.014090  \n",
       "10165                       NaN               NaN               NaN  0.011758  \n",
       "5                      0.010564          0.011888          0.017675  0.011474  \n",
       "6                      0.030276          0.009327          0.007588  0.010801  \n",
       "4                      0.014429          0.015317          0.005367  0.010302  \n",
       "15                     0.010435          0.012391          0.007126  0.009556  \n",
       "1567                   0.011273          0.014814          0.005182  0.007695  \n",
       "6703                        NaN               NaN               NaN  0.007671  \n",
       "13                     0.005540          0.010791          0.004257  0.007251  \n",
       "3251                        NaN               NaN               NaN  0.006898  \n",
       "7                      0.003092          0.011568          0.002869  0.006628  \n",
       "3                      0.001288          0.004527          0.002961  0.006466  \n",
       "8                      0.001868          0.005487          0.003517  0.006455  \n",
       "12                     0.002383          0.009647          0.002499  0.005981  \n",
       "9                      0.008181          0.004527          0.004812  0.005861  \n",
       "10533                       NaN               NaN               NaN  0.005801  \n",
       "52                     0.010758          0.007773          0.001851  0.005768  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_freq['Avg'] = df_freq.drop('Word', axis=1).apply(lambda x: x.mean(), axis=1)\n",
    "df_freq = df_freq.sort_values(ascending = False, by=['Avg'])\n",
    "\n",
    "df_freq.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "experimental-accordance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Early Bardic Literature, Ireland</th>\n",
       "      <th>Life's Progress Through The Passions</th>\n",
       "      <th>Worldwide Effects of Nuclear War: Some Perspectives</th>\n",
       "      <th>Birds in London</th>\n",
       "      <th>The Last Straw</th>\n",
       "      <th>Paper-bag Cookery</th>\n",
       "      <th>Man's Redemption of Man</th>\n",
       "      <th>Prey of the Space Falcon</th>\n",
       "      <th>Jacques Bonneval</th>\n",
       "      <th>The Cup of Comus</th>\n",
       "      <th>Sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>1920.0</td>\n",
       "      <td>3538.0</td>\n",
       "      <td>522.0</td>\n",
       "      <td>4570.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>1527.0</td>\n",
       "      <td>1165.0</td>\n",
       "      <td>827.0</td>\n",
       "      <td>15846.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "      <td>794.0</td>\n",
       "      <td>1832.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>2636.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>945.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>445.0</td>\n",
       "      <td>1028.0</td>\n",
       "      <td>742.0</td>\n",
       "      <td>8892.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that</td>\n",
       "      <td>243.0</td>\n",
       "      <td>952.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>797.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>2958.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>his</td>\n",
       "      <td>152.0</td>\n",
       "      <td>1210.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>362.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>470.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2583.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>with</td>\n",
       "      <td>156.0</td>\n",
       "      <td>647.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>491.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>301.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>2332.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>was</td>\n",
       "      <td>157.0</td>\n",
       "      <td>1018.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>335.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2286.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>for</td>\n",
       "      <td>83.0</td>\n",
       "      <td>615.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1975.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>not</td>\n",
       "      <td>134.0</td>\n",
       "      <td>639.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>392.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1652.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>had</td>\n",
       "      <td>31.0</td>\n",
       "      <td>946.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1603.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>but</td>\n",
       "      <td>105.0</td>\n",
       "      <td>551.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>337.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1551.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>which</td>\n",
       "      <td>187.0</td>\n",
       "      <td>609.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1395.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>this</td>\n",
       "      <td>122.0</td>\n",
       "      <td>382.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>392.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1312.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>him</td>\n",
       "      <td>25.0</td>\n",
       "      <td>863.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1304.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>her</td>\n",
       "      <td>12.0</td>\n",
       "      <td>838.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>74.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>1284.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>have</td>\n",
       "      <td>106.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>418.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1282.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>are</td>\n",
       "      <td>62.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>537.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1194.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>from</td>\n",
       "      <td>116.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>they</td>\n",
       "      <td>70.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>394.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1132.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>all</td>\n",
       "      <td>78.0</td>\n",
       "      <td>382.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>267.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>1111.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567</th>\n",
       "      <td>you</td>\n",
       "      <td>1.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>324.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>949.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word  Early Bardic Literature, Ireland  \\\n",
       "0       the                            1920.0   \n",
       "1       and                             794.0   \n",
       "2      that                             243.0   \n",
       "6       his                             152.0   \n",
       "5      with                             156.0   \n",
       "4       was                             157.0   \n",
       "15      for                              83.0   \n",
       "7       not                             134.0   \n",
       "52      had                              31.0   \n",
       "13      but                             105.0   \n",
       "3     which                             187.0   \n",
       "8      this                             122.0   \n",
       "71      him                              25.0   \n",
       "191     her                              12.0   \n",
       "12     have                             106.0   \n",
       "28      are                              62.0   \n",
       "9      from                             116.0   \n",
       "22     they                              70.0   \n",
       "19      all                              78.0   \n",
       "1567    you                               1.0   \n",
       "\n",
       "      Life's Progress Through The Passions  \\\n",
       "0                                   3538.0   \n",
       "1                                   1832.0   \n",
       "2                                    952.0   \n",
       "6                                   1210.0   \n",
       "5                                    647.0   \n",
       "4                                   1018.0   \n",
       "15                                   615.0   \n",
       "7                                    639.0   \n",
       "52                                   946.0   \n",
       "13                                   551.0   \n",
       "3                                    609.0   \n",
       "8                                    382.0   \n",
       "71                                   863.0   \n",
       "191                                  838.0   \n",
       "12                                   326.0   \n",
       "28                                   159.0   \n",
       "9                                    318.0   \n",
       "22                                   287.0   \n",
       "19                                   382.0   \n",
       "1567                                 215.0   \n",
       "\n",
       "      Worldwide Effects of Nuclear War: Some Perspectives  Birds in London  \\\n",
       "0                                                 522.0             4570.0   \n",
       "1                                                 161.0             2636.0   \n",
       "2                                                  63.0              797.0   \n",
       "6                                                   1.0              362.0   \n",
       "5                                                  37.0              491.0   \n",
       "4                                                  20.0              326.0   \n",
       "15                                                 47.0              420.0   \n",
       "7                                                  17.0              392.0   \n",
       "52                                                  7.0              199.0   \n",
       "13                                                 24.0              337.0   \n",
       "3                                                  46.0              276.0   \n",
       "8                                                  37.0              392.0   \n",
       "71                                                  NaN              110.0   \n",
       "191                                                 NaN               74.0   \n",
       "12                                                 24.0              418.0   \n",
       "28                                                 31.0              537.0   \n",
       "9                                                  41.0              300.0   \n",
       "22                                                 14.0              394.0   \n",
       "19                                                 21.0              267.0   \n",
       "1567                                                NaN               43.0   \n",
       "\n",
       "      The Last Straw  Paper-bag Cookery  Man's Redemption of Man  \\\n",
       "0              229.0             1150.0                    398.0   \n",
       "1              138.0              945.0                    171.0   \n",
       "2              102.0               79.0                     47.0   \n",
       "6               67.0                6.0                     29.0   \n",
       "5               53.0              301.0                     32.0   \n",
       "4              115.0                9.0                     24.0   \n",
       "15              53.0              213.0                     34.0   \n",
       "7               33.0               82.0                     23.0   \n",
       "52              43.0                7.0                     13.0   \n",
       "13              77.0               69.0                     20.0   \n",
       "3                3.0               77.0                     46.0   \n",
       "8               48.0              113.0                     31.0   \n",
       "71              31.0                2.0                      4.0   \n",
       "191             39.0               12.0                      2.0   \n",
       "12              48.0               58.0                     27.0   \n",
       "28              23.0              151.0                     14.0   \n",
       "9               23.0               81.0                     18.0   \n",
       "22              30.0               55.0                      4.0   \n",
       "19              25.0               65.0                     21.0   \n",
       "1567           127.0                NaN                      8.0   \n",
       "\n",
       "      Prey of the Space Falcon  Jacques Bonneval  The Cup of Comus      Sum  \n",
       "0                       1527.0            1165.0             827.0  15846.0  \n",
       "1                        445.0            1028.0             742.0   8892.0  \n",
       "2                        194.0             292.0             189.0   2958.0  \n",
       "6                        470.0             204.0              82.0   2583.0  \n",
       "5                        164.0             260.0             191.0   2332.0  \n",
       "4                        224.0             335.0              58.0   2286.0  \n",
       "15                       162.0             271.0              77.0   1975.0  \n",
       "7                         48.0             253.0              31.0   1652.0  \n",
       "52                       167.0             170.0              20.0   1603.0  \n",
       "13                        86.0             236.0              46.0   1551.0  \n",
       "3                         20.0              99.0              32.0   1395.0  \n",
       "8                         29.0             120.0              38.0   1312.0  \n",
       "71                        85.0             142.0              42.0   1304.0  \n",
       "191                       59.0             138.0             110.0   1284.0  \n",
       "12                        37.0             211.0              27.0   1282.0  \n",
       "28                        38.0             152.0              27.0   1194.0  \n",
       "9                        127.0              99.0              52.0   1175.0  \n",
       "22                        61.0             177.0              40.0   1132.0  \n",
       "19                        59.0             126.0              67.0   1111.0  \n",
       "1567                     175.0             324.0              56.0    949.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_count.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-street",
   "metadata": {},
   "source": [
    "## Second testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-encounter",
   "metadata": {},
   "source": [
    "This definately needs some proper refactoring, but Was curious whether we get anything decent from reading a bunch of random books in\n",
    "\n",
    "Requires an additional folder \"decades\" in the root directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "stuck-submission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all filenames\n",
    "files = [f for f in listdir(file_path) if isfile(join(file_path, f))]\n",
    "\n",
    "# Do only subset\n",
    "## Is done for 5000 files already, so set down to 20 to increase performance. 5000 books are currently stored in the file\n",
    "files = files[0:20]\n",
    "\n",
    "\n",
    "counter = 0\n",
    "for file in files:\n",
    "    counter = counter + 1\n",
    "    # Read in basic information from file\n",
    "    title, author, date, year, language, encoding, content_lines = read_file(file_path + \"/\" + file)\n",
    "    #line_count = len(content_lines)\n",
    "    decade = math.floor(year / 10) * 10\n",
    "    decade_file = \"decades/\" + str(decade) + \".txt\"\n",
    "    content_all = \" \".join(content_lines)\n",
    "    \n",
    "    if os.path.exists(decade_file):\n",
    "        append_write = 'a' # append if already exists\n",
    "    else:\n",
    "        append_write = 'w' # make a new file if not\n",
    "\n",
    "    fileWriter = open(decade_file,append_write)\n",
    "    fileWriter.write(content_all + '\\n')\n",
    "    fileWriter.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-davis",
   "metadata": {},
   "source": [
    "### Read in from the decades files, and see the distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "improving-cooper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00.txt', '0.txt', '2010.txt', '2000.txt', '2020.txt', '1990.txt']\n",
      "2020.txt\n",
      "2010.txt\n",
      "2000.txt\n",
      "1990.txt\n",
      "00.txt\n",
      "0.txt\n"
     ]
    }
   ],
   "source": [
    "# Get all filenames\n",
    "files = [f for f in listdir(\"decades\") if isfile(join(\"decades\", f))]\n",
    "print(files)\n",
    "files.sort(reverse=True)\n",
    "\n",
    "\n",
    "col_names = []\n",
    "col_names.append(\"Word\")\n",
    "\n",
    "tables = []\n",
    "\n",
    "for file_name in files:\n",
    "    print(file_name)\n",
    "    \n",
    "    file = open(\"decades/\" + file_name, encoding=\"ISO-8859-1\")\n",
    "    file_content = file.read()\n",
    "    \n",
    "    # Split into words (and do various cleaning)\n",
    "    all_text_lower = file_content.lower()\n",
    "    words = re.findall(r'(\\b[A-Za-z][a-z]{2,9}\\b)', all_text_lower)\n",
    "\n",
    "    # First analysis, but should do something proper\n",
    "    word_frequencies_table, unique_word_count = get_word_frequencies(words)\n",
    "    tables.append(word_frequencies_table)\n",
    "    col_names.append(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-irish",
   "metadata": {},
   "source": [
    "### Preliminary Conclusion\n",
    "We see that even though the books are quite old, no decade prior to 1990s is found.\n",
    "\n",
    "This is when we found out that the \"year\" that's registered in the dataset is the upload-date. \n",
    "\n",
    "Haven gotten this far, we however decided to see if we could find a pattern in this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-drilling",
   "metadata": {},
   "source": [
    "### Compare ranking between upload-decades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "variable-above",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_count= []\n",
    "list_freq = []\n",
    "list_rank = []\n",
    "\n",
    "for df in tables:\n",
    "    #list_count.append(df[['Word', 'count']])\n",
    "    #list_freq.append(df[['Word', 'freq']])\n",
    "    list_rank.append(df[['Word', 'rank']])\n",
    "    \n",
    "#df_count = reduce(lambda left, right: pd.merge(left, right, on=\"Word\", how='outer'), list_count)\n",
    "#df_count.columns = col_names\n",
    "\n",
    "#df_freq = reduce(lambda left, right: pd.merge(left, right, on=\"Word\", how='outer'), list_freq)\n",
    "#df_freq.columns = col_names\n",
    "\n",
    "df_rank = reduce(lambda left, right: pd.merge(left, right, on=\"Word\", how='outer'), list_rank)\n",
    "df_rank.columns = col_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "nasty-prisoner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>2020.txt</th>\n",
       "      <th>2010.txt</th>\n",
       "      <th>2000.txt</th>\n",
       "      <th>1990.txt</th>\n",
       "      <th>00.txt</th>\n",
       "      <th>0.txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>was</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>with</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>for</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>his</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>not</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>had</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>but</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>which</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>they</td>\n",
       "      <td>13.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>from</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>were</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>have</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>this</td>\n",
       "      <td>17.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>are</td>\n",
       "      <td>18.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>she</td>\n",
       "      <td>19.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>all</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>their</td>\n",
       "      <td>21.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>him</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>her</td>\n",
       "      <td>23.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>its</td>\n",
       "      <td>24.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>one</td>\n",
       "      <td>25.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>there</td>\n",
       "      <td>26.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>them</td>\n",
       "      <td>27.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>what</td>\n",
       "      <td>28.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>has</td>\n",
       "      <td>29.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>been</td>\n",
       "      <td>30.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>will</td>\n",
       "      <td>31.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>would</td>\n",
       "      <td>32.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>said</td>\n",
       "      <td>33.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>when</td>\n",
       "      <td>34.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>more</td>\n",
       "      <td>35.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>who</td>\n",
       "      <td>36.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>into</td>\n",
       "      <td>37.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>out</td>\n",
       "      <td>38.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>then</td>\n",
       "      <td>39.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>other</td>\n",
       "      <td>40.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>men</td>\n",
       "      <td>41.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>only</td>\n",
       "      <td>42.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>can</td>\n",
       "      <td>43.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>upon</td>\n",
       "      <td>44.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>our</td>\n",
       "      <td>45.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>than</td>\n",
       "      <td>46.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>now</td>\n",
       "      <td>47.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>time</td>\n",
       "      <td>48.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>power</td>\n",
       "      <td>49.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>327.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>111.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>great</td>\n",
       "      <td>50.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>these</td>\n",
       "      <td>51.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>government</td>\n",
       "      <td>52.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>430.0</td>\n",
       "      <td>534.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>146.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>man</td>\n",
       "      <td>53.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>over</td>\n",
       "      <td>54.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>139.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>could</td>\n",
       "      <td>55.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>very</td>\n",
       "      <td>56.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>your</td>\n",
       "      <td>57.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>first</td>\n",
       "      <td>58.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>society</td>\n",
       "      <td>59.0</td>\n",
       "      <td>580.0</td>\n",
       "      <td>588.0</td>\n",
       "      <td>535.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>231.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>two</td>\n",
       "      <td>60.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>made</td>\n",
       "      <td>61.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>such</td>\n",
       "      <td>62.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>about</td>\n",
       "      <td>63.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>some</td>\n",
       "      <td>64.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>any</td>\n",
       "      <td>65.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>did</td>\n",
       "      <td>66.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>know</td>\n",
       "      <td>67.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>pendleton</td>\n",
       "      <td>68.0</td>\n",
       "      <td>3217.0</td>\n",
       "      <td>3668.0</td>\n",
       "      <td>773.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>same</td>\n",
       "      <td>69.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>well</td>\n",
       "      <td>70.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>under</td>\n",
       "      <td>71.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>may</td>\n",
       "      <td>72.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>general</td>\n",
       "      <td>73.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>374.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>195.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>before</td>\n",
       "      <td>74.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>most</td>\n",
       "      <td>75.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>even</td>\n",
       "      <td>76.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>much</td>\n",
       "      <td>77.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>like</td>\n",
       "      <td>78.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>stephanie</td>\n",
       "      <td>79.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3647.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>lorraine</td>\n",
       "      <td>80.0</td>\n",
       "      <td>3118.0</td>\n",
       "      <td>3538.0</td>\n",
       "      <td>766.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>383.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>those</td>\n",
       "      <td>81.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>down</td>\n",
       "      <td>82.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>back</td>\n",
       "      <td>83.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>217.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>came</td>\n",
       "      <td>84.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>236.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>see</td>\n",
       "      <td>85.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>how</td>\n",
       "      <td>86.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>way</td>\n",
       "      <td>87.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>145.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>think</td>\n",
       "      <td>88.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>little</td>\n",
       "      <td>89.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>without</td>\n",
       "      <td>90.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>here</td>\n",
       "      <td>91.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>against</td>\n",
       "      <td>92.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>people</td>\n",
       "      <td>93.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>124.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>after</td>\n",
       "      <td>94.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>must</td>\n",
       "      <td>95.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>don</td>\n",
       "      <td>95.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>274.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>where</td>\n",
       "      <td>96.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>never</td>\n",
       "      <td>97.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>own</td>\n",
       "      <td>98.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>right</td>\n",
       "      <td>99.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>161.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word  2020.txt  2010.txt  2000.txt  1990.txt  00.txt  0.txt\n",
       "0          the       1.0       1.0       1.0       1.0     1.0    1.0\n",
       "1          and       2.0       2.0       2.0       2.0     2.0    2.0\n",
       "2         that       3.0       3.0       3.0       3.0     4.0    3.0\n",
       "3          was       4.0       4.0       4.0       4.0    23.0    5.0\n",
       "4          you       5.0       9.0       8.0       5.0   163.0   19.0\n",
       "5         with       6.0       6.0       6.0       7.0     3.0    6.0\n",
       "6          for       7.0       7.0       7.0      11.0    13.0    8.0\n",
       "7          his       8.0       5.0       5.0       6.0     5.0    4.0\n",
       "8          not       9.0      10.0      12.0      12.0    11.0    9.0\n",
       "9          had      10.0       8.0       9.0      10.0    40.0   14.0\n",
       "10         but      11.0      11.0      11.0      13.0    72.0   10.0\n",
       "11       which      12.0      13.0      14.0      22.0    82.0    7.0\n",
       "12        they      13.0      17.0      15.0      16.0    58.0   21.0\n",
       "13        from      14.0      15.0      18.0      21.0    25.0   15.0\n",
       "14        were      15.0      20.0      21.0      20.0    81.0   22.0\n",
       "15        have      16.0      16.0      16.0      18.0    22.0   11.0\n",
       "16        this      17.0      14.0      17.0      15.0     8.0   13.0\n",
       "17         are      18.0      21.0      23.0      27.0    37.0   16.0\n",
       "18         she      19.0      18.0      13.0       9.0    65.0   47.0\n",
       "19         all      20.0      19.0      20.0      17.0    15.0   12.0\n",
       "20       their      21.0      24.0      24.0      26.0    21.0   30.0\n",
       "21         him      22.0      22.0      19.0      14.0    28.0   20.0\n",
       "22         her      23.0      12.0      10.0       8.0    20.0   33.0\n",
       "23         its      24.0      40.0      50.0      98.0    68.0   67.0\n",
       "24         one      25.0      23.0      22.0      25.0    17.0   28.0\n",
       "25       there      26.0      25.0      25.0      23.0    37.0   32.0\n",
       "26        them      27.0      29.0      27.0      32.0    55.0   38.0\n",
       "27        what      28.0      33.0      32.0      24.0    56.0   24.0\n",
       "28         has      29.0      38.0      44.0      46.0   176.0   40.0\n",
       "29        been      30.0      28.0      31.0      33.0    30.0   23.0\n",
       "30        will      31.0      32.0      33.0      37.0    85.0   27.0\n",
       "31       would      32.0      30.0      29.0      31.0   194.0   25.0\n",
       "32        said      33.0      31.0      26.0      19.0    61.0  141.0\n",
       "33        when      34.0      26.0      28.0      28.0   105.0   34.0\n",
       "34        more      35.0      34.0      36.0      42.0   190.0   26.0\n",
       "35         who      36.0      27.0      30.0      30.0    27.0   18.0\n",
       "36        into      37.0      37.0      37.0      39.0    97.0   63.0\n",
       "37         out      38.0      35.0      34.0      29.0    62.0   77.0\n",
       "38        then      39.0      36.0      35.0      35.0    47.0   50.0\n",
       "39       other      40.0      46.0      57.0      58.0    88.0   66.0\n",
       "40         men      41.0      78.0      79.0      81.0   108.0   53.0\n",
       "41        only      42.0      48.0      54.0      60.0   192.0   71.0\n",
       "42         can      43.0      58.0      53.0      45.0    98.0   54.0\n",
       "43        upon      44.0      49.0      55.0      80.0   121.0   89.0\n",
       "44         our      45.0      57.0      48.0      91.0   149.0   43.0\n",
       "45        than      46.0      47.0      47.0      64.0   201.0   31.0\n",
       "46         now      47.0      44.0      41.0      38.0   134.0   42.0\n",
       "47        time      48.0      42.0      43.0      49.0    96.0   57.0\n",
       "48       power      49.0     265.0     241.0     327.0   204.0  111.0\n",
       "49       great      50.0      64.0      58.0      79.0    64.0   37.0\n",
       "50       these      51.0      55.0      64.0      87.0    48.0   60.0\n",
       "51  government      52.0     438.0     430.0     534.0     NaN  146.0\n",
       "52         man      53.0      41.0      40.0      34.0   222.0   29.0\n",
       "53        over      54.0      68.0      66.0      62.0    92.0  139.0\n",
       "54       could      55.0      45.0      39.0      41.0   199.0   64.0\n",
       "55        very      56.0      43.0      42.0      50.0   159.0   65.0\n",
       "56        your      57.0      54.0      45.0      36.0   201.0   55.0\n",
       "57       first      58.0      67.0      76.0      77.0   219.0   81.0\n",
       "58     society      59.0     580.0     588.0     535.0     NaN  231.0\n",
       "59         two      60.0      53.0      59.0      70.0   124.0  100.0\n",
       "60        made      61.0      62.0      61.0      83.0   174.0   82.0\n",
       "61        such      62.0      65.0      73.0      78.0   146.0   45.0\n",
       "62       about      63.0      51.0      46.0      40.0    99.0  113.0\n",
       "63        some      64.0      39.0      38.0      44.0    70.0   49.0\n",
       "64         any      65.0      56.0      56.0      55.0   130.0   49.0\n",
       "65         did      66.0      61.0      51.0      54.0   154.0   99.0\n",
       "66        know      67.0      81.0      77.0      48.0   159.0   98.0\n",
       "67   pendleton      68.0    3217.0    3668.0     773.0     NaN    NaN\n",
       "68        same      69.0     100.0     121.0     118.0   203.0  112.0\n",
       "69        well      70.0      66.0      60.0      52.0    54.0   58.0\n",
       "70       under      71.0     104.0     123.0     153.0   129.0  103.0\n",
       "71         may      72.0      50.0      69.0      86.0   162.0   36.0\n",
       "72     general      73.0     199.0     236.0     374.0   254.0  195.0\n",
       "73      before      74.0      63.0      65.0      59.0   101.0   84.0\n",
       "74        most      75.0      79.0      86.0     120.0   243.0   46.0\n",
       "75        even      76.0      92.0      98.0     125.0    77.0   78.0\n",
       "76        much      77.0      76.0      75.0      76.0   228.0   73.0\n",
       "77        like      78.0      59.0      52.0      47.0    10.0   74.0\n",
       "78   stephanie      79.0       NaN    3647.0       NaN     NaN    NaN\n",
       "79    lorraine      80.0    3118.0    3538.0     766.0     NaN  383.0\n",
       "80       those      81.0      84.0      88.0     138.0    26.0   44.0\n",
       "81        down      82.0      77.0      71.0      63.0    63.0  126.0\n",
       "82        back      83.0      95.0      91.0      85.0   165.0  217.0\n",
       "83        came      84.0      93.0      85.0      74.0   172.0  236.0\n",
       "84         see      85.0      69.0      63.0      51.0   150.0   88.0\n",
       "85         how      86.0      80.0      74.0      57.0   152.0   56.0\n",
       "86         way      87.0      91.0      84.0      69.0   118.0  145.0\n",
       "87       think      88.0     123.0     103.0      82.0   227.0  114.0\n",
       "88      little      89.0      52.0      49.0      43.0   229.0   80.0\n",
       "89     without      90.0     103.0     107.0     107.0   105.0   79.0\n",
       "90        here      91.0      83.0      83.0      68.0    89.0   93.0\n",
       "91     against      92.0     144.0     142.0     171.0   220.0   92.0\n",
       "92      people      93.0     111.0     116.0     121.0   203.0  124.0\n",
       "93       after      94.0      60.0      62.0      53.0   103.0   95.0\n",
       "94        must      95.0      71.0      78.0      88.0   213.0   69.0\n",
       "95         don      95.0     137.0     112.0      56.0     NaN  274.0\n",
       "96       where      96.0      73.0      72.0      84.0   151.0  114.0\n",
       "97       never      97.0      86.0      81.0      75.0   172.0   83.0\n",
       "98         own      98.0      94.0      89.0      96.0   111.0   68.0\n",
       "99       right      99.0     135.0     139.0     117.0   236.0  161.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rank.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-gambling",
   "metadata": {},
   "source": [
    "## Trying to fit models to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-start",
   "metadata": {},
   "source": [
    "### Read in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "specific-founder",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_contents = []\n",
    "targets = []\n",
    "\n",
    "files = [f for f in listdir(file_path) if isfile(join(file_path, f))]\n",
    "files = list(filter(lambda file: file[0].isdigit(), files))\n",
    "random.shuffle(files)\n",
    "\n",
    "targets_=['70','80','90','00','10']\n",
    "iter_ = 0\n",
    "\n",
    "for f in files[:120]:\n",
    "    file = open(\"processedData/\" + f, encoding=\"ISO-8859-1\")\n",
    "    file_contents.append(file.read())\n",
    "    iter_ = iter_+1\n",
    "    targets.append(targets_[iter_%5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-brook",
   "metadata": {},
   "source": [
    "### Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dried-definition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   17.6s finished\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('kbest', SelectKBest(chi2, k=100)),\n",
    "    ('nb', MultinomialNB()),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    #'vect__max_df': [1.0),\n",
    "    # 'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    #'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    # 'tfidf__use_idf': (True, False),\n",
    "    # 'tfidf__norm': ('l1', 'l2'),\n",
    "    #'clf__max_iter': (20),\n",
    "    #'clf__alpha': (0.00001),\n",
    "    #'clf__penalty': ('l2'),\n",
    "    # 'clf__max_iter': (10, 50, 80),\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, verbose=1)\n",
    "\n",
    "grid_search.fit(file_contents, targets)\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-creator",
   "metadata": {},
   "source": [
    "## Realisation and conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-apple",
   "metadata": {},
   "source": [
    "At this point, we came to the conclusion that \"year\" in the Gutenberg dataset shows when the data **was published** to the project, and not the release date of the book.\n",
    "\n",
    "We searched for possible solutions to get the years for book publications, but were unable to find any free API that we could link to our current dataset.\n",
    "\n",
    "We therefore went on a search for other datasets, and to remake our hypothesis entirely.\n",
    "Thus, this part ended in a blind spot. However science is not only about the results, but also about the discoveries along the way, and therefore it is added into this file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-stockholm",
   "metadata": {},
   "source": [
    "# Studying language change in Icelandic parliamentary speeches\n",
    "\n",
    "Our task involves research into language change over the past 100 years. Additionally we have been tasked with working out factors that influence language change. \n",
    "\n",
    "Another proposed research question involves figuring out which languages are going extinct. This particular task has been found out to be near impossible to answer given the available data. It is estimated to be very hard to come up with data that capture the amount of speakers for a large enough ranges of combinations of language and year. Furthermore, any data that are available are likely to apply a different definition of \"speaker\" (sometimes including second/third... language speakers, sometimes not) and is also likely to contain politically motivated noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-israel",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-trinity",
   "metadata": {},
   "source": [
    "Therefore we decided to search for English language corpora containing a wide array of text documents collected over the past century for predefined dialects of English and genre of text (movie, articles, books, ...). This surprisingly turned out to be a complex endeavour as all high quality corpora were available only for a big price tag. \n",
    "\n",
    "We also looked into the material provided by the Guttenberg Project [Link](https://www.projekt-gutenberg.org/). This turned out to be promising at first sight as it appears that there is a lot of recently published material. However release date of these documents does not match the year when the documents were actually written and soon enough we figured out that all material is from before 1923. This obviously did not allow us to look much into language change of the 20th and 21st century.\n",
    "\n",
    "_Gerlach, M., & Font-Clos, F. (2020). A standardized Project Gutenberg corpus for statistical analysis of natural language and quantitative linguistics. Entropy, 22(1), 126._\n",
    "\n",
    "Theoretically one could obtain books from after 1923 and include them into the analysis. But one would quickly run into copyright/licensing issues here.\n",
    "\n",
    "Obtaining the content of these books and preprocessing them for the purposes of data analysis turned out to be quite cumbersome as well. Look at Gunnar's notebooks (first draft [here](firstDraft.ipynb), second draft [here](secondDraft.ipynb)) for the details. \n",
    "\n",
    "Finally we turned to looking for non-English corpora and found an annotated corpus including pre-factured lemmatization of [Icelandic parlimentary speeches](https://clarin.is/en/resources/parliament/) from 1911 until 2018:\n",
    "\n",
    "_Steingrímsson, Steinþór, Sigrún Helgadóttir, Eiríkur Rögnvaldsson, Starkaður Barkarson and Jón Guðnason. 2018. Risamálheild: A Very Large Icelandic Text Corpus. Proceedings of LREC 2018, pp. 4361-4366. Myazaki, Japan._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-conflict",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-arbor",
   "metadata": {},
   "source": [
    "### Load required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "derived-blues",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import glob\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "import random\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "from nltk import ngrams\n",
    "\n",
    "# Used for building models for classifying:\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-plasma",
   "metadata": {},
   "outputs": [],
   "source": [
    "namespace = \"{http://www.tei-c.org/ns/1.0}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-interpretation",
   "metadata": {},
   "source": [
    "### Get the data\n",
    "\n",
    "Download data from here: http://www.malfong.is/index.php?dlid=81&lang=en\n",
    "\n",
    "Then extract zip folder such that a folder labelled `CC_BY` shows up in the parent folder of this notebook. *Test*: `ls ../CC_BY/althingi` should work when run from `.../IcelandicParliamentSpeeches.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-arrangement",
   "metadata": {},
   "source": [
    "### Preprocessing helpers\n",
    "\n",
    "The data are available as XML. The text has already been preprocessed to be separated into paragraphs, sentences and words. Furthermore each word tag also includes a `lemma` attribute relating inflected/declensed forms of words to its lemma. This has been done by the authors of the original paper using Machine Learning approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-duration",
   "metadata": {},
   "source": [
    "Given a relative path to a file, pull out a list with all the words. This can be achieved by looking for all tags of type `w`, additionally also retrieve the lemma for each word.\n",
    "\n",
    "We will discard all sentences of length 3 or smaller to remove noise and to avoid that our models are able to detect year of speech just based on some short introductory/outro phrases. Furthermore the raw data appear to contain plenty of elements tagged as words that comprise of just a single letter followed by a dot. These will be removed here as well.\n",
    "\n",
    "⚠️ *Pitfall*: The namespace from above must be included when parsing out content from these XML files based on tag names.\n",
    "\n",
    "⚠️ In this kind of preprocessing we lose information about sentence boundaries as all punctuation items from the raw data are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-romania",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(path):\n",
    "    xml_tree = ET.parse(open(path, 'r', encoding=\"utf8\"))\n",
    "    words = []\n",
    "    lemmata = []\n",
    "    \n",
    "    for sentence in xml_tree.getroot().iter('{}s'.format(namespace)):\n",
    "        words_in_sent = sentence.findall('{}w'.format(namespace))\n",
    "        if len(words_in_sent) > 2:\n",
    "            for word in sentence.findall('{}w'.format(namespace)):\n",
    "                if not word.text.endswith('.'):\n",
    "                    words.append(word.text)\n",
    "                    lemmata.append(word.attrib['lemma'])\n",
    "        \n",
    "    return words, lemmata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-casino",
   "metadata": {},
   "source": [
    "Extract content of files separated into sentences, note that all stop items are wrapped in a `p` tag in the original documents and are not included here.\n",
    "\n",
    "Also note that some further pre-processing could be done here to exclude items such as numbers, percentages, names, abbreviations, etc. In the original documents these are also assigned to be words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(path, lemma=False):\n",
    "    xml_tree = ET.parse(open(path, 'r', encoding=\"utf8\"))\n",
    "    sentences = []\n",
    "    \n",
    "    for sentence in xml_tree.getroot().iter('{}s'.format(namespace)):\n",
    "        sentence_cur = []\n",
    "        words_in_sent = sentence.findall('{}w'.format(namespace))\n",
    "        \n",
    "        if len(words_in_sent) > 2:\n",
    "            for word in sentence.findall('{}w'.format(namespace)):\n",
    "                if not word.text.endswith('.'):\n",
    "                    if lemma:\n",
    "                        sentence_cur.append(word.attrib['lemma'])\n",
    "                    else:\n",
    "                        sentence_cur.append(word.text)\n",
    "            \n",
    "            sentences.append(sentence_cur)\n",
    "        \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-reaction",
   "metadata": {},
   "source": [
    "Retrieve a random selection of `k` file names from the entire corpus. The files must be of type `xml`. This method does not load the entire corpus into memory and allows you to work with smaller selections for test purposes. This method samples only from the `althingi` folder so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-charm",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_sample(k):\n",
    "    files = [filename for filename in glob.iglob('../CC_BY/althingi/**/*.xml', \n",
    "                                                 recursive=True)]\n",
    "    return random.sample(files, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-tradition",
   "metadata": {},
   "source": [
    "Do the same as above but choose `k` files only from a given year (range: 1911-2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-acquisition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_for_year(year, k):\n",
    "    files = [filename for filename in glob.iglob('../CC_BY/althingi/{}/'.format(year) + '**/*.xml', \n",
    "                                                 recursive=True)]\n",
    "    return random.sample(files, min(len(files), k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-auction",
   "metadata": {},
   "source": [
    "## Preliminary Data Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-trading",
   "metadata": {},
   "source": [
    "Doing some basic analysis on the documents in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-nevada",
   "metadata": {},
   "source": [
    "### Zipf's Law\n",
    "\n",
    "First using frequency distributions of the Natural Language ToolKit (`NLTK`) to look into whether or not we can confirm [Zipf's Law](https://en.wikipedia.org/wiki/Zipf%27s_law) based on the data we have.\n",
    "\n",
    "⚠️ Note that the analysis is done based on 15 randomly selected files from the entire corpus at this point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-prevention",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "\n",
    "for file in get_random_sample(15):\n",
    "    words.extend(extract_words(file)[1])\n",
    "    \n",
    "fq = FreqDist(word.lower() for word in words)\n",
    "fq.plot(25, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-corrections",
   "metadata": {},
   "source": [
    "Visualizing the same data but with using the logarithm of the occurrences, this should ideally obtain a straight line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-presentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df = pd.DataFrame.from_dict(fq, orient='index', columns=['word_occur'])\n",
    "freq_df.sort_values(by='word_occur', inplace=True, ascending=False)\n",
    "freq_df.word_occur = np.log2(freq_df['word_occur'])\n",
    "freq_df.head(25).plot(kind='line')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-gazette",
   "metadata": {},
   "source": [
    "### Disappearing words / new words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-factor",
   "metadata": {},
   "source": [
    "Here is a description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-lloyd",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_1914 = []\n",
    "words_2014 = []\n",
    "\n",
    "for file in get_files_for_year(1914, 25):\n",
    "    words_1914.extend(extract_words(file)[1])\n",
    "    \n",
    "for file in get_files_for_year(2014, 25):\n",
    "    words_2014.extend(extract_words(file)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-swift",
   "metadata": {},
   "source": [
    "### Development of average sentence length\n",
    "\n",
    "This is just one possible metric for the development/analysis of language complexity. There is so much more you could come up with here.\n",
    "\n",
    "Obviously our choice to discard very short sentences in the preprocessing step has an impact on the values here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-square",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sentence_length_year(year, k):\n",
    "    sentence_len = []\n",
    "    for file in get_files_for_year(year, k):\n",
    "        sentences = extract_sentences(file)\n",
    "        sentence_len.extend([len(s) for s in sentences])\n",
    "    \n",
    "    return reduce(lambda a, b: a + b, sentence_len) / len(sentence_len)\n",
    "        \n",
    "sentence_len_years = []\n",
    "\n",
    "for year in range(1924, 2018):\n",
    "    sentence_len_years.append(avg_sentence_length_year(year, 20))\n",
    "    \n",
    "avg_df = pd.DataFrame(sentence_len_years, index=range(1924, 2018), columns=['avg_sent_len'])\n",
    "avg_df.plot(kind='line')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-newport",
   "metadata": {},
   "source": [
    "### n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-nickname",
   "metadata": {},
   "source": [
    "Here is a description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-rwanda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_ngrams(n, top_k, sample):\n",
    "    file_contents = []\n",
    "\n",
    "    for file in get_random_sample(sample):\n",
    "        file_contents.extend(extract_words(file)[1])\n",
    "    \n",
    "    fq_ngr = FreqDist(ngrams(file_contents, n))\n",
    "    fq_ngr.plot(top_k, cumulative=False)\n",
    "    \n",
    "most_common_ngrams(n=3, top_k=25, sample=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-request",
   "metadata": {},
   "source": [
    "## Building model for classifying speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-script",
   "metadata": {},
   "source": [
    "The idea of this section is as follows:\n",
    "\n",
    "* Import necessary libraries for the implementation of the next steps.\n",
    "* Split the data in two sets: training and test.\n",
    "* Select feature extraction methods to be used.\n",
    "* Select classifiers to be trained.\n",
    "* Train the classifiers using different feature extraction methods and different combination of hyperparameters within the classifiers. Perform a cross-validated grid search for that purpose.\n",
    "* Compare the cross validation results within the trained models and select the best ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-credits",
   "metadata": {},
   "source": [
    "### Constructing training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-checklist",
   "metadata": {},
   "source": [
    "⚠️ *So far, we don't have a defined strategy for train-test split. However, we extract some documents to be able to try the models.*\n",
    "\n",
    "⚠️ *We will have to design an appropiate strategy for a train-test split in which we shpuld keep the proportions within each class and so on.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-enzyme",
   "metadata": {},
   "source": [
    "#### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-turner",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set seed for reproducibility\n",
    "random.seed(123)\n",
    "\n",
    "file_contents = []\n",
    "targets = []\n",
    "\n",
    "#for year in [1914, 1933,1959,1968,1971,1984,1997,2005,  2016]:\n",
    "#    for file in get_files_for_year(year, 20):\n",
    "for year in [1914, 1912, \n",
    "             1933, 1936, 1939, 1938, 1934,\n",
    "             1954, 1955, 1957, 1959, 1956,\n",
    "             1975, 1978, 1972, 1971, 1977,\n",
    "             1992, 1995, 1999, 1997, 1993,\n",
    "             2014, 2015, 2013, 2016, 2012]:\n",
    "    for file in get_files_for_year(year, 100):\n",
    "        file_contents.append(extract_words(file)[1])\n",
    "        targets.append(year - year%10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-alabama",
   "metadata": {},
   "source": [
    "Let's randomly choose a fixed number of documents (here currently: 5) from various different decades. Then passing (document, decade) pairs to the model below. The decade is computed by subtracting `mod(<year>, 10)` from `<year>`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-polls",
   "metadata": {},
   "source": [
    "#### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-klein",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed for reproducibility\n",
    "random.seed(123)\n",
    "\n",
    "file_contents_test = []\n",
    "targets_test = []\n",
    "\n",
    "#for year in [1914, 1936, 1955, 1975, 1995, 2015]:\n",
    "for year in [1911, 1937, 1958, 1973, 1994, 2017]:\n",
    "    for file in get_files_for_year(year, 20):\n",
    "        file_contents_test.append(extract_words(file)[1])\n",
    "        targets_test.append(year - year%10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-replacement",
   "metadata": {},
   "source": [
    "### Text feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-prize",
   "metadata": {},
   "source": [
    "We have considered 3 different methods for text feature extraction: Tf-idf, word2vec and doc2vec. All of them will be implemented through the corresponding functions from *sklearn* library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-basis",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-minister",
   "metadata": {},
   "source": [
    "Helper function to transform the data so that it is in the right format for the tfidfVectorizer() function that will be used later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-anaheim",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JoinElement(object):\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        #joins the elements of a list (which represents a document) into a single string \n",
    "        #with a blank space separation between each word\n",
    "        return [' '.join(X[i]) for i in range(len(X))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-karaoke",
   "metadata": {},
   "source": [
    "More information about it: [sklearn documentation.](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-committee",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-april",
   "metadata": {},
   "source": [
    "<ins>Original paper</ins>:\n",
    "Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed representations of words and phrases and their compositionality. Advances in neural information processing systems, 26, 3111-3119.\n",
    "\n",
    "With this model every word is assigned a unique vector of configurable cardinality such that the dot product of two randomly chosen vectors should be proportional to the semantic similarity for the associated words. This happens during the training step using logistic regression and sliding windows. Personally I found that this video delivers a solid explanation of the concepts: https://www.youtube.com/watch?v=QyrUentbkvw\n",
    "\n",
    "However, since we are working with entire documents as training items we have to somehow aggregate the vectors for every word in a given document. This can be done e.g. by taking the mean and/or summing up the vectors (see `MeanEmbeddingVectorizer`), optionally weighted by TF-IDF (see `MeanEmbeddingVectorizerTfidf`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-tunnel",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(BaseEstimator):\n",
    "    def fit(self, X, y):\n",
    "        self.word2vec = Word2Vec(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec.wv[w] for w in words if w in self.word2vec.wv.vocab]\n",
    "                    or [np.zeros(self.word2vec.vector_size)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-cooper",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizerTfidf(BaseEstimator):\n",
    "    def fit(self, X, y):\n",
    "        self.word2vec = Word2Vec(X)\n",
    "        self.X_joined = [' '.join(X[i]) for i in range(len(X))]\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.transformed = self.vectorizer.fit_transform(self.X_joined)\n",
    "        self.transformed = pd.DataFrame.sparse.from_spmatrix(self.transformed)\n",
    "        return self\n",
    "    \n",
    "    def tfidf(self, w, docid):\n",
    "        if w in self.vectorizer.vocabulary_:\n",
    "            return self.transformed[self.vectorizer.vocabulary_[w]][docid]\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec.wv[w] * self.tfidf(w, i) for w in words if w in self.word2vec.wv.vocab]\n",
    "                    or [np.zeros(self.word2vec.vector_size)], axis=0)\n",
    "            for i, words in enumerate(X)\n",
    "        ])\n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        self = self.fit(X, y)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-equilibrium",
   "metadata": {},
   "source": [
    "#### Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-hardware",
   "metadata": {},
   "source": [
    "Finally we are attempting to build a model using _Doc2Vec_. After training this model with our training corpus we receive a vector of configurable cardinality for each document.\n",
    "\n",
    "<ins>Original paper</ins>: Le, Quoc, and Tomas Mikolov. \"Distributed representations of sentences and documents.\" International conference on machine learning. 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-description",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2Vectorizer(BaseEstimator):\n",
    "    def __init__(self, window=2, vector_size=100):\n",
    "        self.window = window\n",
    "        self.vector_size = vector_size\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        docs = [TaggedDocument(X[i], [y[i]]) for i in range(len(X))]\n",
    "        self.doc_vec = Doc2Vec(docs, vector_size=self.vector_size, window=self.window, min_count=1, workers=4)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return [self.doc_vec.infer_vector(X[i]) for i in range(len(X))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "armed-walter",
   "metadata": {},
   "source": [
    "**BERT** (*Bidirectional Encoder Representations from Transformers*) is also interesting to look at, but we'll skip this here because we predict training a model from scratch would use up too many resources. Given more time however you could search for pretrained networks that roughly serve the purpose of classification of documents according to publication year.\n",
    "\n",
    "<ins>Paper</ins>: Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-minutes",
   "metadata": {},
   "source": [
    "### Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-romance",
   "metadata": {},
   "source": [
    "3 different classifiers are going to be trained: Multinomial Naive Bayes, Support Vector Machines and Random Forest Classifier. All of them will be implemented using sklearn library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-sodium",
   "metadata": {},
   "source": [
    "#### Multinominal Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-duncan",
   "metadata": {},
   "source": [
    "Source: [sklearn documentation.](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-copper",
   "metadata": {},
   "source": [
    "#### Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-sight",
   "metadata": {},
   "source": [
    "Source: [sklearn documentation.](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-twins",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-rhythm",
   "metadata": {},
   "source": [
    "Source: [sklearn documentation.](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-financing",
   "metadata": {},
   "source": [
    "### Train models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terminal-auditor",
   "metadata": {},
   "source": [
    "Since there are 3 methods for feature extraction and 3 classifiers, we should train 9 kind of models with their different combinations of hyperparameters. However, multinomial naive bayes does not take negative values produced by Word2Vec and Doc2Vec. Therefore, we have 7.\n",
    "\n",
    "For each model, a grid search is performed with different combinations of hyperparameters for the classifiers and the text extraction methods. Afterwards, the most relevant results of each of the models are stored in a pandas data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-yorkshire",
   "metadata": {},
   "source": [
    "#### Model 1: TFIDF vectorizer, select K best and Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-ethiopia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose parameters for the different steps in the pipeline\n",
    "parameters_model_1 = {\n",
    "    \n",
    "    #select KBest\n",
    "    \"k_best__k\": [1000],\n",
    "    \"k_best__score_func\": [chi2],\n",
    "    \n",
    "    #MultinomialNaiveBayes\n",
    "    \"MNB__alpha\" : [0,0.05,0.1,0.5,1], \n",
    "    \"MNB__fit_prior\": [True,False]\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "#build a pipeline\n",
    "model_1_pipeline = Pipeline([\n",
    "        #joins list into a single string\n",
    "        ('join', JoinElement()),\n",
    "        #tfidf vectorizer\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        #select 1000 best from word vectors\n",
    "        ('k_best', SelectKBest()),\n",
    "        #apply naive bayes\n",
    "        ('MNB', MultinomialNB())\n",
    "    ])\n",
    "\n",
    "#design grid search\n",
    "grid_search_model_1 = GridSearchCV(\n",
    "    #pipeline to be followed\n",
    "    model_1_pipeline,\n",
    "    #parameters\n",
    "    param_grid=parameters_model_1,\n",
    "    #number of folds for CV\n",
    "    cv=5,\n",
    "    #scoring to be considered for the cv\n",
    "    scoring = \"accuracy\",\n",
    "    #parallelize if possible\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#fit the grid search for training data\n",
    "grid_search_model_1.fit(file_contents, targets)\n",
    "\n",
    "#save results of cross validation\n",
    "cv_results_model_1 = pd.DataFrame(grid_search_model_1.cv_results_)\n",
    "\n",
    "#filter columns to be kept in the dataframe\n",
    "filter_col = [col for col in cv_results_model_1 if (col.startswith(\"param_\") or col.startswith(\"mean_\") or col.startswith(\"rank\"))]\n",
    "\n",
    "#save results with only filtered columns\n",
    "cv_results_model_1 = cv_results_model_1[filter_col]\n",
    "\n",
    "#save name of the model for later comparison\n",
    "cv_results_model_1.insert(loc=0, column=\"Model\", value= \"1\")\n",
    "\n",
    "#round mean_test_score\n",
    "cv_results_model_1[\"mean_test_score\"] = cv_results_model_1[\"mean_test_score\"].round(2)\n",
    "\n",
    "#show best 5 sorted by mean_test_score\n",
    "display(cv_results_model_1.sort_values(by=\"mean_test_score\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-fishing",
   "metadata": {},
   "source": [
    "#### Model 2: TFIDF vectorizer, select K best and SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-ultimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose parameters for the different steps in the pipeline\n",
    "parameters_model_2 = {\n",
    "    \n",
    "    #select KBest\n",
    "    \"k_best__k\": [1000],\n",
    "    \"k_best__score_func\": [chi2],\n",
    "    \n",
    "    #SVC\n",
    "    \"SVC__kernel\" : [\"linear\", \"poly\", \"sigmoid\"], \n",
    "    \"SVC__degree\": [2,3]\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "#build a pipeline\n",
    "model_2_pipeline = Pipeline([\n",
    "        #joins list into a single string\n",
    "        ('join', JoinElement()),\n",
    "        #tfidf vectorizer\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        #select 1000 best from word vectors\n",
    "        ('k_best', SelectKBest()),\n",
    "        #apply naive bayes\n",
    "        ('SVC', SVC())\n",
    "    ])\n",
    "\n",
    "#design grid search\n",
    "grid_search_model_2 = GridSearchCV(\n",
    "    #pipeline to be followed\n",
    "    model_2_pipeline,\n",
    "    #parameters\n",
    "    param_grid=parameters_model_2,\n",
    "    #number of folds for CV\n",
    "    cv=5,\n",
    "    #scoring to be considered for the cv\n",
    "    scoring = \"accuracy\",\n",
    "    #parallelize if possible\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#fit the grid search for training data\n",
    "grid_search_model_2.fit(file_contents, targets)\n",
    "\n",
    "#save results of cross validation\n",
    "cv_results_model_2 = pd.DataFrame(grid_search_model_2.cv_results_)\n",
    "\n",
    "#filter columns to be kept in the dataframe\n",
    "filter_col = [col for col in cv_results_model_2 if (col.startswith(\"param_\") or col.startswith(\"mean_\") or col.startswith(\"rank\"))]\n",
    "\n",
    "#save results with only filtered columns\n",
    "cv_results_model_2 = cv_results_model_2[filter_col]\n",
    "\n",
    "#save name of the model for later comparison\n",
    "cv_results_model_2.insert(loc=0, column=\"Model\", value= \"2\")\n",
    "\n",
    "#round mean_test_score\n",
    "cv_results_model_2[\"mean_test_score\"] = cv_results_model_2[\"mean_test_score\"].round(2)\n",
    "\n",
    "#show best 5 sorted by mean_test_score\n",
    "display(cv_results_model_2.sort_values(by=\"mean_test_score\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-excuse",
   "metadata": {},
   "source": [
    "#### Model 3: TFIDF vectorizer, select K best and Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-specialist",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose parameters for the different steps in the pipeline\n",
    "parameters_model_3 = {\n",
    "    \n",
    "    #select KBest\n",
    "    \"k_best__k\": [1000],\n",
    "    \"k_best__score_func\": [chi2],\n",
    "    \n",
    "    #RF classifier\n",
    "    \"clf__n_estimators\" : [10,100,200,300]\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "#build a pipeline\n",
    "model_3_pipeline = Pipeline([\n",
    "        #joins list into a single string\n",
    "        ('join', JoinElement()),\n",
    "        #tfidf vectorizer\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        #select 1000 best from word vectors\n",
    "        ('k_best', SelectKBest()),\n",
    "        #apply naive bayes\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "#design grid search\n",
    "grid_search_model_3 = GridSearchCV(\n",
    "    #pipeline to be followed\n",
    "    model_3_pipeline,\n",
    "    #parameters\n",
    "    param_grid=parameters_model_3,\n",
    "    #number of folds for CV\n",
    "    cv=5,\n",
    "    #scoring to be considered for the cv\n",
    "    scoring = \"accuracy\",\n",
    "    #parallelize if possible\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#fit the grid search for training data\n",
    "grid_search_model_3.fit(file_contents, targets)\n",
    "\n",
    "#save results of cross validation\n",
    "cv_results_model_3 = pd.DataFrame(grid_search_model_3.cv_results_)\n",
    "\n",
    "#filter columns to be kept in the dataframe\n",
    "filter_col = [col for col in cv_results_model_3 if (col.startswith(\"param_\") or col.startswith(\"mean_\") or col.startswith(\"rank\"))]\n",
    "\n",
    "#save results with only filtered columns\n",
    "cv_results_model_3 = cv_results_model_3[filter_col]\n",
    "\n",
    "#save name of the model for later comparison\n",
    "cv_results_model_3.insert(loc=0, column=\"Model\", value= \"3\")\n",
    "\n",
    "#round mean_test_score\n",
    "cv_results_model_3[\"mean_test_score\"] = cv_results_model_3[\"mean_test_score\"].round(2)\n",
    "\n",
    "#show best 5 sorted by mean_test_score\n",
    "display(cv_results_model_3.sort_values(by=\"mean_test_score\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-sight",
   "metadata": {},
   "source": [
    "#### Model 4: Word2Vec and SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose parameters for the different steps in the pipeline\n",
    "parameters_model_4 = {\n",
    "    \n",
    "    #SVC\n",
    "    \"SVC__kernel\" : [\"linear\", \"poly\", \"sigmoid\"], \n",
    "    \"SVC__degree\": [2,3]\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "#build a pipeline\n",
    "model_4_pipeline = Pipeline([\n",
    "        #tfidf vectorizer\n",
    "        ('word2vec', MeanEmbeddingVectorizer()),\n",
    "        #apply naive bayes\n",
    "        ('SVC', SVC())\n",
    "    ])\n",
    "\n",
    "#design grid search\n",
    "grid_search_model_4 = GridSearchCV(\n",
    "    #pipeline to be followed\n",
    "    model_4_pipeline,\n",
    "    #parameters\n",
    "    param_grid=parameters_model_4,\n",
    "    #number of folds for CV\n",
    "    cv=5,\n",
    "    #scoring to be considered for the cv\n",
    "    scoring = \"accuracy\",\n",
    "    #parallelize if possible\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#fit the grid search for training data\n",
    "grid_search_model_4.fit(file_contents, targets)\n",
    "\n",
    "#save results of cross validation\n",
    "cv_results_model_4 = pd.DataFrame(grid_search_model_4.cv_results_)\n",
    "\n",
    "#filter columns to be kept in the dataframe\n",
    "filter_col = [col for col in cv_results_model_4 if (col.startswith(\"param_\") or col.startswith(\"mean_\") or col.startswith(\"rank\"))]\n",
    "\n",
    "#save results with only filtered columns\n",
    "cv_results_model_4 = cv_results_model_4[filter_col]\n",
    "\n",
    "#save name of the model for later comparison\n",
    "cv_results_model_4.insert(loc=0, column=\"Model\", value= \"4\")\n",
    "\n",
    "#round mean_test_score\n",
    "cv_results_model_4[\"mean_test_score\"] = cv_results_model_4[\"mean_test_score\"].round(2)\n",
    "\n",
    "#show best 5 sorted by mean_test_score\n",
    "display(cv_results_model_4.sort_values(by=\"mean_test_score\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-physics",
   "metadata": {},
   "source": [
    "#### Model 5: Word2Vec and Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-diversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose parameters for the different steps in the pipeline\n",
    "parameters_model_5 = {\n",
    "    \n",
    "    #RF classifier\n",
    "    \"clf__n_estimators\" : [10,100,200,300]\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "#build a pipeline\n",
    "model_5_pipeline = Pipeline([\n",
    "        #tfidf vectorizer\n",
    "        ('word2vec', MeanEmbeddingVectorizer()),\n",
    "        #apply naive bayes\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "#design grid search\n",
    "grid_search_model_5 = GridSearchCV(\n",
    "    #pipeline to be followed\n",
    "    model_5_pipeline,\n",
    "    #parameters\n",
    "    param_grid=parameters_model_5,\n",
    "    #number of folds for CV\n",
    "    cv=5,\n",
    "    #scoring to be considered for the cv\n",
    "    scoring = \"accuracy\",\n",
    "    #parallelize if possible\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#fit the grid search for training data\n",
    "grid_search_model_5.fit(file_contents, targets)\n",
    "\n",
    "#save results of cross validation\n",
    "cv_results_model_5 = pd.DataFrame(grid_search_model_5.cv_results_)\n",
    "\n",
    "#filter columns to be kept in the dataframe\n",
    "filter_col = [col for col in cv_results_model_5 if (col.startswith(\"param_\") or col.startswith(\"mean_\") or col.startswith(\"rank\"))]\n",
    "\n",
    "#save results with only filtered columns\n",
    "cv_results_model_5 = cv_results_model_5[filter_col]\n",
    "\n",
    "#save name of the model for later comparison\n",
    "cv_results_model_5.insert(loc=0, column=\"Model\", value= \"5\")\n",
    "\n",
    "#round mean_test_score\n",
    "cv_results_model_5[\"mean_test_score\"] = cv_results_model_5[\"mean_test_score\"].round(2)\n",
    "\n",
    "#show best 5 sorted by mean_test_score\n",
    "display(cv_results_model_5.sort_values(by=\"mean_test_score\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-carrier",
   "metadata": {},
   "source": [
    "#### Model 6: Doc2Vec and Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-daisy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose parameters for the different steps in the pipeline\n",
    "parameters_model_6 = {\n",
    "    \n",
    "    #doc2Vec\n",
    "    'doc2vec__window': [4],\n",
    "    'doc2vec__vector_size': [300],\n",
    "    \n",
    "    #SVC\n",
    "    \"SVC__kernel\" : [\"linear\", \"poly\", \"sigmoid\"], \n",
    "    \"SVC__degree\": [2,3]\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "#build a pipeline\n",
    "model_6_pipeline = Pipeline([\n",
    "        #tfidf vectorizer\n",
    "        ('doc2vec', Doc2Vectorizer()),\n",
    "        #apply naive bayes\n",
    "        ('SVC', SVC())\n",
    "    ])\n",
    "\n",
    "#design grid search\n",
    "grid_search_model_6 = GridSearchCV(\n",
    "    #pipeline to be followed\n",
    "    model_6_pipeline,\n",
    "    #parameters\n",
    "    param_grid=parameters_model_6,\n",
    "    #number of folds for CV\n",
    "    cv=5,\n",
    "    #scoring to be considered for the cv\n",
    "    scoring = \"accuracy\",\n",
    "    #parallelize if possible\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#fit the grid search for training data\n",
    "grid_search_model_6.fit(file_contents, targets)\n",
    "\n",
    "#save results of cross validation\n",
    "cv_results_model_6 = pd.DataFrame(grid_search_model_6.cv_results_)\n",
    "\n",
    "#filter columns to be kept in the dataframe\n",
    "filter_col = [col for col in cv_results_model_6 if (col.startswith(\"param_\") or col.startswith(\"mean_\") or col.startswith(\"rank\"))]\n",
    "\n",
    "#save results with only filtered columns\n",
    "cv_results_model_6 = cv_results_model_6[filter_col]\n",
    "\n",
    "#save name of the model for later comparison\n",
    "cv_results_model_6.insert(loc=0, column=\"Model\", value= \"6\")\n",
    "\n",
    "#round mean_test_score\n",
    "cv_results_model_6[\"mean_test_score\"] = cv_results_model_6[\"mean_test_score\"].round(2)\n",
    "\n",
    "#show best 5 sorted by mean_test_score\n",
    "display(cv_results_model_6.sort_values(by=\"mean_test_score\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-hometown",
   "metadata": {},
   "source": [
    "#### Model 7: Doc2Vec and Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-challenge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose parameters for the different steps in the pipeline\n",
    "parameters_model_7 = {\n",
    "    \n",
    "    #doc2Vec\n",
    "    'doc2vec__window': [4],\n",
    "    'doc2vec__vector_size': [300],\n",
    "    \n",
    "    #RF classifier\n",
    "    \"clf__n_estimators\" : [10,100,200,300]\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "#build a pipeline\n",
    "model_7_pipeline = Pipeline([\n",
    "        #tfidf vectorizer\n",
    "        ('doc2vec', Doc2Vectorizer()),\n",
    "        #apply naive bayes\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "#design grid search\n",
    "grid_search_model_7 = GridSearchCV(\n",
    "    #pipeline to be followed\n",
    "    model_7_pipeline,\n",
    "    #parameters\n",
    "    param_grid=parameters_model_7,\n",
    "    #number of folds for CV\n",
    "    cv=5,\n",
    "    #scoring to be considered for the cv\n",
    "    scoring = \"accuracy\",\n",
    "    #parallelize if possible\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#fit the grid search for training data\n",
    "grid_search_model_7.fit(file_contents, targets)\n",
    "\n",
    "#save results of cross validation\n",
    "cv_results_model_7 = pd.DataFrame(grid_search_model_7.cv_results_)\n",
    "\n",
    "#filter columns to be kept in the dataframe\n",
    "filter_col = [col for col in cv_results_model_7 if (col.startswith(\"param_\") or col.startswith(\"mean_\") or col.startswith(\"rank\"))]\n",
    "\n",
    "#save results with only filtered columns\n",
    "cv_results_model_7 = cv_results_model_7[filter_col]\n",
    "\n",
    "#save name of the model for later comparison\n",
    "cv_results_model_7.insert(loc=0, column=\"Model\", value= \"7\")\n",
    "\n",
    "#round mean_test_score\n",
    "cv_results_model_7[\"mean_test_score\"] = cv_results_model_7[\"mean_test_score\"].round(2)\n",
    "\n",
    "#show best 5 sorted by mean_test_score\n",
    "display(cv_results_model_7.sort_values(by=\"mean_test_score\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-possession",
   "metadata": {},
   "source": [
    "### Compare CV results from trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-notice",
   "metadata": {},
   "source": [
    "In this section, the results from CV are compared within the trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-eating",
   "metadata": {},
   "source": [
    "#### Raw results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-seven",
   "metadata": {},
   "source": [
    "A dataframe showing the best models according to the mean accuracy within the test folds used for cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-choir",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge cv results into 1 that keeps the relevant information\n",
    "\n",
    "#empty dataframe that will keep all the results\n",
    "cv_results = pd.DataFrame()\n",
    "\n",
    "#loop over cv results\n",
    "for i in [cv_results_model_1, cv_results_model_2, cv_results_model_3, cv_results_model_4,\n",
    "          cv_results_model_5, cv_results_model_6, cv_results_model_7]:\n",
    "    \n",
    "    #select relevant columns\n",
    "    selected = i[[\"Model\",\"mean_fit_time\",\"mean_score_time\",\"mean_test_score\"]]\n",
    "    \n",
    "    #append to cv results\n",
    "    cv_results = cv_results.append(selected)\n",
    "    \n",
    "#show models with best scores\n",
    "display(cv_results.sort_values(by=\"mean_test_score\", ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-georgia",
   "metadata": {},
   "source": [
    "⚠️ *This table could be improved by also indicating the parameters used in each model but I thought it would be a bit overwhelming*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-firewall",
   "metadata": {},
   "source": [
    "#### Tradeoff score vs mean fit time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-calibration",
   "metadata": {},
   "source": [
    "A plot to check if there is some kind of tradeoff between accuracy and runtime of the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-polymer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by model\n",
    "groups = cv_results.groupby(\"Model\")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "for name, group in groups:\n",
    "    ax.plot(group.mean_fit_time, group.mean_test_score, marker='o', linestyle='', ms=12, label=\"Model %s\" %name)\n",
    "ax.legend(loc = 1)\n",
    "plt.xlabel(\"Mean fit time\")\n",
    "plt.ylabel(\"Mean test Score (accuracy)\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-acrylic",
   "metadata": {},
   "source": [
    "⚠️ *We can add as much as we want here...*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-orleans",
   "metadata": {},
   "source": [
    "#### Best estimator from each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-medication",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot to add"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-apple",
   "metadata": {},
   "source": [
    "#### Best 5 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-norfolk",
   "metadata": {},
   "outputs": [],
   "source": [
    " #plot to add"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-vietnamese",
   "metadata": {},
   "source": [
    "## Evaluation and model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earlier-redhead",
   "metadata": {},
   "source": [
    "Predict on \"unseen\" data using the best models obtained in the training phase and evaluate using different metrics.\n",
    "\n",
    "⚠️ *Best models will be selected when training the models with the actual train data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-dining",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add models to be evaluated\n",
    "models = [\n",
    "    grid_search_model_1,\n",
    "    grid_search_model_2,\n",
    "    grid_search_model_3,\n",
    "    grid_search_model_7\n",
    "]\n",
    "\n",
    "evaluation = pd.DataFrame(columns=[\"model\",\"accuracy\",\"recall\",\"precision\",\"f1\"])\n",
    "\n",
    "\n",
    "for i in models:\n",
    "    preds = i.best_estimator_.predict(file_contents_test)\n",
    "    model = cv_results.iloc[i.best_index_,0]\n",
    "    to_append = [\n",
    "            # to FIX so that it says to which model number it corresponds\n",
    "            \"Model %s\" %i,\n",
    "            accuracy_score(y_true=targets_test,y_pred=preds),\n",
    "            #choose micro or macro according to criteria\n",
    "            recall_score(y_true=targets_test,y_pred=preds, average=\"micro\"),\n",
    "            precision_score(y_true=targets_test,y_pred=preds, average=\"micro\"),\n",
    "            f1_score(y_true=targets_test,y_pred=preds, average=\"micro\")\n",
    "            ]\n",
    "    evaluation_length = len(evaluation)\n",
    "    evaluation.loc[evaluation_length] = to_append\n",
    "\n",
    "\n",
    "evaluation.sort_values(by=\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-rendering",
   "metadata": {},
   "source": [
    "The **best estimator we found is:** \n",
    "\n",
    "⚠️ *Best models will be selected when training the models with the actual train data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-liechtenstein",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_model_1.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-camera",
   "metadata": {},
   "source": [
    "Predict using this estimator and show **confusion matrix** on test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-abraham",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = grid_search_model_1.best_estimator_.predict(file_contents_test)\n",
    "\n",
    "confusion_matrix(targets_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-vegetation",
   "metadata": {},
   "source": [
    "Main **conclusion**: for unseen data, we would choose to use the estimator from above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-stephen",
   "metadata": {},
   "source": [
    "### Michaels Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-pension",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(type(preds))\n",
    "#print(type(targets_test))\n",
    "#print(targets_test)\n",
    "#print(models[1])\n",
    "\n",
    "#preds_list = preds.tolist()\n",
    "#print(type(preds_list))\n",
    "def eval_(estimator):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    plot_confusion_matrix(estimator=estimator\n",
    "                          , X=file_contents_test\n",
    "                          , y_true=targets_test\n",
    "                          , ax=ax\n",
    "                         )\n",
    "    #accuracy = accuracy_score(y_true=targets_test, y_pred=estimator.predict(file_contents_test))\n",
    "    #print(accuracy)\n",
    "\n",
    "eval_(grid_search_model_1.best_estimator_)\n",
    "eval_(grid_search_model_2.best_estimator_)\n",
    "eval_(grid_search_model_3.best_estimator_)\n",
    "eval_(grid_search_model_4.best_estimator_) # Why you break??\n",
    "eval_(grid_search_model_5.best_estimator_)\n",
    "eval_(grid_search_model_6.best_estimator_)\n",
    "eval_(grid_search_model_7.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-survival",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-implementation",
   "metadata": {},
   "source": [
    "**Draw conclusions from above. but add more evaulations and stuff....**\n",
    "\n",
    "As often is the case in the fields of science, not all research leads to useable results. We ended up having to remodel our plans several times during this project, including a complete pivot of the datasets.\n",
    "\n",
    "This did however give us some insight into how larger projects are managed. This also lead us to an interesting path of looking at a relatively obscure language.\n",
    "\n",
    "Although further works is possible, we reached the conclusion that there is a change in the Icelandic spoken language throughout time, and it is therefore possible to train models that estimates which decade a given speech is from.\n",
    "\n",
    "Overall we did work with Data-Oriented Programming best practices. We were able to develop a scientific workflow. From the given data, we managed to train a model for prediction with decent results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-chest",
   "metadata": {},
   "source": [
    "# Further Works\n",
    "As we drilled down this dataset, we kept getting new ideas that we would like to experiment with, and try to gain better insight.\n",
    "Specifically, our next steps would be:\n",
    "\n",
    "## Predict Different Sources\n",
    "As it currently stands, we are trying to estimate a decade of speeches from \"Althingi\". However, the dataset has several other sources of Icelandic; both written and spoken.\n",
    "\n",
    "We would like to see if it was possible to extend our model to be able to classify the source.\n",
    "\n",
    "## Treating years as Contious Variables\n",
    "We are currently treating decades as a class. By discretizing results from a regression algorithm, we think it should be possible to keep some nominal knowledge of the ordering of the years, and thus improving our predictions\n",
    "\n",
    "## Gaining insight into Explanatory Variables\n",
    "From our results, it is clear that it is somewhat possible to predict the decades. However, we are still treating the algorithms as \"Black Boxes\". \n",
    "We would like to dive deeper into the decision treas/boundaries, to see if we can locate what it is that makes the predictions possible. It might be new words introduced, semantic changes, or something entirely different.\n",
    "\n",
    "## Additional Feature Extraction and Classifiers\n",
    "We would like to extend the list to include more classifiers, as well as trying to develop some additional feature extractions.\n",
    "E.g. \"Glove Embedding\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "105px",
    "width": "242px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "555px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
